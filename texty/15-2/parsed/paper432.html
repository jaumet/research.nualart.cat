<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
	"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"> 
<html xmlns="http://www.w3.org/1999/xhtml"> 

<!-- Mirrored from informationr.net/ir/15-2/paper432.html by HTTrack Website Copier/3.x [XR&CO'2010], Sat, 25 Dec 2010 19:34:14 GMT -->
<head> 
<title>Questioners' credibility judgments of answers in a social questions and answers site</title> 
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1" /> 
<link href="../IRstyle2.css" rel="stylesheet" media="screen" title="serif" /> 
<link rel="alternate stylesheet" type="text/css" media="screen" title="sans" href="../IRstylesans.css" /> 
<link rev="made" href="mailto:t.d.wilson@shef.ac.uk" /> 
 
<!--Enter appropriate data in the content fields--> 
<meta name="dc.title" content="Questioners' credibility judgments of answers in a social questions and answers site" /> 
<meta name="dc.creator" content="Soojung Kim" /> 
<meta name="dc.subject" content="Credibility perception" /> 
<meta name="dc.description" content="Introduction. This paper explores a sample of thirty-six active users' experience of credibility judgments in Yahoo! Answers, the most popular social question and answer (questions and answers) site in the U.S., to understand how users evaluate the credibility of answers provided by fellow users on the Web. Method. Interviews were conducted with thirty-six questioners of Yahoo! Answers via email, Internet chat, and over the telephone. Analysis. The interviews were transcribed and the data was analyzed using the constant-comparison method of content analysis. 
Results. The questioners' credibility judgments in the site link to a broad context of an information seeking process, including pre-search activities and post-search verification behaviours. Many message- and source-related criteria from previous Web credibility research transfer to the social questions and answers environment, but website-related criteria do not apply here because questioners evaluate answerers in the same site. Also, the questioners evaluated message credibility more often than source credibility due to the frequent unavailability of source information. Conclusions. Many questioners exhibited critical evaluation skills at least to some extent. An improved design of the site and user instruction, however, could further help questioners and answerers find or provide credible information. " /> 
<meta name="dc.subject.keywords" content="credibility, online communities, information seeking behaviour" /> 
 
<!--leave the following to be completed by the Editor--> 
<meta name="robots" content="all" /> 

<meta name="dc.publisher" content="Professor T.D. Wilson" /> 
<meta name="dc.coverage.placename" content="global" /> 
<meta name="dc.type" content="text" /> 
<meta name="dc.identifier" scheme="ISSN" content="1368-1613" /> 
<meta name="dc.identifier" scheme="URI" content="paper432.html" /> 
<meta name="dc.relation.IsPartOf" content="infres152.html" /> 
<meta name="dc.format" content="text/html" /> 
<meta name="dc.language" content="en" /> 
<meta name="dc.rights" content="http://creativecommons.org/licenses/by-nd-nc/1.0/" /> 
<meta name="dc.date.available" content="2010-06-15" /> 
 
<script language="javascript" type="text/javascript"> 
 
		var flag;
		flag = true;
		function doChangeFont()
		{
			if (flag)
			{
			var htmlDoc = document.getElementsByTagName('head').item(0);
			var css = document.createElement('link');
			css.setAttribute('rel', 'stylesheet');
			css.setAttribute('type', 'text/css');
			css.setAttribute('href', '../sans.css');
			htmlDoc.appendChild(css);
			flag = false;
			} 
			else
			{
			var htmlDoc = document.getElementsByTagName('head').item(0);
			var css = document.createElement('link');
			css.setAttribute('rel', 'stylesheet');
			css.setAttribute('type', 'text/css');
			css.setAttribute('href', '../IRstyle2.css');
			htmlDoc.appendChild(css);
			flag = true;
			}	
		}
		
	</script> 
	
<style type="text/css"> 
.button {
	width: 45em;
	padding: 0 0 0 0;
	font-family: Verdana, Lucida, Geneva, Helvetica, Arial, sans-serif;
	font-size: small;
	font-weight: bold;  
	background-color: #ffffff;
	color: #000000;
	display: inline;
	text-align: center;
	}
		
 
.button ul {
		list-style: none;
		margin: 0;
		padding: 0;
		border: none;
		display: inline;
		}
		
.button li {
		margin: 0;
		font-family: Verdana, Lucida, Geneva, Helvetica, Arial, sans-serif;
	    font-size: small;
	    font-weight: bold;  
		background-color: #fff000<!-- #2175bc; --> 
		color: #000000;
		text-decoration: none;
		display: inline;
		}
 
.button li a:hover {
		background-color: azure;
		color: #ff0000;
		width: auto;
		}
		
fieldset {
    padding: .5em;
    background: white;
    border: 1px dotted #5E96FD;
    margin-left: 15px;
    margin-right: 15px;
    margin-top: .5em;
	}
 
legend {
    color: white;
    background-color: #5E96FD;
    font-size: medium;
    padding: .1ex .5ex;
    border-right: 1px solid navy;
    border-bottom: 1px solid navy;
    font-weight: bold;
}
 
 
</style> 
 
<link href="http://localhost/texty-academics/css/parser.css" rel="stylesheet" type="text/css" /></head> 
<body bgcolor="#ffffff"> 
<table cellspacing="0" cellpadding="0" align="center" border="0">
  <tbody>
  <tr>
    <td align="center" colspan="5" height="30"><img height="45" alt="header" src="../mini_logo2.gif" 
      width="336" /><br /><span style="font-size: medium; font-variant: small-caps; font-weight: bold;">vol. 15  no. 1, March, 2010</span><br /><br />
      <div class="button">
      <ul>
        <li><a href="infres152.html">Contents</a> |  </li>
        <li><a href="../iraindex.html">Author index</a> |  </li>
        <li><a href="../irsindex.html">Subject index</a> |  </li>
        <li><a href="../search.html">Search</a> |  </li>
        <li><a href="../index-2.html">Home</a>  </li>
   </ul></div></td></tr>
  <tr>
    <td>&nbsp;</td></tr></tbody></table>

<hr style="COLOR: #5E96FD" size="1" /> 
 
<h1>Questioners' credibility judgments of answers in a social question and answer site</h1> 
<br /> 
<div style="margin-left: 10%; margin-right: 10%;"> 
<h4><a href="#author">Soojung Kim</a><br /> 
College of Information Studies, Hornbake South Wing <br />
University of Maryland, College Park, MD 20742-4345</h4> 
</div> 
<br /> 
 

<form action="#"> 
<fieldset> 
<legend>Abstract </legend> 
<blockquote> 
<strong>Introduction.</strong> This paper explores a sample of thirty-six active users' experience of credibility judgments in Yahoo! Answers, the most popular social question and answer site in the U.S., to understand how users evaluate the credibility of answers provided by fellow users on the Web.  <br />  
<strong>Method.</strong> <span>Interviews</span>  <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>were conducted with thirty-six questioners of Yahoo! Answers by e-mail, Internet chat and  telephone.  <br />  
<strong>Analysis.</strong> The <span>interviews</span>  <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>were transcribed and the data were analysed using the constant-comparison method of content analysis.<br />  
<strong>Results.</strong> The questioners' credibility judgments in the site link to a broad context of an information seeking process, including pre-search activities and post-search verification behaviour. 
Many message- and source-related criteria from previous Web credibility research transfer to the social question and answer environment, but Website-related criteria do not apply here because questioners evaluate answerers in the same site. Also, the questioners evaluated message credibility more often than source credibility due to the frequent unavailability of source information.  <br /> 
<strong>Conclusions.</strong> Many questioners exhibited critical evaluation skills at least to some extent. An improved design of the site and user instruction, however, could further help questioners and answerers find or provide credible information. </blockquote> 
</fieldset> 
</form> 
 
<br /> 
<div align="center"> 
<input type="button" value="change font" class="btn" style="font-variant: small-caps; font-weight: bold; font-family: Verdana; color: white; background-color: #5E96FD;" onclick="doChangeFont()" /></div> 

<br /> 
 
<h2>Introduction</h2> 
 
<p>The Web, by its nature, allows anyone to publish without regulations and it has raised concerns about the credibility of information found therein. Although recent studies elucidate the factors affecting users' credibility judgments and practitioners have suggested guidelines to help users develop a keen eye to assess the information on the Web, they are limited to evaluation of individual Websites or Websites as a medium. What is lesser known in the literature are users' credibility judgments in social media such as blogs, wikis, and social question and answer sites. Characterized by user participation and openness in creating and managing content, social media add even more complexity to the complicated task of filtering out credible information on the Web because users evaluate information given by fellow users whose expertise is hard to assess with traditional credibility cues such as an author's affiliation.   </p> 
 
<p>Among a variety of social media, of particular concern to this study is a social question and answer site. A social question and answer site is a community-based Website where people ask and answer questions for one another. Since <a href="http://www.answerbag.com/" target="_blank">AnswerBag</a> was first introduced in 2003, these sites have been growing rapidly in size and importance in the realm of online searching. According to Hitwise (<a href="#hit">2008</a>), U.S. visits to such sites increased 898% between February 2006 and February 2008 and the average visit time among the top five sites increased 44% between March 2007 and March 2008. This dazzling success of social question and answer sites is largely attributed to the millions of lay people who volunteer to answer others' questions. However, these sites are criticized for precisely the same reason: anyone can post answers without a peer-review process, so the <span>quality</span>  <span class="quali" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>of specific answers varies drastically, ranging from excellent to abuse and spam (<a href="#su">Su <em>et al.</em>  2007</a>). Despite widely-held concerns about the credibility of user-generated content on the Web, little is known about how people evaluate the credibility of answers given by laypeople in a social question and answer site. To fill this void, this study aims to explore users' experience of credibility judgments in a social question and answer site. Because assessing credibility is an ongoing and iterative process throughout the information-seeking process, rather than one-time action (<a href="#rieh">Rieh and Hilligoss 2008</a>), the study examines users' evaluation of individual answers in the site along with pre-search activities and post-search verification behaviour.</p> 


<p>The popularity of  social question and answer sites has grown rapidly over the last couple of years, and thus it seems timely to study users' credibility judgments in the environment. The investigation of real users having real questions for their everyday life tasks in such a site extends previous credibility research to a novel environment and thus presents a fuller picture of Web credibility.  </p>
 
<h2>Background </h2>
 
<b>Credibility on the Web</b>

<p>Credibility is conceptualized as a multidimensional construct including believability, trust, reliability, accuracy, fairness, objectivity, and other <span>concepts</span>  <span class="conce" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>(<a href="#sel">Self 1996</a>). Although there is no <span>universal</span>  <span class="conce" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>agreement on what dimensions constitute credibility, the notion of credibility is widely accepted as believability (<a href="#tse">Tseng and Fogg 1999</a>) with two key dimensions: trustworthiness and expertise (<a href="#hov">Hovland, Janis and Kelley 1953</a>). How to sort out believable from unbelievable information is not a new problem, but the Web provides unprecedented conditions with its relative lack of centralized <span>quality</span>  <span class="quali" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>control mechanisms and source attributions, along with its complex interface features and speed of growth (<a href="#bur">Burbules 2001</a>; <a href="#dan">Danielson 2005</a>). Over the past decade, researchers have increasingly recognized the importance of credibility judgments of Web information, which has fueled a burgeoning research area, Web credibility, and thus, a number of studies (for a comprehensive literature review, refer to <a href="#sel">Self 1996</a>; <a href="#wat">Wathen and Burkell 2002</a>; <a href="#metall">Metzger <i>et al.</i> 2003</a>; <a href="#ried">Rieh and Danielson 2007</a>).</p>

<p>Drawing on four years of research on Web credibility by Stanford's Persuasive Technology Lab, Fogg (<a href="#fog03">2003</a>) proposed prominence-interpretation theory, which posits that a person's credibility judgment involves two stages; first, a user notices elements of a Website (prominence) and makes a judgment about it (interpretation). If a specific element is not noticed, it cannot make any impact on the credibility judgment of the site. In other words, the elements that are noticed and interpreted have an impact on credibility judgment. While some previous studies focus on interpretation by presenting users with specific Website elements to see their impact on credibility judgment (e.g., <a href="#fogall01">Fogg <em>et al.</em>  2001</a>), others focus on prominence by letting users notice certain elements as they explore the Websites. This study is in line with the latter approach as it asks users to recall the most prominent elements of an answer that lead them to accept or reject the answer as credible or incredible information. </p>

<p>Empirical studies have uncovered a set of criteria that influence credibility judgments, which can be grouped on three levels (<a href="#rieb08">Rieh and Belkin 1998</a>, <a href="#rieb00">2000</a>; <a href="#fogall00">Fogg <em>et al.</em>  2000</a>; <a href="#rie">Rieh 2002</a>; <a href="#eys">Eysenbach and Kohlelr 2002</a>; <a href="#fogall03">Fogg <em>et al.</em>  2003</a>; <a href="#fre">Freeman and  Spyridakis 2004</a>; <a href="#liu">Liu 2004</a>; <a href="#liuh">Liu and Huang 2005</a>): </p>

<ol>
  <li> Criteria related to the credibility of information itself (e.g., <span>quality</span>, <span class="quali" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span> accuracy, clarity, currency, spelling and grammar, tone of writing, bias, usefulness).  </li>
  <li> Criteria related to the credibility of a source or sponsor of the Website (e.g.,  author's contact information, authority, expertise, affiliation, reputation, presence of the organization's address, type of source). </li>
  <li>	Criteria related to the credibility of a Website as a whole (e.g., design and look, reference by reputable sources, navigability, functionality, advertising, customer service, and Website host) </li>
</ol>

<p>The criteria on the different levels interact with one another. For example, the <span>characteristics</span>  <span class="quali" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>of the source determine perceptions of credibility of information under the assumption that credible sources produce credible information. Conversely, the attributes of information are used to ascertain source credibility in the absence of knowledge of the source (<a href="#ros">Rosenthal 1971</a>; <a href="#sla">Slater and Rouner 1996</a>). Additionally, people transfer the credibility assessment of a media product or information channel to the information within it (<a href="#fla08a">Flanagin and Metzger 2008a</a>; <a href="#hil">Hilligoss and Rieh 2008</a>). For example, students perceive books and scholarly journal articles as more credible media than the Web and blogs. However, while people perceive certain media or channels to be non-credible, they still have good reason to use them (<a href="#hil">Hilligoss and Rieh 2008</a>). For example, blogs may lack credibility because they are opinion-based, but they are useful for getting new <span>ideas</span>. <span class="conce" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span> Therefore, how users perceive the credibility of a social question and answer site and what they look for are likely to influence their decision to use the site and the way they evaluate information in the site. </p>

<p>The relative importance of each criterion varies from study to study because of the <span>characteristics</span>  <span class="quali" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>of <span>participants</span>, <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span> types of source, type of information, and other conditions. In Rieh's (<a href="#rie">2002</a>) study, for example, scholars were more concerned with content and source reputation than with presentation, graphics, and functionality. In similar fashion, Hong (<a href="#hon">2006</a>) found that undergraduates regarded message features (e.g., quotations, reference sources) as more important than Website structural attributes (e.g., <span>domain</span>  <span class="conce" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>names and site maps). The general public in the study by Fogg <i>et al.</i> (<a href="#fogall03">2003</a>) study, on the other hand, evaluated the Website's appearance more frequently than any other features. In a comparative study of undergraduates' and graduates' credibility judgments, Liu and Huang (<a href="#liuh">2005</a>) found that while undergraduates predominantly relied on an author's name, reputation and affiliation, graduate students paid more attention to information accuracy and <span>quality</span>. <span class="quali" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span> These findings confirm that experts tend to evaluate content more than other attributes (<a href="#sta">Stanford  2002</a>). </p>

<p>It should be noted that credibility judgment is an ongoing process rather than a discrete evaluation event as people make three distinctive <span>kinds</span>  <span class="quali" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>of judgments: predictive judgment, evaluative judgment, and verification (<a href="#rieh">Rieh and Hilligoss 2008</a>). People make predictive judgments about a source from which information will be gathered, and upon the selection of a source, make evaluative judgments of information presented in the source. People may attempt to verify information because they are uncertain about the credibility of information when first encountered or because they find the information incorrect later after initially accepting the information. Gray <i>et al.</i>'s (<a href="#gra">2005</a>) health information seekers verified information they obtained from the Web with personal sources offline. Undergraduates in Flanagin and Metzger's study (<a href="#fla00">2000</a>) also performed verification, but did so only rarely or occasionally. Furthermore, people's verbal reports do not accurately represent their actual behaviour. Often, people report they verify credibility, but their actual behavior belies what they claim (<a href="#fla00">Flanagin and Metzger 2000</a>).</p>

<p>While the previous studies provide a valuable theoretical framework, which can serve as a <span>conceptual</span>  <span class="conce" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>basis for this study, social question and answer sites offer a unique venue for understanding Web credibility judgments for two reasons. First, evaluating specific answers is different from evaluating entire Websites, in that individual lay people provide information directly to questioners. Given that noting the institutional level of <span>quality</span>  <span class="quali" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>markers such as URL <span>domain</span>  <span class="conce" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>or author affiliation is a common strategy in Website evaluation (<a href="#rie">Rieh 2002</a>), the absence of such <span>quality</span>  <span class="quali" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>markers in social question and answer sites may force users to more critically scrutinize answers or the credentials of potentially unqualified answerers. Second, still in its infancy as a medium, a social question and answer site requires users to not only possess general evaluative skills, but be acquainted with the new features of the site, which can be used as information <span>quality</span>  <span class="quali" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>markers or credential clues (e.g., an answerer's profile). Some users, especially novices, may have yet to develop effective evaluative skills specific to this environment. </p>

<p>The advent of social question and answer sites adds another layer of complexity to Web credibility with its unique nature. Knowing how users evaluate answers given by lay people will broaden our understanding of Web credibility in the era of user participation and content creation. </p>

<b>Quality of answers in a social questions and answers site</b>

<p>The extreme variability in the <span>quality</span>  <span class="quali" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>of individual answers in a social question and answer site has led to recent studies that seek to investigate users' evaluation of answers or to automatically identify determinants of answer <span>quality</span>  <span class="quali" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>with the purpose of improving the function of the site.   </p>

<p>Gazan (<a href="#gaz">2006</a>) examined the influence of answerers' roles on questioners' evaluations of answers in AnswerBag. He grouped the role of answerers into two types: specialists and synthesists. Specialists provide answers based on their knowledge without referencing external sources, while synthesists provide answers using external sources without claiming any expertise. The study showed that the synthesists' answers were generally rated higher than were those of specialists by questioners. Kim and Oh (<a href="#kim">2009</a>) identified twenty-three relevance criteria questioners use when they select the best answers through content analysis of 2,140 comments questioners left on the best answers in Yahoo! Answers. Their findings illustrate that users evaluate not only the content of answers (e.g., accuracy), but socio-emotional values (e.g., emotional support), utility (e.g., usefulness), information sources (e.g., author's expertise, external links), and other criteria for the selection of best answers. Kim and Oh (<a href="#kim">2009</a>) and Gazan (<a href="#gaz">2006</a>) did not explicitly examine credibility, but it is obvious that their <span>participants</span>  <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>con0ered credibility when evaluating answers, as evidenced by the commonly identified criteria such as references to external links and answerer's expertise. </p>

<p>Adamic <i>et al.</i> (<a href="#ada">2008</a>) predicted a questioner's <span>choice</span>  <span class="conce" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>of best answers based on the attributes associated with the content and answerer drawing on 1.2 million questions and 8.5 million answers in Yahoo! Answers. Their findings showed that the length of the answer was most indicative across topic <span>categories</span>  <span class="conce" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>(lengthier answers tend to be selected as best answers). In certain topic <span>categories</span>, <span class="conce" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span> however, the number of competing answers and the <span>history</span>  <span class="conce" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>of the answerer were more likely to predict answer <span>quality</span>. <span class="quali" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span> These findings are consistent with Agichtein <i>et al.</i>'s (<a href="#agi">2008</a>) study which found that answer length is dominant over other answer features in predicting answer <span>quality</span>. <span class="quali" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span> In the same vein, to predict questioners' satisfaction with the answers presented in <a href="http://answers.yahoo.com/" target="_blank">Yahoo! Answers</a>, Liu <i>et al.</i> (<a href="#liuall">2008</a>) built a model by extracting six sets of features: question, question-answer relationship, asker user <span>history</span>, <span class="conce" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span> answerer user <span>history</span>, <span class="conce" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span> <span>category</span>  <span class="conce" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>features, and textual features. Among them, askers' ratings (satisfaction) of an answer in response to a previous question was the most salient feature to predict their satisfaction with a new answer. On the other hand, the reputation of the answerer was much less important, suggesting that the authority of the answerer might only be important for some, but not all, information needs.</p>

<p>Interestingly, some researchers and operators of social question and answer sites speculate that one of the reasons such sites are not regarded as reliable sources of high-quality information is the prevalence of conversational questions (e.g., Do you believe in evolution?) as opposed to informational questions (e.g., What is the difference between Burma and Myanmar?) (<a href="#har">Harper <i>et al.</i> 2009</a>). Since these two question types are asked for different purposes, people may apply different sets of criteria when evaluating answers provided for each type of question. Therefore, this study categorizes question types into conversational and informational questions and examines their influence on credibility judgments. </p>

<h2>Research questions</h2>

<p>The purpose of this study is to investigate users' credibility judgments in a social questions and answers site. Recognizing credibility judgment as an ongoing process instead of a discrete activity, the study describes a sample of users' motivations to use a social questions and answers site, credibility judgments of answers, and post-search verification behaviour. In addition, it investigates the relationships between question type and credibility judgment. The specific research questions the study addresses are as follows: </p>

<ol>
  <li>Why do people use a social questions and answers site for information searching?  </li>
  <li>What criteria do people use when assessing the credibility of given answers? </li>
  <li>How do people verify the information they obtain from the site?</li>
  <li>What are the relationships between question type and credibility judgment? </li>
</ol>

<h2>Methods</h2>

<p>This study is part of a bigger project whose aim is to understand the information seeking and providing behaviour of questioners and answerers in a social questions and answers site. Since the project was necessarily descriptive and exploratory in nature, <span>interviews</span>  <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>were conducted to investigate both questioners and answerers' experiences of the site. This study reports only on the <span>interviews</span>  <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>with questioners regarding their credibility judgments. E-mail, chat, and telephone <span>interviews</span>  <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>were held with thirty-six questioners of Yahoo! Answers and the <span>interview</span>  <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>transcripts were analysed using the constant-comparison method (<a href="#lin">Lincoln and Guba 1985</a>).</p>

<b>Yahoo! Answers </b>

<p>This study selected Yahoo! Answers as a research setting because of its dominant status among social question and answer sites. As of March 2008, Yahoo! Answers was the most visited question and answer site in the U.S., accounting for 74% of all  visits (<a href="#hit">Hitwise 2008</a>). It has attracted twenty-five million users with 237 million answers in the U.S. and 135 million users with 500 million answers worldwide (<a href="#mcg">McGee 2008</a>). The astonishing scale of data and diversity of topics have made Yahoo! Answers a popular setting for recent research on such sites despite its short <span>history</span>  <span class="conce" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>(<a href="#agi">Agichtein <em>et al.</em>  2008</a>). </p>

<p>The process of asking and obtaining answers to a question is quite simple: a user (questioner) posts a question under a relevant <span>category</span>  <span class="conce" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>from twenty-five top-level topic <span>categories</span>  <span class="conce" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>and it becomes an open question. Once the question is posted, any user (answerers) can post answers to it. Among all answers posted, the questioner can select the best answer or, alternatively, allow the community to vote for the best answer. When a best answer is chosen, either by the questioner or by the vote, the question becomes a resolved question and remains in the Discover section for browsing and searching. </p>

<p>To encourage user participation and reward high <span>quality</span>  <span class="quali" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>answers, Yahoo! Answers implements a point system and, based on the points, categorizes members into different levels (<a href="#yah">Yahoo! Answers 2009</a>). For example, an answerer gets two points for each answer s/he posts and gets ten points when the answer is selected as the best answer. When  questioners choose the best answer for themselves or vote for the best answer for others they also get points. The earned points allow everyone to recognize how active and helpful a user has been in the site. </p>

<b>Data collection and analysis </b>

<p>Starting November 2008 and ending April 2009, a solicitation e-mail was sent to 750 Yahoo! Answers users individually for the project. Each week during the twenty-five week period, thirty users of Yahoo! Answers (fifteen questioners and fifteen answerers) were selected from one of twenty-five top-level topic <span>categories</span>  <span class="conce" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>in the Discover section using three criteria. The first criterion was to select those who asked a question most recently in each topic <span>category</span>. <span class="conce" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span> Because of the heavy traffic, the selected <span>participants</span>  <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>had usually asked questions within the last day. The second criterion was to select those whose Yahoo! e-mail addresses were public in their profiles, so that the researcher could contact them by e-mail. The third criterion excluded those who explicitly stated in profiles, questions or answers that they were under 18. </p>

<p>In the solicitation e-mail, the <span>participants</span>  <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>were given four options of interviewing: telephone, e-mail, chat, and face-to-face (for nearby <span>participants</span>  <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>only). Given that Yahoo! Answers users are geographically dispersed and they vary con0erably in terms of Internet proficiencies and writing skills, it was an appropriate <span>choice</span>  <span class="conce" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>to provide as many <span>interview</span>  <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>methods as possible. By allowing people to select the <span>interview</span>  <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>method they felt most comfortable with, the weaknesses associated with each method were expected to be reduced to a minimum. </p>

<p>For the bigger project, two types of semi-structured <span>interview</span>  <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>questionnaires were prepared: one was for questioners and the other for answerers. The <span>participants</span>  <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>were asked to select the type of questionnaire(s) they would like to complete based on their questioning and answering experience in the site. As aforementioned, this study reports only on the data derived from the <span>interviews</span>  <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>with questioners. The semi-structured <span>interviews</span>  <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>for questioners included seven open-ended questions about: </p>

<ol>
  <li>What questions they had asked recently in Yahoo! Answers. </li>
  <li>What motivated them to use the site.</li>
  <li>If they searched for information before going to the site.</li>
  <li>How they assessed the credibility of answers given by fellow users. </li>

  <li>How they verified the obtained information with external sources. </li>
  <li>What credibility criteria they think are important in general when evaluating answers in the site. </li>
  <li>What they think of the credibility of the entire site.</li> 
</ol>

<p>A questioner's familiarity with the topic of the question, urgency of the information need, experience with the site, and demographic information (age, sex, occupation) were solicited at the end of the <span>interview</span>. <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span> The Critical Incident Technique was used to help the <span>participants</span>  <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>focus on their most recent questions and evaluation processes. This is a popular <span>interview</span>  <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>technique used to identify specific incidents which <span>participants</span>  <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>experienced personally rather than eliciting their generalized opinions on a critical issue. Since the purpose of this study is to describe how the <span>participants</span>  <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>assess information, the method was useful in drawing out realistic details without observing them directly. </p>

<p>From the 750 e-mails sent, thirty-six <span>interviews</span>  <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>resulted with questioners and forty-four <span>interviews</span>  <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>with answerers. A possible reason for the low participation rate is the use of Yahoo! e-mail addresses as a contact method to reach potential <span>participants</span>. <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span> Creating a Yahoo! e-mail account is mandatory to register for Yahoo! Answers. People create the e-mail accounts as a means to be a member of the site, but not all of them are actually using the accounts. This may have caused a high undeliverable rate. </p>

<p>Among thirty-six <span>interviews</span>  <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>with questioners, there were 17 e-mail <span>interviews</span>, <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span> 10 through Internet chat (Chatmaker and Yahoo! Messenger), and 9 by telephone. Each chat or telephone <span>interview</span>  <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>took approximately 40 minutes to an hour and a half. During the chat and telephone <span>interviews</span>, <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span> some questioners pulled up their questions in the site and walked through their evaluation processes with associated answers. The chat session transcripts were automatically recorded and the telephone <span>interviews</span>  <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>were audio-taped and transcribed verbatim. Five follow-up <span>interviews</span>  <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>were conducted with the e-mail interviewees for clarification and missing data. </p>

<p>The data was analysed using the constant-comparison method of content analysis (<a href="#lin">Lincoln and Guba 1985</a>).  The researcher read through the transcripts and classified individual statements into <span>categories</span>  <span class="conce" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>with a simultaneous comparison of other <span>categories</span>. <span class="conce" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>  Throughout the process, themes formed inductively, guided by the <span>interview</span>  <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>questions and patterns emerged to provide various perspectives on central issues.  To see the influence of question type on credibility judgments, the <span>participants</span>' <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span> questions were categorized into two groups as in Harper <i>et al.</i> (<a href="#har">2009</a>):</p>

<ol>
  <li>informational questions: questions asked with the intent of getting information that the asker seeks to receive; and</li>
  <li>conversational questions: questions asked with the intent of stimulating discussion or surveying opinions for fun. </li> 
</ol>

<p>Conversational questions are <span>intended</span>  <span class="quali" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>to spark a discussion and do a poll on a particular issue, and therefore are not expected to have one correct answer. On the other hand, informational questions are <span>intended</span>  <span class="quali" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>to call for facts, procedures, recommendations for products and services, or sources of information. This type of question is expected to have one correct answer or appropriate recommendation. </p>

<p>When it comes to credibility criteria, the researcher and a library science graduate student coded the mentions of criteria independently. Through the initial coding, the two coders developed a codebook iteratively by reaching a consensus on the analysis of shared transcripts. The codebook included a list of criteria the <span>participants</span>  <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>used along with their <span>definitions</span>  <span class="conce" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>and examples. With the finalized codebook, the coders coded the entire transcripts again independently. After this round of coding, inter-coder reliability was calculated using Cohen's kappa. The value of Cohen's kappa was 77%. If Cohen's kappa is greater than 70%, the agreement is regarded as substantial (<a href="#lan">Landis and Koch 1977</a>).  All disagreements were resolved through discussion to reach consensus.  </p>

<p>To verify the researcher's interpretations and conclusions, a member check of the results occurred with four <span>participants</span>, <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span> selected based on the number of times they were quoted in the study. A member check is regarded as the most critical method in establishing validity in a <span>qualitative</span>  <span class="quali" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>study (<a href="#lin">Lincoln and Guba 1985: 314</a>). A preliminary draft of the results was sent to the four <span>participants</span>  <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>and they confirmed and agreed with all the findings presented. </p>

<h2>Results</h2>

<b>Interview <span>participant</span>  <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span><span>characteristics</span>  <span class="quali" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span></b>

<p>Most <span>participants</span>  <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>were male (72%, n=26) (Table 1). The <span>participants</span>  <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>ranged widely in age from 18 to 67 (mean: 37, SD: 13.6), although over half of them were in their 20s or 30s. It was assumed that the <span>participants</span>  <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>would be from the United States, but at least two of them were from other countries, as revealed by the questions they asked. The <span>participants</span>  <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>greatly varied in their occupations including student, banker, bus driver, <span>computer</span>  <span class="compus" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>programmer, graphic designer, aerospace engineer, hair stylist, homemaker, unemployed, and more.</p>

<table width="40%" border="1" cellspacing="0" cellpadding="3" align="center" style="border-right: #99f5fb solid; border-top: #99f5fb solid; font-size: smaller; border-left: #99f5fb solid; border-bottom: #99f5fb solid; font-style: normal; font-family: verdana, geneva, arial, helvetica, sans-serif; background-color: #fdffdd"> <caption align="bottom"><br /><strong>Table 1: <span>Interview</span>  <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>participant demographics (n=36) </strong><br/>(Note: % rounded up to the nearest whole number throughout this paper, 
so total percentages may sometimes equal more than 100 %. ) </caption> 

<tr><th>Demographic characteristic</th>	<th>N</th>	<th>%</th>	</tr>
<tr><th>Sex</th>  <th>&nbsp;</th> <th>&nbsp;</th> </tr>
<tr><td>Female</td><td align="center">	10</td><td align="center">	28</td>	 </tr>
<tr><td>Male</td><td align="center">	26</td><td align="center">	72</td>	</tr>
<tr><th>Ages</th>  <th>&nbsp;</th> <th>&nbsp;</th></tr>
<tr><td>   18-19</td>	<td align="center">2</td>  <td align="center">	6</td>	</tr>
<tr><td>20-29</td>      <td align="center"> 9</td><td align="center">	25</td>	</tr>
<tr><td>30-39</td><td align="center">	11</td><td align="center">	31</td>	</tr>
<tr><td>40-49</td><td align="center">	7</td><td align="center">	19</td>	</tr>
<tr><td>50-59</td><td align="center">	4</td><td align="center">	11</td>	</tr>
<tr><td>60-69</td><td align="center">   3</td><td align="center">8</td> </tr>
</table> 

<p>Regarding the experience with Yahoo! Answers, most <span>participants</span>  <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>(78%, n=28) had used the site for over a year as of the time of interviewing. With respect to the frequency of using the site, two-thirds of the <span>participants</span>  <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>(67%, n=24) reported using the site at least three to four times a week while the rest (33%, n=12) were occasional users. Con0ering the <span>participants</span>' <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span> long experience with the site and the frequency of use, a majority of the <span>participants</span>  <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>were experts accustomed to using various features of the site. In addition, most of the questioners were familiar with the topics of their questions (78%, n=28), but only a small number of them had urgent information needs (22%, n=8).   </p>

<b>Motivations to use Yahoo! Answers </b>

<p>The analysis found that thirteen questioners (36%) asked conversational questions and twenty-three (64%) asked informational questions. The type of question the <span>participants</span>  <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>asked was closely tied to their motivation for using Yahoo! Answers. </p>

<p>For those questioners who asked conversational questions, Yahoo! Answers was a natural <span>choice</span>  <span class="conce" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>because they '<i>could get answers from millions of real people</i>' (Participant 11 (P11)) and answerers were believed to have '<i>an ability to answer as openly as possible</i>' (P16). While three out of the thirteen questioners did a pre-search for background information, most did not feel a need to search information prior to a discussion. Put differently, when the questioners wanted to initiate a discussion or do a poll on a specific issue, they usually went to Yahoo! Answers directly without consulting other sources. In this case, the credibility of the site was not an important con0eration. Instead, the questioners sought tools that allowed them to participate in conversation, as one <span>participant</span>  <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>prioritized the social interaction taking place in the site over its trustworthiness:</p>

<blockquote> 'It's not about trustworthy at all. I like when people notice my question and answer it, 
and in the end make me smile, cheer me up.' (P12)</blockquote> 

<p>The other <span>participants</span>  <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>asked informational questions to look for solutions to problems at hand, to expand knowledge on a topic, or to find a fact. As opposed to those who asked conversational questions, most of this group searched information before coming to Yahoo! Answers, mainly using the Web or interpersonal sources. For example, a college student working part-time in technical support services at his college was facing difficulty in accessing his local network from one <span>computer</span>. <span class="compus" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span> After performing many Internet searches, calling the <span>hardware</span>  <span class="compus" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>manufacturer, and consulting a few of his colleagues at work, he went to Yahoo! Answers and finally found what the problem was. While this example demonstrates the use of Yahoo! Answers as the last resort when searches fail with other sources, several questioners used the site to confirm the information they gathered from other sources, as P13 wanted to check if the information that his mechanic gave to him was real and could be trusted. </p>

<p>One <span>participant</span>  <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>illustrated the type of questions that can be better addressed by a social questions and answers site than a general Web search engine. When he found a spider in his bedroom, he took a picture of it and posted a link to the picture in the site to identify what kind of spider it was: </p>

<blockquote> 'Search engines like Yahoo! and Google, and whatever in words, but in picture, it won't search the pictures for it. So I figured the only way would be to ask a person. So when I ask Yahoo!, I can ask the people.' (P34)</blockquote> 

<p>In a nutshell, the questioners came to the site because of its abilities to deal with difficult questions calling for discussion, personal advice, suggestion, or other information that cannot be easily answered by a traditional Web search engine. Moreover, previous positive experience with Yahoo! Answers raised its perceived credibility and motivated the questioners to use it as an information source. Seven questioners made comments such as:</p>

<blockquote>'Whenever I ask a question on here, I get great results that I can trust.' (P6)</blockquote>

<p>It is evident that the perceived credibility of the site influences one's decision to use it. It is not always the first and foremost factor, however, because questions may not require credible information. For example, the ability to compare a wide <span>spectrum</span>  <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>of others' opinions can come first before the credibility issue, depending on the <span>goal</span>  <span class="compus" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>of asking a question.</p>

<p>The next section examines the questioners' opinions on the credibility of the site in more detail. </p>

<b>Overall credibility of the site</b>

<p>With respect to the overall credibility of the site, one third of the questioners (31%, n=11) gave highly favorable evaluation to the site as P30 states:</p>

<blockquote>'[the site is] trustworthy enough, since people by and large come here to have a good time, their intentions are good and they may be sarcastic but mean no harm.' </blockquote>

<p>As opposed to these questioners who trusted the goodwill of the people giving them answers, ten questioners (28%) were skeptical about the competencies of Yahoo! Answers' users as information providers:</p>

<blockquote>'Who are motivated by bias, a lot motivated by hate, many just exhibiting ignorance of the topic, they have this air of immaturity. A lot of people are answering questions just for the points, not because they know anything about what's being asked.' (P33) </blockquote>

<p>They also reported that biased and hateful users were abundant particularly in politics, religion, and global warming <span>categories</span>  <span class="conce" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>where opinion was particularly <span>divided</span>. <span class="quanti" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span> Accordingly, the nature of the subject <span>category</span>  <span class="conce" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>a questioner has been active in strongly influences his evaluation of the entire site; for example, a favourable evaluation by a <span>participant</span>  <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>who asked a hunting question may have resulted from experience with the less <span>controversial</span>  <span class="quali" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>topic shared among like-minded people who have the same hobby in the Hunting <span>category</span>. <span class="conce" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span> </p>

<p>Another group of fifteen questioners (42%) expressed more caution in judging the overall credibility of the site stating that each question should be rated individually since it all depends on what and how a person answers. If a questioner can find a serious person who is willing to take time to research and explain his answer along with a source, the information is very likely to be credible. A problem is that the chances of encountering such a competent answerer are just about '<i>as if you had asked someone standing next to [sic] in line somewhere in public</i>.' (P8) Some questioners pointed out the potential limits to the utility of the site as a serious source:</p>

<blockquote>'I don't think Yahoo! Answers, they don't have the capability as of yet of being a reputable scholarly source, and I would not use them as such.' (P2) </blockquote>

<blockquote>'In general, it is good for small questions? like when you plant vegetables in your garden, or the name of a movie you can't quite recall. But I would never recommend it for medical, marital, or financial advice.' (P35). </blockquote>

<p>What is notable here is that most of the questioners who gave negative evaluation to the site asked conversational questions. They do not see the site as a credible place for getting questions answered accurately because people often present their opinions rather than hard facts: </p>

<blockquote>'Yahoo! Answers should be more aptly named Yahoo! Opinion because normal responses only convey what an individual thinks or feels on the subject.' (P18)</blockquote>

<p>On the other hand, those who gave a favorable evaluation mostly asked informational questions, advocating asking factual questions in the site:</p>

<blockquote>'For factual information, I would say it's really good. I use it all the time. I would say it's about 99 percent accurate. For opinions, it depends on the <span>category</span>. <span class="conce" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>' (P34) </blockquote>

<p>To conclude, individual questioners have formed a perception about the overall credibility of the site and according to the perception, they ask the type of question that they think is more suited to the site.</p>

<b>Credibility criteria for specific questions </b>

<p>Not all questioners evaluated the credibility of answers given to their recent questions. Four questioners (11%) did not do so because the questions were supposed to get emotional support or to make people laugh or because the answers did not provide substantial information to evaluate. When other questioners evaluated credibility, they did not always evaluate all answers. Especially when the number of given answers was high (the range of the number of answers received by each <span>participant</span>  <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>was between on and thirty-seven with two-thirds of the questions getting fewer than ten answers), the questioners focused on a small number of answers that got their attention during initial scanning to examine them more carefully. </p>

<p>Furthermore, the questioners did not apply the same set of criteria to every answer in an equal manner. Instead, they noticed certain salient attributes associated with each answer and made a judgment about them, which confirms Prominence-Interpretation Theory (<a href="#fog03">Fogg 2003</a>). 
For example, a questioner who asked about travelling in a foreign country received two answers. One answerer claimed to live in the country and gave a very grumpy answer while the other answerer said he had travelled there and linked to other sites that the questioner had heard of. The questioner effortlessly judged the second answer to be more credible because of '<i>the references to other sites that have good credentials</i>' (P3). He also mentioned that the first answerer's tone of writing negatively influenced his credibility perception despite the positive self-claimed expertise and qualifications.</p>

<p>Table 2 lists the credibility criteria the questioners used together with the frequency of use (when a questioner applied a single criterion to multiple answers, it was counted as one). In total, twenty-two criteria were identified and they were grouped into three categories: message criteria, source criteria, and others. The questioners used each criterion either positively or negatively or both in credibility judgments. For example, the fact criterion was used both positively and negatively; a factual assertion made in the answer to a discussion question positively impacted credibility judgment while a lack of fact-based information resulted in a negative credibility judgment. Con0ering the limitation of the data collection method and the fact that the average number of criteria mentioned per questioner is 2.6, the <span>participants</span>  <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>might not have remembered all the criteria they used during the credibility judgment process. Nonetheless, this list of criteria gives insights into the most salient attributes of answers users notice and evaluate in a social questions and answers site. </p>


<table width="70%" border="1" cellspacing="0" cellpadding="3" align="center" style="border-right: #99f5fb solid; border-top: #99f5fb solid; font-size: smaller; border-left: #99f5fb solid; border-bottom: #99f5fb solid; font-style: normal; font-family: verdana, geneva, arial, helvetica, sans-serif; background-color: #fdffdd"> 
<caption align="bottom"><br /><strong>Table 2. Frequency of use of credibility criteria</strong><br/></caption> 

<tr><th colspan="2">Criteria</th>	<th>Positive use</th>	<th>Negative use</th>	<th>Total</th></tr>	
<tr><td rowspan="13"><b>Message criteria</b></td> <td>Accuracy</td> <td align="center"> 4</td> <td align="center"> 1</td> <td align="center"> 5</td> </tr>
 <tr>  <td>Clarity </td> <td align="center"> 3</td> <td align="center"> 1</td> <td align="center"> 4</td></tr>
<tr>  <td>Completeness </td> <td align="center"> 2</td> <td align="center"> 0</td> <td align="center"> 2</td> </tr>
<tr>  <td>Detail  </td> <td align="center"> 3</td> <td align="center"> 0</td> <td align="center"> 3</td></tr>
<tr>  <td>Fact  </td> <td align="center"> 1</td> <td align="center"> 1</td> <td align="center"> 2</td></tr> 
<tr>  <td>Layout </td> <td align="center"> 6</td> <td align="center"> 0</td> <td align="center"> 6</td></tr> 
<tr>  <td>Length </td> <td align="center"> 0</td> <td align="center"> 2</td> <td align="center"> 2</td></tr> 
<tr>  <td>Logic  </td> <td align="center"> 8</td> <td align="center"> 1</td> <td align="center"> 9</td></tr> 
<tr>  <td>Novelty  </td> <td align="center"> 0</td> <td align="center"> 1</td> <td align="center"> 1</td> </tr>
<tr>  <td>Spelling and grammar</td> <td align="center">7</td> <td align="center"> 0</td> <td align="center"> 7</td> </tr>
<tr>  <td>Tone of writing </td> <td align="center"> 3</td> <td align="center"> 1</td> <td align="center"> 4</td> </tr>
<tr>  <td>Topicality  </td> <td align="center"> 1</td> <td align="center"> 2</td> <td align="center"> 3</td> </tr>
<tr>  <td><b>Subtotal</b> </td> <td align="center"> <b>38</b></td> <td align="center"> <b>1</b></td> <td align="center"> <b>4</b></td> </tr>
<tr>  <th rowspan="7"><b>Source criteria</b></th><td>Answerer's attitude </td> <td align="center"> 1</td> <td align="center"> 2</td> <td align="center"> 3</td> </tr>
<tr>  <td>Known answerer  </td> <td align="center"> 3</td> <td align="center"> 0</td> <td align="center"> 3</td> </tr>
<tr>  <td>Perceived expertise based on the answer</td> <td align="center"> 4</td> <td align="center"> 1</td> <td align="center"> 5</td> </tr>
<tr>  <td>Perceived expertise based on an answerer's profile </td> <td align="center"> 8</td> <td align="center"> 0</td> <td align="center"> 8</td> </tr>
<tr>  <td>Reference to external sources  </td> <td align="center"> 4</td> <td align="center">0</td> <td align="center"> 4</td> </tr>
<tr>  <td>Self-claimed expertise or qualification  </td> <td align="center">4</td> <td align="center"> 0</td> <td align="center"> 4</td> </tr>
<tr>  <td><b>Subtotal</b> </td> <td align="center"> <b>24</b></td> <td align="center"> <b>3</b></td> <td align="center"> <b>27</b></td> </tr>
<tr>  <th rowspan="4"><b>Others</b></th><td>Ratings on the answer </td> <td align="center"> 2</td> <td align="center"> 0</td> <td align="center"> 2</td> </tr>
<tr>  <td>Usefulness</td> <td align="center"> 2</td> <td align="center">0</td> <td align="center"> 2</td> </tr>
<tr>  <td>Verifiability </td> <td align="center"> 7</td> <td align="center"> 2</td> <td align="center"> 9</td> </tr>
<tr>  <td><b>Subtotal</b> </td> <td align="center"> <b>11</b></td> <td align="center"><b>2</b></td> <td align="center"> <b>13</b></td> </tr>
<tr> <th>&nbsp; </th> <td><b>Total</b> </td><td align="center"><b>73</b></td>
<td align="center"><b>15</b></td><td align="center"><b>88</b></td> </tr> 
</table> 


<p>Out of the total 88 times of using credibility criteria, 48 times (55%) were related to message criteria. 
The questioners con0ered both the content-related criteria such as accuracy and completeness and the presentation-related criteria such as layout,  spelling and grammar. <span>Logic</span>  <span class="conce" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>or plausibility of arguments was the most frequently used criterion in this <span>category</span>  <span class="conce" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>followed by spelling/grammar. While seven questioners thought using basic grammar and writing skills reflected a more intelligent and knowledgeable answer, several said that clerical errors did not bother them as long as they could understand what was written.</p>

<p>The questioners also evaluated source credibility twenty-eight times (32%). In the absence of institutional-level sources and author affiliation information, an answerer's profile turned out to be the most frequently consulted information about one's credentials because it provides the <span>history</span>  <span class="conce" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>of answers including the best answer rating:</p>

<blockquote>'He [the answerer] had a 34% Best Answer rating after answering 2,610 questions over roughly the last three years. His predominant <span>category</span>  <span class="conce" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>of answers fell into the military genre.' (P18) 
</blockquote>

<p>The questioners also gauged the answerer's expertise by examining the content of the answer or the answerer's self-claimed expertise: </p>

<blockquote>'She sounds like she knows what she's talking about.' (P2)</blockquote>

<blockquote>'She lives in the area I asked about and studies regional <span>history</span>, <span class="conce" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span> which helps.' (P26)</blockquote>

<p>Like the trustworthiness of the entire site, the self-claimed expertise drew a wide range of views from extremely cynical to unconditionally trusting. Some questioners were in-between the two ends of the <span>spectrum</span>, <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span> indicating that their judgments would depend on the type of expert: </p>

<blockquote>'I wouldn't believe anyone who said they were a doctor or lawyer on Answers for free, no. I do believe people who say, "Used to work in a medical office or law office". I have no reason to doubt them.' (P28) </blockquote>

<blockquote>'You can generally tell by the wording of the answers with most disciplines. For instance, a doctor will never use the word "crazy".' (P22) 
</blockquote>

<p>Furthermore, answerers who proved themselves knowledgeable and competent in a specific topic <span>category</span>  <span class="conce" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>over time earned the perception of strong credibility with the questioners. Recognition of such a known answerer influenced credibility perception positively without fail: </p>

<blockquote>'The answerer has answered my questions before and has been chosen best answer.' (P10)</blockquote>

<p>In addition to the perceived expertise of an answerer, a reference citation was an important clue in judging the credibility of information. References to external sources, mostly links to other Websites, are known to serve either as tools for central processing of information or as cues for peripheral processing (<a href="#fre">Freeman and Spyridakis 2004</a>).  Some questioners were engaged in peripheral processing by simply noting the presence of links to other sites and coming to the conclusion that the answer was credible. Others were engaged in central processing by following links to gain additional information to verify the answer with information from another Website. In the study, the former was coded as 'reference to external sources' and the latter as 'verifiability.' When quoted Websites did not coincide with what the answerer said, the credibility of the answer was damaged because it was not verifiable. </p>

<p>Two questioners looked at the ratings on answers given by the members of the community with a belief that the collective decision could be superior to the smartest individual. This criterion is notable because the questioners took advantage of the nature of the social questions and answers site by relying on fellow users' decision making. </p>

<p>An answerer's attitude also influenced credibility. An answerer who exhibited a sense of humour, politeness, or emotional support was regarded as a credible source. In addition to 'ratings on the answer,' this criterion shows social interaction taking place in the community.  </p>

<p>The type of question influenced the credibility criteria the <span>participants</span>  <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>used. Not surprisingly, the content-related criteria such as <span>logic</span>  <span class="conce" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>and accuracy as well as verifiability were important for informational questions (Table 3). For conversational questions, accuracy of spelling and grammar was the most frequently used criterion.  Although conversational questions are asked to initiate a discussion and are not expected to have one correct answer, the questioners who asked conversational questions sometimes checked spelling/grammar, <span>logic</span>, <span class="conce" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span> and other criteria to assess the credibility of back-up information that supported one's argument in the answers.</p>

<table width="50%" border="1" cellspacing="0" cellpadding="3" align="center" style="border-right: #99f5fb solid; border-top: #99f5fb solid; font-size: smaller; border-left: #99f5fb solid; border-bottom: #99f5fb solid; font-style: normal; font-family: verdana, geneva, arial, helvetica, sans-serif; background-color: #fdffdd"> 
<caption align="bottom"><br /><strong>Table 3: Most popular credibility criteria by type of questions</strong><br/>(Note: % rounded up to the nearest whole number throughout this paper, <br />
so total percentages may sometimes equal more than 100%.)</caption> 

<tr><th>Type of questions</th>	<th>Criteria</th>	<th>Frequency of use</th></tr>	
<tr><th rowspan="4"><b>Conversational questions</b></th> <td>	Spelling/grammar</td> <td align="center"> 4</td> </tr>
 <tr>  <td>   Layout</td> <td align="center">	3</td>	 </tr>
<tr><td>	Logic</td> <td align="center">	3</td>	</tr>
 <tr><td>	Perceived expertise based on an answerer's profile</td><td align="center">3</td>	</tr>
 <tr> <th rowspan="4"> <b> Informational questions</b> </th><td>Logic </td><td align="center">	6</td></tr>		
 <tr><td>Verifiability </td><td align="center">6</td>	</tr>
 <tr><td>	Perceived expertise based on an answerer's profile</td><td align="center">	5</td>	</tr>
 <tr> <td> Accuracy</td><td align="center">5</td> </tr> 
</table> 

<b>Credibility criteria in general </b>

<p>When asked to list up to three most important credibility criteria in general, the questioners repeated a majority of the criteria they used for their recent questions (Table 4). What is notable here, however, is that as many as fourteen questioners mentioned '<i>references to external sources</i>'. Knowing that only four questioners actually used that criterion for their specific questions, it is speculated that the questioners con0er reference citations critical clues, but the unavailability of citations in the answers prevented them from using them.</p>


<table width="60%" border="1" cellspacing="0" cellpadding="3" align="center" style="border-right: #99f5fb solid; border-top: #99f5fb solid; font-size: smaller; border-left: #99f5fb solid; border-bottom: #99f5fb solid; font-style: normal; font-family: verdana, geneva, arial, helvetica, sans-serif; background-color: #fdffdd"> 
<caption align="bottom"><br/><strong>Table 4. Most important credibility criteria in general </strong><br/></caption> 

<tr> <th colspan="2">Criteria</th>	<th>Frequency of mentions</th>	</tr>
<tr> <th rowspan="12"><b>Message criteria</b></th> <td>Accuracy</td> <td align="center">2</td> </tr>
 <tr>  <td>Clarity </td> <td align="center"> 4</td> </tr>
<tr>  <td>Completeness </td> <td align="center"> 3</td> </tr>
<tr>  <td>Detail  </td> <td align="center"> 4</td> </tr>
<tr>  <td>Fact  </td> <td align="center"> 2</td> </tr>
<tr>  <td>Layout </td> <td align="center"> 4</td> </tr>
<tr>  <td>Length </td> <td align="center"> 2</td> </tr>
<tr>  <td>Logic  </td> <td align="center"> 12</td> </tr>
<tr>  <td>Spelling/grammar</td> <td align="center">9</td> </tr>
<tr>  <td>Tone of writing </td> <td align="center"> 6</td> </tr>
<tr>  <td>Topicality  </td> <td align="center"> 3</td> </tr>
<tr>  <td><b>Subtotal</b> </td> <td align="center"> <b>50</b></td> </tr>
<tr>  <th rowspan="7"><b>Source criteria</b></th><td>Answerer's attitude </td> <td align="center"> 2</td> </tr>
<tr>  <td>Honesty</td> <td align="center"> 3</td> </tr>
<tr>  <td>Perceived expertise based on the answer</td> <td align="center"> 3</td> </tr>
<tr>  <td>Perceived expertise based on an answerer's profile </td> <td align="center"> 6</td> </tr>
<tr>  <td>Reference to external sources  </td> <td align="center"> 14</td> </tr>
<tr>  <td>Self-claimed expertise/qualification  </td> <td align="center">3</td></tr> 
<tr>  <td><b>Subtotal</b> </td> <td align="center"> <b>32</b></td> </tr>
<tr>  <th rowspan="3"><b>Others</b></th><td>Usefulness </td> <td align="center"> 7</td> </tr>
<tr>  <td>Verifiability </td> <td align="center"> 4</td> </tr>
<tr>  <td><b>Subtotal</b> </td> <td align="center"> 11</td> </tr>
<tr> <th>&nbsp;</th> <td><b>Total</b> </td><td align="center"><b>93</b></td></tr> 
</table> 



<p>The criterion that the questioners think is important in general, but fail to use for their specific questions, is honesty. Honesty, which is equivalent to trustworthiness, has been treated as an essential component of credibility constructs together with expertise. A potential reason for not evaluating honesty for the specific questions could be a difficulty in assessing an anonymous answerer's willingness to answer honestly in the social questions and answers environment. </p>

<p>Novelty, known answerer, and ratings on answers were used for evaluating the specific questions, but not regarded as important in general. They might be secondary to major criteria such as reference to external sources. </p>

<p>These findings illustrate that the <span>participants</span>' <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span> ability to use various credibility cues is closely tied to the availability of those cues in the evaluation environment. Therefore, the actual criteria that the <span>participants</span>  <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>use differ from the ideal criteria that they should use generally to determine credibility.</p>

<p><b>Verification </b></p>

<p>Twelve questioners (33%) reported that they verified information with external sources. Ten of them did it when first encountering answers in the site as a part of credibility judgment by continuing searches until they were satisfied that the given answers were correct. Only two questioners verified the information later when they came to doubt the credibility of the information while using the information. The verification rate increases to 43% with informational questions (ten out of twenty-three questions) and decreases to 15% with conversational questions (two out of thirteen questions). Put in another way, those who asked informational questions tended to verify information more than those who asked conversational questions because their intent was to obtain facts. The verification rate for informational questions in the study was higher than expected compared to the rare or occasional verification behaviours reported in previous research (e.g., <a href="#fla00">Flanagin and Metzger 2000</a>).</p> 

<p>A potential <span>explanation</span>  <span class="conce" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>for this is that the use of the self-report method in this study led the questioners to say they verified the information more than they actually did due to the social desirability effects. An alternative interpretation is that the presence of Website links embedded in the answers accelerated their verification behaviour, as evidenced by five questioners who followed up on the suggested links right away. While clicking a given link is quick and easy to perform, seven other questioners did more laborious work by using search engines or by consulting a Website they were already familiar with to cross-check consistency of the given information. For example, P7, looking into a new cat litter box, wanted people's experience with new technologies for cat litter boxes. After getting a viable answer in Yahoo! Answers, he checked out the Webpage of the product and several consumer sites that rate products to finally arrive at credibility judgment. </p>

<p>More interesting findings may come from those who did not verify the information with other sources. The major reasons the questioners gave for not verifying information include:  </p>

<blockquote><ol>
  <li> 'It's a matter of opinion so I'm sure that it's the opinion of the answerer. It doesn't mean it's necessarily correct or incorrect.' (P32) </li>
  <li> 'I didn't feel I had obtained any information from the site.' (P14)</li>
  <li> 'If I have to answer or ask a question, it's because I have no other resource for gathering that information. I exhausted all of my other resources.' (P9) </li>
  <li> 'I wouldn't check twice. I checked it once before.' (P34) </li>
  <li> 'Because as I said it's a topic I'm very familiar with anyway.' (P33)</li> 
  <li> '[The answer] didn't provide site links.' (P26)</li>
  <li> 'Only one person provided any information relevant to my question.' (P4) </li>
  <li> 'I simply didn't have time.' (P13)</li>
</ol></blockquote>

<p>In (1) and (2), the questioners thought that verification was unnecessary because they were seeking opinions instead of facts, or they did not obtain substantial information to verify. Reasons (3) and (4) reveal the close relationship between pre-search activities and verification behaviour: when a questioner had searched the information in advance somewhere else and the site was used as the last resort for confirmation, he or she tended not to verify the information once again. This finding indicates that verification behaviour and its links to the broader subject of information seeking in a social questions and answers site should be examined in the context of an overall information seeking process. In (5), the questioner did not feel a need to verify the answer with other sources because he was confident in his own abilities to understand and evaluate the answer. This shows that verification can be achieved by relying exclusively on personal knowledge/experience without referring to external sources or trusted people. Reason (6) illustrates some questioners' penchant to rely on easy-to-perform verification behaviour (e.g., click on a link) over those requiring additional action (e.g., leave Yahoo! Answers and search Google). This finding echoes the principle of least effort suggested by Zipf (<a href="#zip">1949</a>) although there were some questioners who made more aggressive effort to verify the information. In (7), since there was only one relevant answer, the questioner decided to accept the information anyway without further verification. Reason (8) shifts our attention from personal-level attributes to the contextual factors, more specifically, the influence of urgency on verification behaviour. </p>

<p>Taken together, verification behavior are an interplay of many factors including context factors (e.g., time constraint, the number of answers given), questioner-level attributes (e.g., knowledge level), pre-searches, and the presence of external links in answers although it is unclear how they interact or which factors are stronger than others.   </p>

<h2>Discussion</h2>

<p>The findings of the study clearly show that credibility judgments in a social questions and answers site are better understood in a broader context of an information seeking process because it is closely connected to the selection decision for the site, pre-search activities, and post-search verification behaviours. When deciding whether to use Yahoo! Answers, the perceived credibility of the site is one aspect users con0er, although it is not necessarily a determining factor. Even those who do not give great credence to the site still use it to collect first-hand accounts from other people who had a similar problem or to find information that is not easily retrieved by a traditional Web search engine. Put differently, the credibility of a social questions and answers site as a medium is transferred to individual answers in that site. However, although users know they will get anonymous answerers' opinions rather than objective facts and thus, perceive the site as non-credible, they have other good reasons be0es credibility to use the site (e.g., to enjoy social interaction).</p>

<p>For the evaluation of the credibility of individual answers, the questioners applied a variety of criteria related to message, source, or others. A notable finding is that the questioners evaluated message credibility more frequently than source credibility. This finding contrasts with previous research showing that source <span>characteristics</span>  <span class="quali" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>are the <span>primary</span>  <span class="quali" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>criteria people use when making judgments on information <span>quality</span>  <span class="quali" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>(e.g., <a href="#rieb08">Rieh and Belkin 1998</a>, <a href="#rieb00">2000</a>; <a href="#rie">Rieh 2002</a>). The frequent use of message criteria in this study can be explained by two reasons. First, it is attributed to the high rate of the absence of source information in the answers. According to Oh <i>et al.</i> (<a href="#oh">2008</a>), less than 8% of answers in Yahoo! Answers include information about a source from which the answer was derived. Consistent with Slater and Rouner's (<a href="#sla">1996</a>) finding, the questioners used message attributes to ensure credibility in the absence of source information. Alternatively, it might be due to the high level of familiarity the questioners had with the topics of their questions. Experts who are equipped with sufficient knowledge can make informed decisions on credibility by paying more attention to message content than to other attributes (<a href="#sta">Stanford <i>et al.</i> 2002</a>). Unfortunately, with a small number of the questioners who were unfamiliar with their topics, the study could not systematically uncover the relationship between the level of knowledge and credibility judgments. </p>

<p>Moreover, the message-related criteria identified in this study, such as accuracy, clarity, completeness, spelling/grammar, tone of writing, and layout, con0erably overlap with those from earlier studies (e.g., <a href="#eys">Eysenbach and Kohlelr 2002</a>; <a href="#rie">Rieh 2002</a>; <a href="#fogall03">Fogg <em>et al.</em>  2003</a>; <a href="#liu">Liu 2004</a>), implying that there is a set of core message criteria used in evaluating Web information across contexts. On the other hand, 'topicality' was not found in previous research. It might have been presupposed that topicality is already met when <span>participants</span>  <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>assess the credibility of a Website. In Yahoo! Answers, however, answerers sometimes go off the topic of a question, so sticking to the topic is important for ensuring a credible answer.   </p>

<p>Although message credibility was frequently evaluated, the questioners were also aware of the fact that answerers are not information specialists who are bound by the standards of professional training and <span>ethics</span>, <span class="quali" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span> and thus, source credibility should be carefully scrutinized as well. As in other credibility studies (e.g., <a href="#eys">Eysenbach and Kohlelr 2002</a>; <a href="#rie">Rieh 2002</a>; 

<a href="#fogall03">Fogg <em>et al.</em>  2003</a>; <a href="#liu">Liu 2004</a>), an author's expertise, known answerer, and links/references were important source criteria in Yahoo! Answers although the <span>participants</span>  <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>had to identify available cues that replace those for Websites. For example, while 'an author's publications in a subject area' was useful for assessing a scholarly author's expertise in Liu (<a href="#liu">2004</a>), in Yahoo! Answers, 'an answerer's profile' was used to gauge an answerer's expertise in a topic <span>category</span>  <span class="conce" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>based on the number of answers posted by the answerer and the best answer rating in that <span>category</span>. <span class="conce" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span> A picture of the site owner (<a href="#eys">Eysenbach and Kohlelr 2002</a>), a source's affiliation (<a href="#fogall03">Fogg <em>et al.</em>  2003</a>; <a href="#liu">Liu 2004</a>), and other criteria associated with a source's <span>identity</span>  <span class="conce" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>(e.g., contact information) found in previous research were not used as credibility cues in Yahoo! Answers because the <span>identity</span>  <span class="conce" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>of answerers are usually hidden in the site unless users opt to make their personal information public. </p>

<p>A notable source criterion, which is not found in earlier Web credibility research, is 'answerer's attitude.' As opposed to a typical credibility judgment situation where a user interacts with a Website, this criterion points out the social aspect of Yahoo! Answers where people interact with other people through the question and answering process. As more animated, poised, and good-natured speakers were judged to be higher in credibility in interpersonal communication (<a href="#metall">Metzger, Flanagin, Eyal, Lemus and McCann 2003</a>), answerers who were humorous, polite, and provided emotional support were regarded as more credible in this study. In short, this criterion is evidence that similarities exist between interpersonal credibility and social questions and answers credibility.</p>

<p>Among all source criteria, 'answerer's profile' was the most frequently used criterion to gauge the expertise of an anonymous and potentially unqualified answerer. What should be mentioned here is that there is a gap between the criteria perceived important and the criteria actually used. Whilst 'answerer's profile' was most frequently used, 'reference to external sources' was perceived as most important. 
This contradiction is due to the frequent unavailability of references, which prevented the questioners from using the most important criterion. After all, people's ability to use various credibility cues is constrained by the availability of those cues in the evaluation environment.</p>

<p>Another criterion that highlights the social interaction taking place in the site is 'ratings on the answer.' The questioners drew on consensus decision-making using the nature of the collective intelligence that emerges from the collaboration in the social questions and answers site. This confirms that social media is shifting the paradigm of credibility assessment from widely shared standards among established authorities to bottom-up assessments through collective or community efforts (e.g., ratings and reputation systems) (<a href="#fla08b">Flanagin and Metzger 2008b</a>).     </p>

<p>While many source and message criteria from previous research translate to the social questions and answers site environment, website-related criteria did not apply here because the questioners evaluated the answers in the same site. For example, the visual design of the site, which is a popular criterion in earlier studies (e.g., <a href="#fogall03">Fogg <em>et al.</em>  2003</a>), could not be used as a credibility cue to assess the credibility of each answer. </p>

<h2>Implications</h2>

<p>Theoretically, this study extends earlier credibility research by examining a new environment, a social questions and answers site. where users evaluate individual answers in the same site instead of evaluating individual Websites. </p>

<p>This study links pre-search activities, credibility judgments in the site, and post-search verification behaviours as a continuous credibility judgment process. This process is not <span>linear</span>  <span class="quanti" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>because users might not go through all stages. Users can skip the pre-search stage and go directly to a social questions and answers site, or do pre-searches but skip the verification stages afterwards. In the first stage, previous experience with the site influences the perceived credibility of the site and the perceived credibility influences the type of question to ask. Subsequently, the type of question influences evaluation of individual answers. Reversely, the outcome of evaluation of individual answers reinforces or challenges the perceived credibility of the site. Since the relationships among the stages and factors influencing each stage are tentative, the credibility judgment process and the factors identified in this study need verification by additional research involving a large number of users in multiple social question and answer sites.</p>

<p>When it comes to specific credibility criteria, this study indicates that there is a set of relatively consistent credibility cues across Web contexts. Message criteria such as accuracy, clarity, completeness, spelling/grammar, tone of writing, and layout/organization remain important regardless of contexts. With respect to source criteria, a source's expertise, known answerer, and links/references are consistently important although people may use different cues specific to a context. Website-related criteria such as the visual design of the site and functionality are not used in a social questions and answers site because questioners evaluate individual answers in the same site. In addition to identifying a core set of credibility criteria on the Web, this study directs attention to the social and collaborative nature of communication in a social questions and answers site; an answerer's attitude shown through a communication process and the community members' ratings on an answer influence credibility judgments in the site.</p>

<p>In summary, as social question and answer sites are a specific type of Website, many criteria from previous Web credibility research transfer to the social questions and answers environment. Interpersonal traits identified in traditional source credibility literature apply to this environment as well due to the social interaction occurring therein.  </p>

<p>Practically, the findings of the study have implications for improving the design of the social questions and answers site. First, the point system of the site was developed to facilitate the exchange of high <span>quality</span>  <span class="quali" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>information, but it does not do enough to deter users from posting low-quality answers (<a href="#nam">Nam <i>et al.</i> 2009</a>) because it gives an incentive to answer as many questions as possible without con0ering the <span>quality</span>  <span class="quali" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>of information. This study suggests revising the point system to give more rewards to those who cite references because references are regarded as the most critical credibility clue. Secondly, the site can develop a search <span>algorithm</span>  <span class="quanti" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>that incorporates the credibility criteria the questioners con0er important. The site could rank answers containing no <span>grammatical</span>  <span class="quali" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>errors, with external references, or posted by reputable answerers higher in the search result. The data also point to the need for an <span>algorithm</span>  <span class="quanti" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>capable of analysing the type of question and returning the most credible answers accordingly. </p>

<p>Overall, many questioners in the study exhibited critical appraisal skills at least to some extent. They share the same concern with researchers on the <span>quality</span>  <span class="quali" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>of information presented in the social questions and answers site and thus have developed strategies specific to the environment (e.g., look up an answerer's profile). However, some questioners have extremely positive or negative attitudes toward the credibility of the site, which may be barriers to making appropriate credibility judgments based on accurate analysis of individual answers. As the most widely discussed implication of credibility research is user instruction, this finding also proposes that information professionals and librarians should teach users how to effectively evaluate information given by lay information providers in a social questions and answers site and how to verify the information with more reliable sources. However, teaching users how to identify credible information is not the only way information professionals can address the problem of information credibility in a social questions and answers context. Another is to help users become competent answerers to provide credible information. Although traditional user instruction has revolved around the notion of users as information seekers, it is time to think about the <span>idea</span>  <span class="conce" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>of users as information creators and providers. The findings of the study can contribute to developing a user instruction program or guideline to help answerers understand what criteria questioners con0er important when filtering out credible information and how to prepare credible answers in a social questions and answers context.</p>

<h2>Future research</h2>

<p>The biggest limitation of this study lies in its self-report research method. As a way to explore users' experience of credibility judgments in the novel environment, the research method produced fruitful findings, but it may have caused potential response bias problems as people know they should critically analyse the information they obtain online (<a href="#met">Metzger 2007</a>). To minimize the bias, the <span>interviews</span>  <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>used the critical incident technique, which allowed the <span>participants</span>  <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>to focus on a recent specific episode rather than generalizing their opinions. Nonetheless, the social desirability effects may have come into play, especially in answering verification behaviour in the data. Therefore, further research is required to confirm how users actually verify information obtained through a social question and answer site through a direct, unobtrusive research method in a natural setting.</p>

<p>Also, with the data, it was sometimes unclear whether the questioners con0ered the mentioned criteria by ignoring other attributes available in the site, or because of the absence of those attributes. Content analysis of the questions and associated answers together, through a direct, unobtrusive research method would reveal this <span>phenomenon</span>  <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>more clearly.</p>

<p>Finally, further research should delve into the influence of questioners' motivation, ability, and other individual <span>characteristics</span>  <span class="quali" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>on credibility judgment effort. Since most <span>participants</span>  <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>had prior knowledge about their topics and non-urgent information needs in the study, it was not possible to examine the influence of urgent need and low familiarity with a topic on credibility judgments. </p>

<p>Another serious limitation of this study is the small, non-random sample. The findings cannot be generalized to the general public or even to the population of Yahoo! Answers' users because the sample size is very small and there may be discrepancies between those <span>participants</span>  <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>who self-selected into the study and those who did not. Most of the <span>participants</span>  <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>turned out to be heavy users of Yahoo! Answers and had sufficient knowledge on the topics of their questions, which probably made them more sensitive to the credibility issue. On the other hand, it is possible that those who are less concerned about information credibility in a social question and answer site are less likely to participate in the study because of the lack of interest in that topic. Therefore, the findings of the study should be regarded as suggestive rather than conclusive. Future research using random sampling or a large sample of people could generalize credibility judgment behaviour identified here to a larger population.  </p>

<h2>Conclusion</h2>

<p>The current study extends earlier credibility research that examined how people evaluate the credibility of Web information to a social questions and answers environment where users evaluate answers given by lay information providers. </p>

<p>The major findings of the study include: (1) the questioners do not always evaluate all given answers nor apply the same criteria to every answer; (2) there is a set of relatively consistent credibility cues across Web contexts; (3) the questioners rely more on message credibility than source credibility partly due to the frequent unavailability of source information; and (4) the type of question is a key factor that characterizes a credibility judgment process including pre-search and post-search verification. </p>

<p>Theoretically, the study has strengths in that it examined real users who asked real questions for their everyday life tasks in a novel environment and placed their credibility judgments in a broader context of an information searching process. Practically, it has implications for the design of social question and answer sites and user instruction on how to evaluate Web information. The questioners' credibility judgment behaviours identified in the study can help information professionals develop a user instruction programme to teach information seekers who want to find credible information as well as lay information providers who want to create credible answers. </p>


 
<h2>Acknowledgements</h2> 
 
<p>The author is grateful to Leia Dickerson, graduate student, for her help with data collection and coding. The author would also like to acknowledge the anonymous reviewers for their useful comments.</p>

<h2>About the author</h2>

<p>Soojung Kim is an instructor in the College of Information Studies, University of Maryland.  She received her PhD from the University of Maryland. She can be contacted at: <a href="mailto:kimsoojung1@gmail.com">kimsoojung1@gmail.com</a></p>
 
<form action="#"> 
<fieldset> 
<legend style="color: white; background-color: #5E96FD; font-size: medium; padding: .1ex .5ex; border-right: 1px solid navy; border-bottom: 1px solid navy; font-weight: bold;">References</legend> 

<ul> 
 
<li><a id="ada" name="ada"></a> Adamic, L.A., Zhang, J., Bakshy, E. &amp; Ackerman, M.S. (2008). Knowledge sharing and Yahoo Answers: everyone knows something. In <em> Proceedings of the 17th International Conference on World Wide Web</em> (pp. 665-664). New York, NY: ACM Press. </li> 

<li><a id="agi" name="agi"></a> Agichtein, E., Castillo, C., Donato, D., Gionis, A. &amp; Mishne, G. (2008). Finding high-quality content in social media. In <em>Proceedings of the International Conference on Web Search and Data Mining</em> (pp. 183-194). New York, NY: ACM.</li> 

<li><a id="bur" name="bur"></a> Burbules, N.C. (2001). Paradoxes of the Web: the ethical dimensions of credibility. <em>Library Trends</em>, <strong>49</strong>(4), 441-453.  </li> 

<li><a id="dan" name="dan"></a> Danielson, D.R. (2005). Web credibility. In C. Ghaoui (Ed.), <em>Encyclopedia of human-computer interaction</em> (pp. 713-721). Hershey, PA: <span>Idea</span>  <span class="conce" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>Group. </li>

<li><a id="eys" name="eys"></a> Eysenbach, G. &amp; Kohlelr, C. (2002). <a href="http://www.webcitation.org/5qaCBDF19">How do customers search for and appraise health information on the World Wide Web? <span>Qualitative</span>  <span class="quali" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>study using focus groups, usability tests, and in-depth <span>interviews</span>. <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span></a>  <em>British Medical Journal,</em>  <strong>324</strong>, 573-577. Retrieved 17 June, 2010 from http://www.bmj.com/cgi/content/full/324/7337/573 (Archived by WebCite&reg; at http://www.webcitation.org/5qaCBDF19)</li> 

<li><a id="fla00" name="fla00"></a> Flanagin, A.J. &amp; Metzger, M.J. (2000). Perceptions of Internet information credibility. <em>Digital media, youth, and credibility</em>, <strong>77</strong>(3), 515-540. </li> 

<li><a id="fla08a" name="fla08a"></a> Flanagin, A.J. &amp; Metzger, M.J. (2008a). Digital media and youth: unparalleled opportunity and unprecedented responsibility. In M. Metzger &amp; A. Flanagin (Eds.),  <em>Digital media, youth, and credibility</em> (pp. 5-28).  Cambridge, MA: The MIT Press.  </li> 

<li><a id="fla08b" name="fla08b"></a> Flanagin, A.J. &amp; Metzger, M.J. (2008b). The credibility of volunteered geographic information. <em>GeoJournal</em>, <strong>72</strong>(3-4), 137-148.  </li> 

<li><a id="fog03" name="fog03"></a> Fogg, B.J. (2003). Prominence-interpretation theory: explaining how people assess credibility online. In  <em>CHI'03 extended abstracts on human factors in <span>computing</span>  <span class="compus" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>systems </em> (pp. 722-723). New York, NY: ACM Press.   </li>

<li><a id="fogall03" name="fogall03"></a> Fogg, B.J., Soohoo, C., Danielson, D.R., Marable, L., Stanford, J. &amp; Tauber, E.R. (2003). How do users evaluate the credibility of Web sites? A study with over 2,500 <span>participants</span>. <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span> In <em>Proceedings of the 2003 Conference on Designing for User Experiences</em> (pp. 1-15). New York, NY: ACM Press. </li> 

<li><a id="fogall00" name="fogall00"></a> Fogg, B.J., Marshall, J., Laraki, O., Osipovich, A., Varma, C., Fang, N., Paul J., <em>et al.</em>  (2000). Elements that affect Web credibility: early results from a self-report study. In <em>CHI'03 extended abstracts on human factors in <span>computing</span>  <span class="compus" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>systems</em> (pp. 287-288). New York, NY: ACM Press.  </li> 

<li><a id="fogall01" name="fogall01"></a> Fogg, B.J., Marshall, J., Laraki, O., Osipovich, A., Varma, C., Fang, N., Paul, J., <em>et al.</em>  (2001). What makes Web sites credible? A report on a large <span>quantitative</span>  <span class="quanti" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>study. In  <em>Proceedings of the SGICHI Conference on Human Factors in <span>Computing</span>  <span class="compus" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>Systems </em> (pp. 61-68). New York, NY: ACM Press.   </li> 

<li><a id="fre" name="fre"></a> Freeman, K.S. &amp; Spyridakis, J.H. (2004). An examination of factors that affect the credibility of online health information.  <em>Technical Communication</em>, <strong>51</strong>(2), 239-263   </li> 

<li><a id="gaz" name="gaz"></a> Gazan, R. (2006). Specialists and synthesists in a question answering community.  <em>Proceedings of the Annual Meeting of the American Society for Information Science and Technology</em>, <strong>43</strong>, 1-10. </li> 

<li><a id="gra" name="gra"></a> Gray, N.J., Klein, J.D., Noyce, P.R., Sesselberg, T.S. &amp; Cantrill, J.A. (2005). Health information-seeking behavior in adolescence: the place of the Internet. <em>Social Science &amp; Medicine</em>, <strong>60</strong>(7), 1467-1478.  </li> 

<li><a id="har" name="har"></a> Harper, F.M., Moy, D. &amp; Konstan, J.A. (2009). Facts or friends? Distinguishing informational and conversational questions in social Q&amp;A sites. In  <em>Proceedings of the 27th International Conference on Human Factors in <span>Computing</span>  <span class="compus" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>Systems</em> (pp. 759-768).  New York, NY: ACM Press.</li> 

<li><a id="hil" name="hil"></a> Hilligoss, B. &amp; Rieh, S.Y. (2008). Developing a unifying framework of credibility assessment: construct, heuristics, and interaction in context.  <em>Information Processing and Management</em>, <strong>44</strong>, 467-1484.   </li> 

<li><a id="hit" name="hit"></a> Hitwise. (2008). <a href="http://www.webcitation.org/5k7EsNrUd">U.S. visits to question and answer websites increased 118 percent year-over-year.</a> United States Newsletter, March. Retrieved 19 September, 2009 from http://www.hitwise.com/news/us200803.html (Archived by WebCite at http://www.webcitation.org/5k7EsNrUd) </li> 

<li><a id="hon" name="hon"></a> Hong, T. (2006). The influence of structural and message features on Web site credibility.  <em>Journal of the American Society for Information Science and Technology</em>, <strong>57</strong>(1), 114-127.    </li> 

<li><a id="hov" name="hov"></a> Hovland, C.I., Janis, I.L. &amp; Kelley, H.H. (1953).  <em>Communication and persuasion</em>, New Haven, CT: Yale University Press.   </li> 

<li><a id="kim" name="kim"></a> Kim, S. &amp; Oh, S. (2009). Users' relevance criteria for evaluating answers in a social questions and answers site.  <em>Journal of the American Society for Information Science and Technology</em>, <strong>60</strong>(4), 716-727.   </li> 

<li><a id="lan" name="lan"></a> Landis, J. R. &amp; Koch, G. G. (1977). The <span>measurement</span>  <span class="exper" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>of observer agreement for categorical data.  <em>Biometrics</em>, <strong>33</strong>(1), 159-174.    </li> 

<li><a id="lin" name="lin"></a> Lincoln, Y. &amp; Guba, E. (1985). <em>Naturalistic inquiry.</em> Beverly Hills, CA: Sage Publications. </li> 

<li><a id="liuall" name="liuall"></a> Liu, Y., Bian, J. &amp; Agichtein, E. (2008). Predicting information seeker satisfaction in community question answering. In  <em>Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</em>, (pp. 483-490). New York, NY: ACM Press.   </li> 

<li><a id="liu" name="liu"></a> Liu, Z. (2004).  Perceptions of credibility of scholarly information on the Web. <em>Information Processing &amp; Management</em>, <strong>40</strong>(6), 1027-1038.    </li> 

<li><a id="liuh" name="liuh"></a> Liu, Z. &amp; Huang, X. (2005). Evaluating the credibility of scholarly information on the Web: a cross cultural study.  <em>International Information and Library Review</em>, <strong>37</strong>(2), 99-106.</li> 

<li><a id="mcg" name="mcg"></a> McGee, M. (2008).  <em><a href="http://www.webcitation.org/5k7FAXdy7">Yahoo Answers: 11 million answers per month.</a> </em> Retrieved 19 September, 2009, from http://www.smallbusinesssem.com/yahoo-answers-11-million-answers-per-month/1147/
(Archived by WebCite at http://www.webcitation.org/5k7FAXdy7)</li> 

<li><a id="met" name="met"></a> Metzger, M.J. (2007). Making sense of credibility on the Web: models for evaluating online information and recommendations for future research. <em>Journal of the American Society for Information Science and Information Technology</em>, <strong>58</strong>(13), 2078-2091.     </li> 

<li><a id="metll" name="metll"></a> Metzger, M.J., Flanagin, A.J., Eyal, K., Lemus, D., &amp; McCann, R. (2003). Credibility for the 21st century: Integrating perspectives on source, message, and media credibility in the contemporary media environment. <em>Communication Yearbook</em>, <strong>27</strong>, 293-335.</li> 

<li><a id="nam" name="nam"></a> Nam, K.K., Ackerman, M.S. &amp; Adamic, L. (2009). Questions in, Knowledge iN? A study of Naver's question answering community. In <em>Proceedings of the 27th International Conference on Human Factors in <span>Computing</span>  <span class="compus" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>Systems</em> (pp. 779-788). New York, NY: ACM Press.</li> 

<li><a id="oh" name="oh"></a> Oh, S., Oh, J. &amp; Shah, C. (2008). <a href="http://www.webcitation.org/5qaDVfdO4">The use of information sources by Internet users in answering questions</a>.  <em>Proceedings of the Annual Meeting of the American Society for Information Science and Technology</em>, <strong>45</strong>, 1-13.  Retrieved     
17 June, 2010 from http://www.unc.edu/~shoh/papers/ASIST_InfoSources_Oh_.pdf . (Archived by WebCite&reg; at http://www.webcitation.org/5qaDVfdO4)</li>

<li><a id="rie" name="rie"></a> Rieh, S.Y. (2002). Judgment of information <span>quality</span>  <span class="quali" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>and <span>cognitive</span>  <span class="conce" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>authority in the Web. <em>Journal of the American Society for Information Science and Technology</em>, <strong>53</strong>(2), 145-161.    </li> 

<li><a id="rieb08" name="rieb08"></a> Rieh, S.Y. &amp; Belkin, N.J. (1998). Understanding judgment of information <span>quality</span>  <span class="quali" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>and <span>cognitive</span>  <span class="conce" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>authority in the WWW.  <em>Proceedings of the Annual Meeting of the American Society for Information Science</em>, <strong>35</strong>, 279-289.  </li> 

<li><a id="rieb00" name="rieb00"></a> Rieh, S.Y. &amp; Belkin, N.J. (2000). Interaction on the Web: scholars' judgment of information <span>quality</span>  <span class="quali" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>and <span>cognitive</span>  <span class="conce" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>authority.  <em>Proceedings of the Annual Meeting of the American Society for Information Science</em>, <strong>37</strong>, 25-38.   </li> 

<li><a id="ried" name="ried"></a> Rieh, S.Y. &amp; Danielson, D.R. (2007). Credibility: a multidisciplinary framework. <em>Annual Review of Information Science and Technology</em>, <b>41</b>, 307-364. </li> 

<li><a id="rieh" name="rieh"></a> Rieh, S.Y. &amp; Hilligoss, B. (2008). College students' credibility judgments in the information seeking process. In M. Metzger &amp; A. Flanagin (Eds.), <em>Digital media, youth, and credibility</em>, (pp. 49-72).  Cambridge, MA: The MIT Press.</li> 

<li><a id="ros" name="ros"></a> Rosenthal, P.I. (1971). Specificity, verifiability, and message credibility. <em>Quarterly Journal of Speech</em>, <strong>57</strong>(4), 393-401.</li> 

<li><a id="sel" name="sel"></a> Self, C.S. (1996). Credibility. In M. Salwen &amp; D. Stacks (Eds.), <em>An integrated approach to communication theory and research</em> (pp. 421-441). Mahwah, NJ: Lawrence Erlbaum Associates.  </li> 

<li><a id="sla" name="sla"></a> Slater, M.D. &amp; Rouner, D. (1996). How message evaluation and source attributes may influence credibility assessment and belief change. <em>Journalism and Mass Communication Quarterly</em>, <strong>73</strong>(4), 974-991.    </li> 

<li><a id="sta" name="sta"></a> Stanford, J., Tauber, E.R, Fogg, B.L. &amp; Marable, L. (2002).  <em><a href="http://www.webcitation.org/5oq86WyUq">Experts vs. online consumers: a comparative credibility study of health and finance web sites</a>. </em> Consumer WebWatch Research Report. Retrieved 6 April, 2010 from http://www.consumerwebwatch.org/dynamic/web-credibility-reports-experts-vs-online.cfm  
(Archived by WebCite at http://www.webcitation.org/5oq86WyUq</li> 

<li><a id="su" name="su"></a> Su, Q., Pavlov, D., Chow, J.H. &amp; Baker, W.C. (2007). Internet-scale collection of human-reviewed data. In  <em>Proceedings of the 16th International Conference on World Wide Web</em> (pp. 231-240). New York, NY: ACM Press.</li> 

<li><a id="tse" name="tse"></a> Tseng, S. &amp; Fogg, B.J. (1999). Credibility and <span>computing</span>  <span class="compus" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>technology.  <em>Communications of the ACM</em>, <strong>42</strong>(5), 39-44.    </li> 

<li><a id="wat" name="wat"></a> Wathen, C.N. &amp; Burkell, J. (2002). Believe it or not: factors influencing credibility on the Web.  <em>Journal of the American Society for Information Science and Technology</em>, <strong>53</strong>(2), 134-144.    </li> 

<li><a id="zip" name="zip"></a> Zipf, G. K. (1949).  <em>Human behavior and the principle of least-effort: an introduction to human ecology</em>. Cambridge, MA: Addison-Wesley Press.  </li> 

<li><a id="yah" name="yah"></a> Yahoo! Answers. (n.d.).  <em><a href="http://www.webcitation.org/5k7FO7cCC">Points and levels</a>.</em> Retrieved 19 September, 2009 from http://answers.yahoo.com/info/scoring_system (Archived by WebCite at http://www.webcitation.org/5k7FO7cCC</li>

</ul> 
</fieldset> 
</form> 
 
 
<form action="#"> 

<fieldset> 
<legend style="color: white; background-color: #5E96FD; font-size: medium; padding: .1ex .5ex; border-right: 1px solid navy; border-bottom: 1px solid navy; font-weight: bold;">How to cite this paper</legend> 
<div><br /> 
Kim, S. (2010). &quot;Questioners' credibility judgments of answers in a social question and answer site&quot; <em>Information Research</em>, <strong>15</strong>(2) paper 432. [Available at http://InformationR.net/ir/15-2/paper432.html]</div> 
</fieldset> 
</form> 

<table cellspacing="10" align="center"> 
	<tr> 
		<td colspan="3" align="center" style="background-color: #5E96FD; color: white; font-family: verdana; font-size: small; font-weight: bold;">Find other papers on this subject</td></tr> 
	<tr> 
		<td align="center" valign="top"> 
 

<form method="get" action="http://scholar.google.com/scholar" target="_blank"> 
			<table bgcolor="#ffffff"> 
				<tr> 
					<td nowrap="nowrap" valign="top" align="center" height="32"><input type="hidden" name="q" size="31" maxlength="255" value="(&quot;Q&amp;A sites&quot; OR &quot;question and answer sites&quot; credibility Websites"></input> <br /> 
<input type="submit" name="sa" value="Scholar Search"  style="font-size: small; font-family: Verdana; font-weight: bold;"></input> 
<input type="hidden" name="num" value="100"></input> 
					</td> 
				</tr> 
			</table> 
 
</form> 
		</td> 
		<td align="center" valign="top"> 
<!-- Search Google --> 
 
<form method="get" action="http://www.google.com/custom" target="_blank"> 
			<table bgcolor="#ffffff"> 
				<tr> 
					<td nowrap="nowrap" valign="top" align="center" height="32"><input type="hidden" name="q" size="31" maxlength="255" value="(&quot;Q&amp;A sites&quot; OR &quot;question and answer sites&quot; credibility Websites"></input><br /> 

<input type="submit" name="sa" value="Google Search" style="font-family: Verdana; font-weight: bold; font-size: small;"></input> 
<input type="hidden" name="client" value="pub-5081678983212084"></input> 
<input type="hidden" name="forid" value="1"></input> 
 
<input type="hidden" name="ie" value="ISO-8859-1"></input> 
<input type="hidden" name="oe" value="ISO-8859-1"></input> 
<input type="hidden" name="cof" value="GALT:#0066CC;GL:1;DIV:#999999;VLC:336633;AH:center;BGC:FFFFFF;LBGC:FF9900;ALC:0066CC;LC:0066CC;T:000000;GFNT:666666;GIMP:666666;FORID:1;"></input> 
<input type="hidden" name="hl" value="en"></input> 
					</td> 
				</tr> 
			</table> 
</form> 
</td> 
<td align="center" valign="top"> 
<form method="get" action="http://www.bing.com/" target="_blank"> 
			<table bgcolor="#ffffff"> 
				<tr> 
 
					<td nowrap="nowrap" valign="top" align="center" height="32"><input type="hidden" name="q" size="31" maxlength="255" value="(&quot;Q&amp;A sites&quot; OR &quot;question and answer sites&quot; credibility Websites"></input> <br /> 

<input type="submit" name="sa" value="Bing"  style="font-size: small; font-family: Verdana; font-weight: bold;"></input> 
<input type="hidden" name="num" value="100"></input> 
					</td> 
				</tr> 
			</table> 
</form> 
 
		</td> 
	</tr> 
</table> 
 
<!-- <div align="center">Articles citing this paper, <a href="http://scholar.google.com/scholar?hl=en&lr=&cites=15087050128968045568" target="_blank">according to Google Scholar</a></div>
<br /> --> 
<div align="center"> 
<img src="http://images.del.icio.us/static/img/delicious.small.gif" alt="logo" /> <a href="http://del.icio.us/post" onclick="window.open('http://del.icio.us/post?v=4&amp;noui&amp;jump=close&amp;url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title), 'delicious', 'toolbar=no,width=700,height=400'); return false;">Bookmark This Page</a> 
</div> 
<hr size="1" style="color: #5E96FD;" /> 



<table align="center" cellpadding="10"> 
<tr><td align="center" valign="top"><div>  <a href="http://www.digits.com/" target="_blank"><img src="http://counter.digits.com/?counter={5be63dcd-eff1-2614-1599-c8181eb77bba}&amp;template=simple" 
     alt="Hit Counter by Digits" border="0"  /></a>
</div></td> 
 
<td align="center" valign="top"><div> 
&copy; the author, 2010. <br />Last updated: 17 June, 2010
</div></td> 
 
         <td align="center" valign="middle"><img src="../valid-xhtml10.gif" alt="Valid XHTML 1.0!" height="16" width="44" /><!--ONESTAT SCRIPTCODE START--> 
<!--
// Modification of this <span>code</span>  <span class="compus" style="width:1px;border-width:126px 42px 126px 42px;position:relative;">.</span>is not allowed and will permanently disable your account!
// Account ID : 281971
// Website URL: http://InformationR.net/ir/
// Copyright (C) 2002-2006 OneStat.com All Rights Reserved
--> 
<div id="OneStatTag"><table border='0' cellpadding='0' cellspacing='0'><tr><td align='center'> 
<script type="text/javascript"> 
<!--
function OneStat_Pageview()
{
    var d=document;
    var 0="281971";
    var CONTENTSECTION="";
    var osp_URL=d.URL;
    var osp_Title=d.title;
    var t=new Date();
    var p="http"+(d.URL.indexOf('paper432.html')==0?'s':'')+"://stat.onestat.com/stat.aspx?tagver=2&0="+0;
    p+="&url="+escape(osp_URL);
    p+="&ti="+escape(osp_Title);
    p+="&section="+escape(CONTENTSECTION);
    p+="&rf="+escape(parent==self?document.referrer:top.document.referrer);
    p+="&tz="+escape(t.getTimezoneOffset());
    p+="&ch="+escape(t.getHours());
    p+="&js=1";
    p+="&ul="+escape(navigator.appName=="Netscape"?navigator.language:navigator.userLanguage);
    if(typeof(screen)=="object"){
       p+="&sr="+screen.width+"x"+screen.height;p+="&cd="+screen.colorDepth;
       p+="&jo="+(navigator.javaEnabled()?"Yes":"No");
    }
    d.write('<a href="http://www.onestatfree.com/aspx/login.aspx?0='+0+'" target=_blank><img id="ONESTAT_TAG" border="0" src="'+p+'" alt="This site tracked by OneStatFree.com. Get your own free site tracker."></'+'a>');
}
 
OneStat_Pageview();
//--> 
</script> 
<noscript> 
<a href="http://www.onestatfree.com/"><img border="0" src="http://stat.onestat.com/stat.aspx?tagver=2&amp;0=281971&amp;js=No&amp;" alt="online web site analytics" /></a> 
</noscript> 
</td></tr><tr><td align='center'><div style="COLOR:black;display:none;FONT-FAMILY:'Verdana';"><a href="http://www.onestat.com/" style="text-decoration:none;">online web site analytics</a><br /></div></td></tr></table></div> 

<!--ONESTAT SCRIPTCODE END--> 
</td></tr> 
</table> 
<hr size="3" style="color: #5E96FD;" /> 
 
<table align="center"><tr><td><div class="button"> 
 

<ul>
	<li><a href="infres152.html">Contents</a> | </li>
	<li><a href="../iraindex.html">Author index</a> | </li>
	<li><a href="../irsindex.html">Subject index</a> | </li>
	<li><a href="../search.html">Search</a> | </li>
	<li><a href="../index-2.html">Home</a></li>
</ul> 

</div></td></tr></table> 
 <hr size="3" style="color: #5E96FD;" />
 
<script src="http://www.google-analytics.com/urchin.js" type="text/javascript"> 
</script> 
<script type="text/javascript"> 
_uacct = "UA-672528-1";
urchinTracker();
</script> 
</body> 

<!-- Mirrored from informationr.net/ir/15-2/paper432.html by HTTrack Website Copier/3.x [XR&CO'2010], Sat, 25 Dec 2010 19:34:15 GMT -->
</html> 

