<!doctype html public "-//W3C//DTD HTML 4.0 Transitional//EN">
<html>

<!-- Mirrored from informationr.net/ir/8-3/paper154.html by HTTrack Website Copier/3.x [XR&CO'2010], Sat, 25 Dec 2010 19:11:22 GMT -->
<head>
<title>Web search: how the Web has changed information retrieval</title>
<link href="../IRstyle.css" rel=stylesheet>
<meta http-equiv=content-type content="text/html; charset=windows-1252">
<meta name=generator  content="MSHTML 6.00.2722.900">
<link rev=made href="mailto:t.d.wilson@shef.ac.uk">
<meta name="rating" content=mature />
<meta name="vw96.objecttype" content=document />
<meta name="robots" content=all /> 
<meta name="keywords" content="WWW, World Wide Web, Web, information retrieval, open Web closed Web, meta-data, metadata, meta-tags" /> 
<meta name="DC.Title" content="Web search: how the Web has changed information retrieval" />
<meta name="DC.Creator" content="Brooks, Terrence A." />
<meta name="DC.Description" content="Topical metadata are simultaneously hailed as building blocks of the semantic Web and derogated as spam. The significance of the metadata controversy depends on the technological appropriateness of adding them to Web pages. A survey of Web technology suggests that Web pages are both transient and volatile: poor hosts of topical metadata. A more supportive environment exists in the closed Web. The vast majority of Web pages, however, exist in the open Web, an environment that challenges the application of legacy information retrieval concepts and methods." /> 
<meta name="DC.Subject.keywords" content="WWW, World Wide Web, Web, information retrieval, open Web closed Web, meta-data, metadata, meta-tags" />
<meta name="DC.Subject" content="WWW, World Wide Web, Web, information retrieval, open Web closed Web, meta-data, metadata, meta-tags" />
<meta name="DC.Type" content="text" />
<meta name="DC.Identifier" scheme="ISSN" content="1368-1613" />
<meta name="DC.Identifier" content="paper154.html" /> 
<meta name="DC.Relation.IsPartOf" content=http://InformationR.net/ir/8-3/infres83.html />
<meta name="DC.Format" content="text/html" />
<meta name="DC.Language" content="en" />
<meta name="DC.Publisher" content="Professor T.D. Wilson" />
<meta name="DC.Rights" content="http://creativecommons.org/licenses/by-nd-nc/1.0/" />
<meta name="DC.Coverage.placename" content=global />
<script language="JavaScript" type="text/JavaScript">
<!--
function MM_preloadImages() { //v3.0
  var d=document; if(d.images){ if(!d.MM_p) d.MM_p=new Array();
    var i,j=d.MM_p.length,a=MM_preloadImages.arguments; for(i=0; i<a.length; i++)
    if (a[i].indexOf("#")!=0){ d.MM_p[j]=new Image; d.MM_p[j++].src=a[i];}}
}

function MM_findObj(n, d) { //v4.01
  var p,i,x;  if(!d) d=document; if((p=n.indexOf("?"))>0&&parent.frames.length) {
    d=parent.frames[n.substring(p+1)].document; n=n.substring(0,p);}
  if(!(x=d[n])&&d.all) x=d.all[n]; for (i=0;!x&&i<d.forms.length;i++) x=d.forms[i][n];
  for(i=0;!x&&d.layers&&i<d.layers.length;i++) x=MM_findObj(n,d.layers[i].document);
  if(!x && d.getElementById) x=d.getElementById(n); return x;
}

function MM_nbGroup(event, grpName) { //v6.0
  var i,img,nbArr,args=MM_nbGroup.arguments;
  if (event == "init" && args.length > 2) {
    if ((img = MM_findObj(args[2])) != null && !img.MM_init) {
      img.MM_init = true; img.MM_up = args[3]; img.MM_dn = img.src;
      if ((nbArr = document[grpName]) == null) nbArr = document[grpName] = new Array();
      nbArr[nbArr.length] = img;
      for (i=4; i < args.length-1; i+=2) if ((img = MM_findObj(args[i])) != null) {
        if (!img.MM_up) img.MM_up = img.src;
        img.src = img.MM_dn = args[i+1];
        nbArr[nbArr.length] = img;
    } }
  } else if (event == "over") {
    document.MM_nbOver = nbArr = new Array();
    for (i=1; i < args.length-1; i+=3) if ((img = MM_findObj(args[i])) != null) {
      if (!img.MM_up) img.MM_up = img.src;
      img.src = (img.MM_dn && args[i+2]) ? args[i+2] : ((args[i+1])? args[i+1] : img.MM_up);
      nbArr[nbArr.length] = img;
    }
  } else if (event == "out" ) {
    for (i=0; i < document.MM_nbOver.length; i++) {
      img = document.MM_nbOver[i]; img.src = (img.MM_dn) ? img.MM_dn : img.MM_up; }
  } else if (event == "down") {
    nbArr = document[grpName];
    if (nbArr)
      for (i=0; i < nbArr.length; i++) { img=nbArr[i]; img.src = img.MM_up; img.MM_dn = 0; }
    document[grpName] = nbArr = new Array();
    for (i=2; i < args.length-1; i+=2) if ((img = MM_findObj(args[i])) != null) {
      if (!img.MM_up) img.MM_up = img.src;
      img.src = img.MM_dn = (args[i+1])? args[i+1] : img.MM_up;
      nbArr[nbArr.length] = img;
  } }
}
//-->
</script>

</head>
<body bgcolor="#ffffff" onLoad="MM_preloadImages('../figs/iauthori1.gif','../figs/isubji1.gif','../figs/isearch1.gif','../figs/ihome1.gif','../figs/contents1.gif')">
<table align="center" border="0" cellpadding="0" cellspacing="0">
<tr><td height="30" align="center" colspan="5"><h4>Information Research, Vol. 8  No. 3, April 2003</h4></td></tr>
  <tr> 
    <td><a href="infres83.html" target="_top" onClick="MM_nbGroup('down','group1','contents','',1)" onMouseOver="MM_nbGroup('over','contents','../figs/contents1.gif','',1)" onMouseOut="MM_nbGroup('out')"><img name="contents" src="../figs/contents.gif" border="0" alt="" onLoad=""></a></td>
    <td><a href="../iraindex.html" target="_top" onClick="MM_nbGroup('down','group1','authorindex','',1)" onMouseOver="MM_nbGroup('over','authorindex','../figs/iauthori1.gif','',1)" onMouseOut="MM_nbGroup('out')"><img src="../figs/iauthori.gif" alt="" name="authorindex" width="120" height="20" border="0" onload=""></a></td>
    <td><a href="../irsindex.html" target="_top" onClick="MM_nbGroup('down','group1','subjindex','',1)" onMouseOver="MM_nbGroup('over','subjindex','../figs/isubji1.gif','',1)" onMouseOut="MM_nbGroup('out')"><img src="../figs/isubji.gif" alt="" name="subjindex" width="120" height="20" border="0" onload=""></a></td>
    <td><a href="../search.html" target="_top" onClick="MM_nbGroup('down','group1','search','',1)" onMouseOver="MM_nbGroup('over','search','../figs/isearch1.gif','',1)" onMouseOut="MM_nbGroup('out')"><img src="../figs/isearch.gif" alt="" name="search" width="120" height="20" border="0" onload=""></a></td>
    <td><a href="../index.html" target="_top" onClick="MM_nbGroup('down','group1','home','',1)" onMouseOver="MM_nbGroup('over','home','../figs/ihome1.gif','',1)" onMouseOut="MM_nbGroup('out')"><img name="home" src="../figs/ihome.gif" border="0" alt="" onLoad=""></a></td>
  </tr>
</table>
<hr color=#ff00ff size=3>

<h1>Web search: how the Web has changed information retrieval</h1>

<h4 align=center><a href="mailto:tabrooks@u.washington.edu">Terrence A. Brooks</a> 
<br>Information School
<br>University of Washington
<br>Seattle, Washington, USA</h4>

<div align=center><b>Abstract</b></div>

<blockquote>Topical metadata have been used to indicate the subject of Web pages.  They have been simultaneously hailed as building blocks of the semantic Web and derogated as spam. At this time major Web browsers avoid harvesting topical metadata. This paper suggests that the significance of the topical metadata controversy depends on the technological appropriateness of adding them to Web pages. This paper surveys Web technology with an eye on assessing the appropriateness of Web pages as hosts for topical metadata.  The survey reveals Web pages to be both transient and volatile: poor hosts of topical metadata. The closed Web is considered to be a more supportive environment for the use of topical metadata. The closed Web is built on communities of trust where the structure and meaning of Web pages can be anticipated.  The vast majority of Web pages, however, exist in the open Web, an environment that challenges the application of legacy information retrieval concepts and methods. 
</blockquote>


<h2>Introduction</h2>

<p>Search is a compelling human activity that has extended from the Library at Alexandria to the World Wide Web. The Web has introduced millions of people to search. The information retrieval (IR) community stands ready (<a href="#bates">Bates, July 2002</a>) to suggest helpful strategies for finding information on the Web. One classic IR strategy - indexing Web pages with topical metadata - has already been tried, but the results are disappointing.  Apparently, relying on Web authors to garnish their Web pages with valid topical metadata runs afoul of human nature:</p>

<ul>
<li>Sullivan (<a href="#Sullivan1012002">2002c</a>) reports that the meta <em>keywords</em> tag, an HTML element designed for adding descriptors to Web pages, is regarded as untrustworthy and avoided by all major search engines.</li>
<li>A FAQ at the Dublin Core site explains that well-known 'all the Web' search engines 'tend to avoid using the information found in meta elements' for fear it is spam (<a href="#faq">FAQ</a>).</li>
</ul>

<p>Applying topical metadata to Web pages provokes a controversy that pits partisans who envisage a semantic Web featuring topic maps and ontologies of shared meanings (<a href="#berners">Berners-Lee, Hendler &amp; Lassila, May 2001</a>) versus detractors who disdain topical metadata as 'metacrap' (<a href="#doctorow">Doctorow, August 26, 2001</a>) and warn us of a Web of deception (<a href="#mintz">Mintz, 2002</a>).  The significance of the controversy, however, awaits the examination of a more fundamental issue: does it make <em>technological</em> sense to add topical metadata to Web pages?</p>

<p>If the Web is a big, distributed document database and Web pages are composed in HTML (i.e., 'the document in my browser goes from &lt;html> down to &lt;/html>'), the answer is ‘yes.’ In this case, it makes technological sense for Web authors to add topical metadata to Web pages, just as an indexer might add descriptors to a document in a database.  An affirmative answer validates the topical metadata debate.  If, however, the Web is not a big document database, but is instead a network of rapidly changing presentations, the answer is ‘no.’ In this view HTML is primarily a presentation technology, and most Web pages are transitory and volatile presentations governed by the whims of viewer taste and the contingencies of viewer technology.  A negative answer signals that debating the value of topical metadata is premature until it can be shown that they are technologically appropriate additions to Web pages.</p>

<p>Lurking behind the topical metadata controversy is our unsteady application of the concept of 'document' to Web content and presentation.  We inherit our notion of document from vertical-file systems and document databases, technological environments not known for schisms between content and presentation.  Viewed from the document-database tradition, indexing Web pages appears to be a simple extension of current practice to a new, digital form of document.  Viewed from the HTML tradition, however, indexing Web pages confuses presentation for content.  Topical metadata are intended to index information content, not arbitrary or personalized views of content, and the majority of Web pages are arbitrary presentations contingent on Web browsers, security settings, scripts, plug-ins, cookies, style sheets and so on.</p>

<p>Considering the appropriateness of the document metaphor for the Web has fundamental consequences for the application of IR’s extensive body of theory and practice.  Controversies about topical metadata aside, recognizing the familiar IR notion of 'document' on the Web would suggest that Web searchers are retrieving information, and that we can apply IR concepts and methods to help Web searchers.  In this case, the topical metadata controversy gains significance.  Realizing that the document metaphor does not map to the Web, however, heralds a paradigm shift.  Perhaps Web searchers are not retrieving information, but doing something else.  ‘Web search’ is used in this essay to name the activity of discovering, not retrieving, information on the Web.</p>
  
<h3>IR and the 'document' metaphor</h3>

<h4>The technological legacy of search</h4>

<p>The foundation of search in the last century has been the storage and retrieval of paper based on some form of labeling.  Yates (<a href="#yates">2000</a>) describes vertical filing that made information accessible by using labeled files to hold one or more papers:</p>

<blockquote>Vertical filing, first presented to the business community at the 1893 Chicago World's Fair (where it won a gold medal), became the accepted solution to the problem of storage and retrieval of paper documents….The techniques and equipment that facilitated storage and retrieval of documents and data, including card and paper files and short- and long-term storage facilities, were key to making information accessible and thus potentially useful to managers.  (<a href="#yates">Yates, 2000: 118 -120</a>)</blockquote>

<p>The application of computer databases to search by mid-20th century extended the vertical file paradigm of storage and retrieval.  A computer database is a storage device resembling a vertical file just as a database record is a unit of storage resembling a piece of paper.  The more abstract term 'document' addressed any inexactitude in the equivalence of 'database record = piece of paper.”  Computer databases were seen as storing and retrieving documents, which were considered to be objects carrying information:</p>

<ul>
<li>'Information retrieval is best understood if one remembers that the information being processed consists of documents.' (<a href="#salton">Salton &amp; McGill, 1988, p. 7</a>)</li>
<li>'With the appearance of writing, the document also appeared which we shall define as a material carrier with information fixed on it.' (<a href="#frants">Frants <em>et al.</em>,  1997: 46</a>)</li>
<li>'Document: a unit of retrieval. It might be a paragraph, a section, a chapter, a Web page, an article, or a whole book.' (<a href="#baeza">Baeza-Yates &amp; Ribeiro-Neto, 1999: 440</a>)</lio>
</ul>

<p>Digitizing documents greatly boosted the systematic study of IR.  Texts could be parsed to identify and evaluate words, thereby perhaps discovering meaning.  Facilitating assumptions about the nature of documents and authorial strategies were advanced.  For example, Luhn (<a href="#luhn">1959: 160</a>) suggested that 'the frequency of word occurrence in an article furnishes a useful measurement of word significance.'  In the following extract Salton and McGill (<a href="#salton">1988</a>) suggest where subject topical terms are located in documents, and how text can be processed to find these terms: </p>

<blockquote>The first and most obvious place where appropriate content identifiers might be found is the text of the documents themselves, or the text of document titles and abstracts…. Such a process must start with the identification of all the individual words that constitute the documents…. Following the identification of the words occurring in the document texts, or abstracts, the high-frequency function words need to be eliminated… It is useful first to remove word suffixes (and possibly also prefixes), thereby reducing the original words to word stem form.  (<a href="#salton">Salton &amp; McGill, 1988: 59, 71</a>).</blockquote>

<p>The document-database search technology sketched above maps easily to the Web and suggests that searching on the Web is an extension of IR:</p>

<ul>
<li>Vast numbers of documents are available on the Web (e.g., 'the Web is a big database.')</li>
<li>Viewing the source of a Web presentation reveals a structured document (e.g.: 'the document goes from &lt;html> down to &lt;/html>.')</li>
<li>Google seems to index Web pages (e.g., 'Google is a big index made up of words found in Web pages.')</li>
</ul>
      <h4>The legacy social context of search</h4>

<p>We inherit, as well, an elaborate social context of search that has been applied to the Web.   Librarianship was the source of powerful social conventions of search even before the introduction of the technology of vertical files.  For example, Charles A. Cutter suggested rules for listing bibliographic items in library catalogues as early as 1876.  Bibliographic standardization, expressed in the Anglo-American Cataloguing Code, was a powerful idea that promoted the view that the world could cooperate in describing bibliographic objects.  An equally impressive international uniformity was created by the wide acceptance of classification schemes, such as the Dewey Decimal Classification (DDC):</p>

<blockquote>Other influences are equally enduring but more invisible, and some are especially powerful because they have come to be accepted as 'natural.'  For example, the perspectives Dewey cemented into his hierarchical classification system have helped create in the minds of millions of people throughout the world who have DDC-arranged collections a perception of knowledge organization that had by the beginning of the twentieth century evolved a powerful momentum.  
(<a href="#wiegand">Wiegand, 1996: 371</a>)</blockquote> 

<p>The application of computer databases by mid-20th century spurred many information communities to establish or promote social conventions for their information.  For example, the Education Resources Information Center (ERIC), 'the world’s largest source of education information' (<a href="#houston">Houston, 2001: xiv</a>), represents a community effort to structure and index the literature of education.  At the height of the database era in the late 1980s, vendors such as the Dialog Corporation offered access to hundreds of databases like ERIC, each presenting one or more literatures structured and indexed.  This social cooperation and technological conformity fostered the impression that, at least in regards to certain subject areas, the experts had their information under control.</p>
 
<p>The social context of document-database search sketched above maps easily to the Web and suggests a benign, socially cooperative information environment:</p>

<ul>
<li>Web authors will add topical metadata to their Web pages (e.g., 'I index my Web pages with keywords and Dublin Core metadata so people will find them on the Web.')</li>
<li>Everyone will use topical metadata (e.g., 'The semantic Web will be constructed by millions of Web authors indexing their Web pages.')</li>
<li>Web crawlers, like Google, will harvest topical metadata (e.g., 'Google has indexed my topical metadata and now my Web pages are available for retrieval.')</li>
</ul>

<p>We are now just learning that the Web has a different social dynamic.  The Web is not a benign, socially cooperative environment, but an aggressive, competitive arena where authors seek to promote their Web content, even by abusing topical metadata.  As a result, Web crawlers must act in self defense and regard all keywords and topical metadata as spam.</p>

<p>Debating whether topical metadata are spam or an essential step towards the construction of the semantic Web assumes that they are technologically appropriate additions to Web pages.  To what extent are Web pages analogues of the legacy IR document-container of information?</p>

<h3>The Web and the 'document' metaphor</h3>

<h4>A Web page is a 'snapshot'</h4>

<p>Documents added to the ERIC database thirty years ago are still retrievable.  There is every expectation that they can be retrieved next year.  This expectation provides a rough definition of what it means to retrieve information – finding the same document time and again.  The metaphor used in the working draft on the <em>Architectural Principles of the Web</em> (<a href="#jacobs">Jacobs, August 30, 2002</a>), however, does not suggest retrieving the same thing time and again.  Interacting with a Web resource gives one a snapshot:</p>

<blockquote>There may be several ways to interact with a resource.  One of the most important operations for the Web is to retrieve a representation of a resource (such as with HTTP GET), which means to retrieve a snapshot of a state of the resource. (<a href="#jacobs">Jacobs, August 30, 2002, section 2.2.2</a>)</blockquote>

<p>Web resources are characterized as evolving, not static, resources.  They are more like loose-leaf binder services than time-invariant database records:</p>

<blockquote>An integrating resource is a bibliographic resource that is added to or changed by means of updates that do not remain discrete and are integrated into the whole.  Examples of integrating resources include updating loose-leafs and updating Web sites.  (<a href="#task">Task group on implementation of integrating resources, 2001</a>)</blockquote> 
 
<p>If Web pages are snapshots then the critical question is rate of update. Some ERIC records are thirty years old; the oldest HTML pages date from about ten years ago, but most Web content is much more volatile:</p>

<ul>
<li>Brewington and Cybenko (<a href="#brewington">2000</a>) observed that half of all Web pages are no more than 100 days old, while only abut 25% are older than one year.</li>

<li>Cho and Garcia-Molina (<a href="#cho">2000</a>) found 40% of Web pages in the <em>.com</em> domain change everyday. The half-life of Web pages in the <em>.gov</em> and <em>.edu</em> is four months.</li>

<li>Koehler (<a href="#koehler">1999</a>) found the half-life of Web content is two years.</li>  

<li>Spinellis (<a href="#spinellis">2003</a>) found the half-life of URLs is four years.</li>

<li>Markwell and Brooks (<a href="#markwell">April 15, 2002</a>) found the half-life of science education URLs to be fifty-five months.</li>

<li>Cockburn and McKenzie (<a href="#cockburn">2001</a>) found that the half-life of bookmarks to be two months.</li>
</ul>

<p>Content churn and rapid birth and death cycles distinguish Web pages from the legacy IR document-container of information.  Philosophers can address the issue of repeated refreshing of the 'same' Web page that presents 'different' content each time, as to whether this is the 'same' Web page or 'different' Web pages.  Whatever grist falls from the philosophical mill, it is clear that Salton and McGill didn’t consider database documents to be snapshots.</p>

      <h4>Web pages are cultural artifacts</h4>

<p>Web content is only available through the mediation of a presentation device, such as a Web browser.  Complicating Web presentation are security settings, different computer monitors, safe and unsafe Web colors, plug-ins, cookies, scripts, and so on.  In fact, Web authors expend enormous amounts of time and energy engineering a consistent presentation across platforms.</p>

<blockquote>The representations of a resource may vary as a function of factors including time, the identity of the agent accessing the resource, data submitted to the resource when interacting with it, and changes external to the resource.' (<a href="#jacobs">Jacobs, August 30, 2002, section 2.2.5</a>)
</blockquote>

<p>Figure 1 illustrates the process of converting HTML to a browser display for the Mozilla layout engine (<a href="#waterson">Waterson, June 10, 2002</a>).</p>

<div align="center"><img src="p154fig1.jpg" width="650" height="388" border="0" alt=""></div>


<div align="center"><strong>Figure 1: Basic data flow in Mozilla layout engine (<a href="#waterson">Waterson, June 10, 2002</a>)</strong></div>

<p>This diagram illustrates that HTML code is parsed and deconstructed into a hierarchical content model.  Style sheets that reference content elements are also parsed.  A frame constructor mixes content with style rules into a hierarchy of content frames.  Nested content frames are painted to create the presentation in a Web browser.  Figure 1 implies that different HTML parsing rules, style sheet applications, frame construction algorithms, and so on, would produce a different presentation.</p>
 
<p>Figure 1 also implies that assembling Web content to look like a printed document is not a technical necessity, but a cultural convention. If your Web browser presents you with something that looks like a printed page, it is because the engineers of Web browsers are obeying the cultural expectations of the majority of their users; that is, information should resemble the familiar printed page.  The mutability of Web presentation is not deplored, but actually trumpeted as an advantage in delivering customized appearance.  In short, Web content can be made to look like your <em>favorite printed page</em>:
</p>

<blockquote>Cookies serve to give Web browsers a ‘memory’, so that they can use data that were input on one page in another, or so they can recall user preferences or other state variables when they user leaves a page and returns.  (<a href="#flanagan">Flanagan, 1997, p.231</a>)</blockquote>

<p>Finding the Web page in your browser located between &lt;HTML> and &lt;/HTML> tags reflects how your browser constructed the byte stream from the source server machine, but says nothing about how the content was structured on the source server machine.  The source content could be distributed among a number of databases, XML documents, scripts, files and so on.  XSLT style sheets can assemble Web site 'skins' from databases and XML sources with equal ease (<a href="#pierson">Pierson, March 2003</a>)</p>

<p>During the early years of the Web, most Web pages were constructed in HTML and many handcrafted Web pages are still written this way.  Efficiencies of scale, however, have forced large producers of Web content to automate Web page production:</p>

<ul>
<li>Turau (<a href="#turau">1999</a>) speculated that 75% of Web pages are generated from databases.</li>  
<li>Bergman (<a href="#bergman">2001</a>) describes the 'deep' Web as 400 to 500 times larger than the 'surface' Web.  The deep Web is composed of database generated pages.</li>
</ul>

<p>Web pages are presentation contingencies and server programming artifacts.  This schism between content and presentation distinguishes them from the legacy IR notion of document-container of information.  The document in your Web browser may look like a document, but probably has no documentary origin at all.</p>

     <h4>Google is not a Web index</h4>

<p>An index helps searchers find information in a database.  It is generally true that success in finding information in a database is directly proportional to knowledge about how the database documents were indexed.  Database vendors such as the Dialog Corporation are famous for running classes helping searchers understand how information is indexed.  
<a href="http://www.google.com/">Google</a> is a popular search tool for Web content, twice voted most outstanding search engine by the readers of <a href="http://searchenginewatch.com/">Search Engine Watch</a>.  In August 2002, about 28% of Web search was done with Google (<a href="#sullivan9172002">Sullivan, 2002b</a>). 
'Google gets 150 million queries a day from more than 100 countries' (<a href="#harmon">Harmon, February 17, 2003</a>). Google is famous for presenting results according to page rank, but nobody knows how Google’s parsing algorithm works.</p>
  
<p>Sullivan (<a href="#sullivan932002">2002a</a>) surmises that Google uses over 100 
  factors to parse Web content, which still includes 'traditional on-the-page 
  factors.' (The algorithm of Salton and McGill focuses on these factors.) If 
  Google were to expose its parsing algorithm, it would be immediately exploited 
  by Web authors seeking to gain advantage and visibility for their Web content. 
  Google’s economic viability depends maintaining this secret: a corporate strategy 
  strikingly different from legacy database vendors like the Dialog Corporation. 
  Google warns Web authors who would attempt to ferret out and exploit their parsing 
  algorithm:</p>

<blockquote>We will not comment on the individual reasons a page was removed and we do not offer an exhaustive list of practices that can cause removal. However, certain actions such as cloaking, writing text that can be seen by search engines but not by users, or setting up pages/links with the sole purpose of fooling search engines may result in permanent removal from our index.  (http://www.google.com/Webmasters/2.html).... Google's complex, automated methods make human tampering with our results extremely difficult (http://www.google.com/technology/index.html).
</blockquote>

<p>Google does not attempt to cover the entire Web. It systematically excludes Web sites with doorway pages or splash screens, frames and pages generated 'on-the-fly' by scripts and databases.  Jesdanun (<a href="#jesdanun">October 25, 2002</a>) reports content removed from Google to satisfy national prohibitions.  Many Web pages also include objects Google finds opaque such as image files and applets.  Increasing numbers of Web presentations have no text at all: 'Graphic design can be content where users experience a Web-site with little or no "text" <em>per se</em>'. (<a href="#vartanian">Vartanian, 2001</a>). </p>
 
<p>Legacy indexing algorithms were open for inspection.  Adding topical metadata to Web pages in the hope that Google will harvest them is betting on an unknown indexing strategy. Google will no tell you what it did with your topical metadata because being a black box is a corporate survival strategy.</p>

<h2>Conclusion</h2>

<p>The preceding survey of Web technology indicates that Web pages make poor hosts for topical metadata.  This is not an evaluative judgment about topical metadata themselves, but merely an observation that they are misapplied to a technology characterized by churning content in arbitrary presentations parsed by unknowable algorithms.  The cost and effort of adding topical metadata to an information structure is only recouped if that information structure persists in time with a predictable structure, identity and contents.  An example of such a structure is the legacy IR document-container of information.  Topical metadata await their more appropriate application on the Web in environments where the technical and social factors supporting the IR document-container of information can be re-created.  This can be done by 'closing' the Web.</p>

	<h3>Closing the Web to do information retrieval</h3>

<p>The legacy technical and social environments supporting IR in document databases are sketched above. It is possible to re-create this environment on the Web behind passwords in venues such as intranets, enterprise computing, and digital libraries.  These applications are driven by social groups that can reach agreements on information structure and topical metadata.  For example, a social group can arbitrarily decide to construct and present its information in HTML or any other presentation technology.  It can also decide to use the meta keywords tag or Dublin Core metadata with its choice of thesaurus of indexing terms and phrases.</p>  

<p>Social agreements take precedence over technology in closed Webs where the Web is reduced to a communications venue.  Predictability in structure and meaning is the fundamental facilitator permitting in-house Web crawlers to harvest topical metadata for the retrieval benefit of the local community.  In a closed Web, one can build a legacy database and do IR.</p>  

<p>If our terms of reference are a closed Web, then the topical metadata controversy recognizes topical metadata as important elements of a semantic (i.e., 'closed') Web.</p>

	<h3>Web search on the 'open' Web</h3>

<p>Now and in the future, Google and similar tools will scan billions of Web presentations on the open Web.  Everyday searchers will use Google to find information, and many will characterize their activity as retrieving information, despite disappearing Web pages, rotten links and Web content that changes on each viewing.</p>

<p>The open Web is a network where the cost of entry is merely access to a server machine.  There are no social conventions about who can author a Web presentation or what can be presented.  It is an unconstrained environment where initiatives requiring Web authors to add indexing terms and phrases or to structure their Web pages a certain way are doomed to failure, or will be exploited by the unscrupulous.  In the open Web there is no guarantee that Web presentations will remain or that servers will continue to function.</p>
  
<p>Many will use the Web to 'retrieve information,' but they are engaged in Web search, a process of constant discovery, not retrieval.  The only way to preserve a Web presentation is to cache it, which is to take a <em>snapshot of a snapshot</em> and thereby create a new static representation of a continuously evolving process. </p>
 
<p>The open Web challenges us to ransack our IR legacy of concepts and methods to find any that can be applied. But it is possible that the open Web is so novel a technological platform that we will be forced to recognize that our IR legacy of concepts and methods has been historicized to the modern database era of the late 20th century. </p>

<h2>References:</h2>

<ul>
<li><a name="baeza"></a>Baeza-Yates, R. &amp; Ribeiro-Neto, B. (1999) <em>Modern information retrieval</em>.  Reading, MA: Addison-Wesley.</li>

<li><a name="bates"/>Bates, M.J. (2002). <a href="http://firstmonday.org/issues/issue7_7/bates/index.html">After the dot-bomb: getting Web information retrieval right this time.</a> <em>First Monday</em>, <b>7</b> (7)  http://firstmonday.org/issues/issue7_7/bates/index.html   (20 March 2003)</li>

<li><a name="bergman"></a>Bergman, M. K. (2001) <a href="http://www.press.umich.edu/jep/07-01/bergman.html">The deep Web: surfacing hidden information</a>   <em>The Journal of Electronic Publishing</em>, <b>7</b> (1) http://www.press.umich.edu/jep/07-01/bergman.html (18 January 2003)</li>

<li><a name="berners"></a>Berners-Lee, T., Hendler, J. &amp; Lassila, O. (2001)  <a href="http://www.scientificamerican.com/article.cfm?articleID=00048144-10D2-1C70-84A9809EC588EF21&amp;catID=2">The semantic Web</a>.  <em>Scientific American</em>, <strong>284</strong>(5), 34-+  http://www.scientificamerican.com/article.cfm?articleID=00048144-10D2-1C70-84A9809EC588EF21&catID=2  (20 March 2003)</li>

<li><a name="brewington"></a>Brewington, B.E. &amp; Gybenko, G. (2000) <a href="http://www9.org/w9cdrom/264/264.html">How dynamic is the Web?</a> In: [Proceedings of the] Ninth International World Wide Web Conference, Amsterdam May 15-19, 2000.  WWW9.org. http://www9.org/w9cdrom/264/264.html (20 March 2003)</li>

<li><a name="cho"></a>Cho, J. &amp; Garcia-Molina, H. (2000)  <a href="http://rose.cs.ucla.edu/~cho/papers/cho-evol.pdf">The evolution of the Web and implications for an incremental crawler.</a>  In:  Amr El Abbadi, Michael L. Brodie, Sharma Chakravarthy, Umeshwar Dayal, Nabil Kamel, Gunter Schlageter, and Kyu-Young Whang, editors. <em>VLDB 2000, Proceedings of 26th International Conference on Very Large Data Bases, September 10-14, 2000, Cairo, Egypt</em>. San Francisco, CA: Morgan Kaufmann.  http://rose.cs.ucla.edu/~cho/papers/cho-evol.pdf (20 March 2002) </li>

<li><a name="cockburn"></a>Cockburn, A. &amp; McKenzie, B. (2001)  'What do Web users do? An empirical analysis of Web use'  <em>International Journal of Human-Computer Studies</em>, <b>54</b>(6), 903-922</li>

<li><a name="doctorow"></a>Doctorow, C. (2001) <a href="http://www.well.com/~doctorow/metacrap.htm">Metacrap: putting the torch to seven straw-men of the meta-utopia.</a> [Personal Web site]  http://www.well.com/~doctorow/metacrap.htm (20 March 2003)</li>

<li><a name="faq"></a><a href="http://dublincore.org/resources/faq/#whatsearchenginessupport">FAQ: What search-engines support the Dublin Core Metadata Element Set?</a> &nbsp;&nbsp;Dublin Core Metadata Initiative.  http://dublincore.org/resources/faq/#whatsearchenginessupport (20 March 2003)</li>

<li><a name="flanagan" >Flanagan, D.  1997.  <em>JavaScript: the definitive guide</em>.  Sebastopol, CA: O’Reilly.</li>

<li><a name="frants"></a>Frants, V.I., Shapiro, J. &amp; Voiskunskii, V.G. (1997)  <em>Automated informaton retrieval: theory and methods.</em>  San Diego, CA: Academic Press.</li>

<li><a name="harmon"></a>Harmon, A. (2003) Google deal ties company to Weblogs. <em>The New York Times</em> Monday, February 17, C3. </li>

<li><a name="houston"></a>Houston, J. E. (2001)  <em>Thesaurus of ERIC descriptors</em>, 14th edition.  Westport, CT: Oryx Press.</li>

<li><a name="jacobs"></a>Jacobs, I. (2002).  <em><a href="http://www.w3.org/TR/2002/WD-Webarch-20020830/">Architectural principles of the World Wide Web, W3C working draft.</a></em>. World Wide Web Consortium (W3C.org) http://www.w3.org/TR/2002/WD-Webarch-20020830/  (20 March 2003)</li>

<li><a name="jesdanun"></a>Jesdanun, A. (2002) <a href="http://www.news.com.au/common/printpage/0,6093,5356761,00.html">Report: sites missing from Google.</a>&nbsp;&nbsp;News.com.au &nbsp;&nbsp;   http://www.news.com.au/common/printpage/0,6093,5356761,00.html (20 March 2003)</li>

<li><a name="koehler"></a>Koehler, W. (1999)  <a href="../4-4/paper60.html">Digital libraries and the World Wide Web sites and page persistence.</a>  <em>Information Research</em>, <b>4</b>, (4) http://informationr.net/ir/4-4/paper60.html (23 March 2003)</li>

<li><a name="luhn"></a>Luhn, H. P.  (1959) The automatic creation of literature abstracts.  <em>IBM Journal of Research and Development</em> <strong>2</strong>(2), 159-165.</li>

<li><a name="markwell"></a>Markwell, J. &amp; Brooks, D.W. (2002)  <a href="http://www-class.unl.edu/biochem/url/broken_links.html">Broken links: just how rapidly do science education hyperlinks go extinct?</a>  Lincoln, NE: University of Nebraska-Lincoln. Department of Biochemistry.  http://www-class.unl.edu/biochem/url/broken_links.html  (20 March 2003)</li>

<li><a name="mintz"></a>Mintz, A. (2002) Web of deception: misinformation on the Internet.  Medford, NJ: Information Today.</li>

<li><a name="pierson"></a>Pierson, H. (2003)  <a href="http://msdn.microsoft.com/msdnmag/issues/03/03/SiteSkinning/default.aspx">Site skinning: rich XML classes let users personalize their visual experience on your ASP.NET site</a>.  <em>MSDN Magazine</em>, March, 87-92.  http://msdn.microsoft.com/msdnmag/issues/03/03/SiteSkinning/default.aspx (20 March 2003)</li>

<li><a name="salton"></a>Salton, G. &amp; McGill, M.J. (1988)  <em>Introduction to modern information retrieval</em>.  New York, NY: McGraw-Hill.</li>

<li><a name="spinellis"></a>Spinellis, D. (2003) The decay and failures of Web references. <em>Communications of the ACM</em>, <b>46</b>(1), 71-77</li>

<li><a name="sullivan932002"></a>Sullivan, D. (2002a) <a href="http://searchenginewatch.com/sereport/02/09-google.html">Google: Can the Marsha Brady of search stay sweet?</a> &nbsp;&nbsp;Search Engine Watch.   http://searchenginewatch.com/sereport/02/09-google.html  (20 March 2003) </li>

<li><a name="sullivan9172002"></a>Sullivan, D. (2002b) <a href="http://www.searchenginewatch.com/reports/netratings.html">Nielsen//NetRatings search engine ratings.</a>&nbsp;&nbsp;Search Engine Watch.  
http://www.searchenginewatch.com/reports/netratings.html (20 March 2003)</li>

<li><a name="Sullivan1012002"></a>Sullivan, D. (2002c) <a href="http://searchenginewatch.com/sereport/02/10-meta.html">Death of a meta tag</a> Search Engine Watch&nbsp;&nbsp; http://searchenginewatch.com/sereport/02/10-meta.html (20 March 2003)</li>

<li><a name="task"></a>Task Group on Implementation of Integrating Resources.  (2001)  <em><a href="http://www.loc.gov/catdir/pcc/tgintegrpt.html">Interim report.</a></em> Washington, DC: Library of Congress.&nbsp;&nbsp;  http://www.loc.gov/catdir/pcc/tgintegrpt.html (20 March 2003)</li>

<li><a name="turau"></a>Turau, V.  (1999)  <a href="http://www.informatik.fh-wiesbaden.de/~turau/ps/legacy.pdf">Making legacy data accessible for XMl applications</a>.  [Personal Web site]
http://www.informatik.fh-wiesbaden.de/~turau/ps/legacy.pdf (20 March 2003)</li>

<li><a name="vartanian"></a>Vartanian, I. (2001) <em>Now loading....</em> Coret Madera, CA: Gingko Press</li>
		
<li><a name="waterson" ></a>Waterson, C. (2002)  <a href="http://www.mozilla.org/newlayout/doc/gecko-overview.htm">Introduction to layout in Mozilla</a>.&nbsp;&nbsp;Mozilla.org&nbsp;&nbsp; http://www.mozilla.org/newlayout/doc/gecko-overview.htm (20 March 2003)</li>

<li><a name="wiegand"></a>Wiegand, W.A. (1996)  <em>Irrepressible reformer, A biography of Melvil Dewey.</em>  Chicago, IL: American Library Association.</li>

<li><a name="yates"></a>Yates, J. (2000) Business use of information and technology during the industrial age, in <em>A Nation transformed by information: how information has shaped the United States from Colonial Times to the Present</em>, edited by A.D. Chandler &amp; J.W. Cortada. pp. 107-136.  New York, NY: Oxford University Press.</li>
</ul>

<hr style="COLOR: #000080" size="1" />
<table cellspacing="10" align="center">
<tr><td colspan="2" align="center" style="font-family: verdana; font-size: small; font-weight: bold;">Find other papers on this subject.<br /></td></tr>
<tr><td align="center" valign="top">
<center>
<form method="get" action="http://scholar.google.com/scholar" target="_blank">
<table bgcolor="#ffffff">
<tr><td nowrap="nowrap" valign="top" align="center" height="32">
<input type="text" name="q" size="31" maxlength="255" value="(Web OR &quot;World Wide Web&quot;) searching &quot;information retrieval&quot;" style="background-color: Yellow;"></input> <br />
<input type="submit" name="sa" value="Scholar Search"  style="font-family: Verdana; font-weight: bold;"></input>
<input type="hidden" name="num" value="100"></input>
</td></tr></table></form>
</center>
<td align="center" valign="top">
<!-- Search Google -->
<center>
<form method="get" action="http://www.google.com/custom" target="_top">
<table bgcolor="#ffffff">
<tr><td nowrap="nowrap" valign="top" align="center" height="32">
<input type="text" name="q" size="31" maxlength="255" value="(Web OR &quot;World Wide Web&quot;) searching &quot;information retrieval&quot;" style="background-color: Yellow;"></input><br />
<input type="submit" name="sa" value="Google Search" style="font-family: Verdana; font-weight: bold;" /></input>
<input type="hidden" name="client" value="pub-5081678983212084"></input>
<input type="hidden" name="forid" value="1"></input>
<input type="hidden" name="ie" value="ISO-8859-1"></input>
<input type="hidden" name="oe" value="ISO-8859-1"></input>
<input type="hidden" name="cof" value="GALT:#0066CC;GL:1;DIV:#999999;VLC:336633;AH:center;BGC:FFFFFF;LBGC:FF9900;ALC:0066CC;LC:0066CC;T:000000;GFNT:666666;GIMP:666666;FORID:1;"></input>
<input type="hidden" name="hl" value="en"></input>
</td></tr></table>
</form>
</center>
<!-- Search Google -->
</td></tr>
</table>

<hr style="COLOR: #000080" size="1" />
<div align="center">
<h4>How to cite this paper:</h4>
Brooks, Terrence A.&nbsp;(2003) "Web Search: how the Web has changed information retrieval" &nbsp; <em>Information Research</em>, <strong>8</strong>(3) paper no. 154 [Available at http://InformationR.net/ir/8-3/paper154.html] </div>
<hr color=#ff00ff size=1>

<div align=center>&copy; the author, 2003. <br>Last updated: 20 March 2003</div>
<hr color="#ff00ff" size="1">
								<div align="center">Articles citing this paper, <a href="http://scholar.google.com/scholar?hl=en&amp;lr=&amp;q=link:BTpIW8Mb8jYJ:scholar.google.com/" target="_blank">according to Google Scholar</a></div>
								 <hr color="#ff00ff" size="1">



<table border="0" cellpadding="15" cellspacing="0" align="center">
<tr> 
    <td><a href="infres83.html"><h4>Contents</h4></a></td>
   <td align="center" valign="top"><h5 align="center"><img src="http://counter.digits.com/wc/-d/-z/6/-b/FF0033/paper154" ALIGN=middle  WIDTH=60 HEIGHT=20 BORDER=0 HSPACE=4 VSPACE=2><br><a href="http://www.digits.com/">Web Counter</a></h5></td>
    <td><a href="../index.html"><h4>Home</h4></a></td>
  </tr>
</table>
<hr color=#ff00ff size=3>
<script src="http://www.google-analytics.com/urchin.js" type="text/javascript">
</script>
<script type="text/javascript">
_uacct = "UA-672528-1";
urchinTracker();
</script>
</body>
<!-- Mirrored from informationr.net/ir/8-3/paper154.html by HTTrack Website Copier/3.x [XR&CO'2010], Sat, 25 Dec 2010 19:11:22 GMT -->
</html>
