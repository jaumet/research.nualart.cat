<!doctype html public "-//W3C//DTD HTML 4.0 Transitional//EN">
<html>

<!-- Mirrored from informationr.net/ir/8-3/paper152.html by HTTrack Website Copier/3.x [XR&CO'2010], Sat, 25 Dec 2010 19:11:20 GMT -->
<head>
<title>The IIR evaluation model: a framework for evaluation of interactive information retrieval systems</title>
<link rel="stylesheet" href="../IRstyle.css" />
<meta http-equiv=content-type content="text/html; charset=windows-1252" />
<meta name="keywords"  content="evaluation, interactive information retrieval systems, evaluation, IR interaction, information searching, information retrieval, relevance assessments, scenarios, simulated work task situations, performance measures, information need, situational relevance, relative relevance, ranked half-life, cumulated gain, cumulated gain with discount" >
<meta name="description" content="An alternative approach to evaluation of interactive information retrieval (IIR) systems is proposed.  The model provides a framework for the collection and analysis of IR interaction data." />
<meta name="rating" content="mature" />
<meta name="vw96.objecttype" content="document" />
<meta content="all" name=robots />
<meta name="DC.Title" content="The IIR evaluation model: a framework for evaluation of interactive information retrieval systems" />
<meta name="DC.Creator" content="Borlund, Pia" />
<meta name="DC.Subject" content="evaluation, interactive information retrieval systems, evaluation, IR interaction, information searching, information retrieval, relevance assessments, scenarios, simulated work task situations, performance measures, information need, situational relevance, relative relevance, ranked half-life, cumulated gain, cumulated gain with discount" />
<meta name="DC.Description" content="An alternative approach to evaluation of interactive information retrieval (IIR) systems, referred to as the IIR evaluation model, is proposed.  The model provides a framework for the collection and analysis of IR interaction data.  The aim of the model is two-fold: 1) to facilitate the evaluation of IIR systems as realistically as possible with reference to actual information searching and retrieval processes, though still in a relatively controlled evaluation environment; and 2) to calculate the IIR system performance taking into account the non-binary nature of the assigned relevance assessments.  The IIR evaluation model is presented as an alternative to the system-driven Cranfield model which still is the dominant approach to the evaluation of IR and IIR systems.  Key elements of the IIR evaluation model are the  use of realistic scenarios, known as simulated work task situations, and the (call for) alternative performance measures.  A simulated work task situation, which is a short ‘cover story’, serves two main functions: 1) it triggers and develops a simulated information need by allowing for user interpretations of the situation, leading to cognitively individual information need interpretations as in real life; and 2) it is the platform against which situational relevance is judged.  Further, by being the same for all test persons experimental control is provided.  Hence, the concept of a simulated work task situation ensures the experiment both realism and control.  Guidelines and recommendations for the application of simulated work task situations are provided.  Examples of alternative performance measures are: relative relevance (RR), ranked half-life (RHL), cumulated gain (CG) and cumulated gain with discount (DCG). These measures can incorporate non-binary relevance assessments, necessary due to the result of realistic interaction and relevance assessment behaviour of users in the process of searching and assessing relevance of retrieved information objects." />
<meta name="DC.Type" content="text" />
<meta name="DC.Identifier" scheme="ISSN" content="1368-1613" />
<meta name="DC.Identifier" content="paper152.html" /> 
<meta name="DC.Relation.IsPartOf" content="infres83.html" />
<meta name="DC.Format" content="text/html" />
<meta name="DC.Language" content="en" />
<meta name="DC.Date.Available" content="2003-04-15" />
<meta name="DC.Publisher" content="Professor T.D. Wilson" />
<meta name="DC.Rights" content="http://creativecommons.org/licenses/by-nd-nc/1.0/" />
<meta name="DC.Coverage.placename" content=global />
<script language="JavaScript" type="text/JavaScript">
<!--
function MM_preloadImages() { //v3.0
  var d=document; if(d.images){ if(!d.MM_p) d.MM_p=new Array();
    var i,j=d.MM_p.length,a=MM_preloadImages.arguments; for(i=0; i<a.length; i++)
    if (a[i].indexOf("#")!=0){ d.MM_p[j]=new Image; d.MM_p[j++].src=a[i];}}
}

function MM_findObj(n, d) { //v4.01
  var p,i,x;  if(!d) d=document; if((p=n.indexOf("?"))>0&&parent.frames.length) {
    d=parent.frames[n.substring(p+1)].document; n=n.substring(0,p);}
  if(!(x=d[n])&&d.all) x=d.all[n]; for (i=0;!x&&i<d.forms.length;i++) x=d.forms[i][n];
  for(i=0;!x&&d.layers&&i<d.layers.length;i++) x=MM_findObj(n,d.layers[i].document);
  if(!x && d.getElementById) x=d.getElementById(n); return x;
}

function MM_nbGroup(event, grpName) { //v6.0
  var i,img,nbArr,args=MM_nbGroup.arguments;
  if (event == "init" && args.length > 2) {
    if ((img = MM_findObj(args[2])) != null && !img.MM_init) {
      img.MM_init = true; img.MM_up = args[3]; img.MM_dn = img.src;
      if ((nbArr = document[grpName]) == null) nbArr = document[grpName] = new Array();
      nbArr[nbArr.length] = img;
      for (i=4; i < args.length-1; i+=2) if ((img = MM_findObj(args[i])) != null) {
        if (!img.MM_up) img.MM_up = img.src;
        img.src = img.MM_dn = args[i+1];
        nbArr[nbArr.length] = img;
    } }
  } else if (event == "over") {
    document.MM_nbOver = nbArr = new Array();
    for (i=1; i < args.length-1; i+=3) if ((img = MM_findObj(args[i])) != null) {
      if (!img.MM_up) img.MM_up = img.src;
      img.src = (img.MM_dn && args[i+2]) ? args[i+2] : ((args[i+1])? args[i+1] : img.MM_up);
      nbArr[nbArr.length] = img;
    }
  } else if (event == "out" ) {
    for (i=0; i < document.MM_nbOver.length; i++) {
      img = document.MM_nbOver[i]; img.src = (img.MM_dn) ? img.MM_dn : img.MM_up; }
  } else if (event == "down") {
    nbArr = document[grpName];
    if (nbArr)
      for (i=0; i < nbArr.length; i++) { img=nbArr[i]; img.src = img.MM_up; img.MM_dn = 0; }
    document[grpName] = nbArr = new Array();
    for (i=2; i < args.length-1; i+=2) if ((img = MM_findObj(args[i])) != null) {
      if (!img.MM_up) img.MM_up = img.src;
      img.src = img.MM_dn = (args[i+1])? args[i+1] : img.MM_up;
      nbArr[nbArr.length] = img;
  } }
}
//-->
</script>
</head>
<body bgcolor="#ffffff" onLoad="MM_preloadImages('../figs/iauthori1.gif','../figs/isubji1.gif','../figs/isearch1.gif','../figs/ihome1.gif','../figs/contents1.gif')">
<table align="center" border="0" cellpadding="0" cellspacing="0">
<tr><td height="30" align="center" colspan="5"><h4>Information Research, Vol. 8  No. 3, April 2003</h4></td></tr>
  <tr> 
    <td><a href="infres83.html" target="_top" onClick="MM_nbGroup('down','group1','contents','',1)" onMouseOver="MM_nbGroup('over','contents','../figs/contents1.gif','',1)" onMouseOut="MM_nbGroup('out')"><img name="contents" src="../figs/contents.gif" border="0" alt="" onLoad=""></a></td>
    <td><a href="../iraindex.html" target="_top" onClick="MM_nbGroup('down','group1','authorindex','',1)" onMouseOver="MM_nbGroup('over','authorindex','../figs/iauthori1.gif','',1)" onMouseOut="MM_nbGroup('out')"><img src="../figs/iauthori.gif" alt="" name="authorindex" width="120" height="20" border="0" onload=""></a></td>
    <td><a href="../irsindex.html" target="_top" onClick="MM_nbGroup('down','group1','subjindex','',1)" onMouseOver="MM_nbGroup('over','subjindex','../figs/isubji1.gif','',1)" onMouseOut="MM_nbGroup('out')"><img src="../figs/isubji.gif" alt="" name="subjindex" width="120" height="20" border="0" onload=""></a></td>
    <td><a href="../search.html" target="_top" onClick="MM_nbGroup('down','group1','search','',1)" onMouseOver="MM_nbGroup('over','search','../figs/isearch1.gif','',1)" onMouseOut="MM_nbGroup('out')"><img src="../figs/isearch.gif" alt="" name="search" width="120" height="20" border="0" onload=""></a></td>
    <td><a href="../index.html" target="_top" onClick="MM_nbGroup('down','group1','home','',1)" onMouseOver="MM_nbGroup('over','home','../figs/ihome1.gif','',1)" onMouseOut="MM_nbGroup('out')"><img name="home" src="../figs/ihome.gif" border="0" alt="" onLoad=""></a></td>
  </tr>
</table>
<hr color=#ff00ff size=3>

<h1>The IIR evaluation model: a framework for evaluation of interactive
information retrieval systems</h1>

<h4 align=center><a href="mailto:pb@db.dk">Pia Borlund</a><br>Department of
Information Studies <br>Royal School of Library and Information Science,
Aalborg branch<br>Aalborg, Denmark</h4><br> <div
align=center><b>Abstract</b></div>

<blockquote>An alternative approach to evaluation of interactive information
retrieval (IIR) systems, referred to as the IIR evaluation model, is proposed.
The model provides a framework for the collection and analysis of IR
interaction data.  The aim of the model is two-fold: 1) to facilitate the
evaluation of IIR systems as realistically as possible with reference to actual
information searching and retrieval processes, though still in a relatively
controlled evaluation environment; and 2) to calculate the IIR system
performance taking into account the non-binary nature of the assigned relevance
assessments.  The IIR evaluation model is presented as an alternative to the
system-driven Cranfield model (<a href="#cle66b">Cleverdon, Mills &amp; Keen,
1966</a>; <a href="#cle66a"> Cleverdon &amp; Keen, 1966</a>) which still is the
dominant approach to the evaluation of IR and IIR systems.  Key elements of the
IIR evaluation model are the  use of realistic scenarios, known as simulated
work task situations, and the (call for) alternative performance measures.  A
simulated work task situation, which is a short ‘cover story’, serves two main
functions: 1) it triggers and develops a simulated information need by allowing
for user interpretations of the situation, leading to cognitively individual
information need interpretations as in real life; and 2) it is the platform
against which situational relevance is judged.  Further, by being the same for
all test persons experimental control is provided.  Hence, the concept of a
simulated work task situation ensures the experiment both realism and control.
Guidelines and recommendations for the application of simulated work task
situations are provided.  Examples of alternative performance measures are:
relative relevance (RR), ranked half-life (RHL) (<a href="#bor98">Borlund &amp;
Ingwersen, 1998</a>), cumulated gain (CG) and cumulated gain with discount
(DCG) (<a href="#jär00">Järvelin &amp; Kekäläinen,  2000</a>).&nbsp; These
measures can incorporate non-binary relevance assessments, necessary due to the
result of realistic interaction and relevance assessment behaviour of users in
the process of searching and assessing relevance of retrieved information
objects.</blockquote>

<h2>Introduction</h2>

<p>Various researchers (e.g., <a href="#sar95"> Saracevic, 1995</a>; <a
href="#har96"> Harter, 1996</a>; <a href="#beu96"> Beaulieu, Robertson &amp;
Rasmussen, 1996</a>; <a href="#ell96b"> Ellis, 1996b</a>; <a href="#bor97">
Borlund &amp; Ingwersen, 1997</a>; <a href="#kek02"> Kekäläinen &amp; Järvelin,
2002</a>) have expressed a demand for alternative approaches to the performance
evaluation of interactive information retrieval systems (IIR systems).  That
is, alternative to the experimental Cranfield model which still is the dominant
evaluation approach to the evaluation of IR and IIR systems.  The Cranfield
model derives directly from Cranfield II (<a href="#cle66b">Cleverdon, Mills
&amp; Keen, 1966</a>; <a href="#cle66a"> Cleverdon &amp; Keen, 1966</a>) and is
based on the principle of test collections, that is: a collection of documents;
a collection of queries; and a collection of relevance assessments.  The
Cranfield model includes also the measurement of recall and precision ratios as
indicators of system performance.  The Cranfield model constitutes the
empirical research tradition of the development and testing of IR systems
employed by the system-driven approach to IR (e.g., <a href="#swa86"> Swanson,
1986</a>; <a href="#ell96a"> Ellis, 1996a</a>).  The emphasis in this research
tradition is on controlled laboratory tests.  The objective of the Cranfield
model is to keep all variables controlled and to obtain results, about which
one can state conclusions about retrieval systems in general (Robertson <a
href="#rob81">1981</a>: 12).  However, the Cranfield model suffers from
limitation due to its restricted assumptions on the cognitive and behavioural
features of the environment in which (I)IR systems function (Ellis <a
href="#ell96a">1996a</a>: 20). These limitations are the reasons for the demand
for alternative approaches to evaluation of IR and IIR systems.  In brief, the
demand is described and summarised with the <i> three revolutions</i> put
forward by Robertson and Hancock-Beaulieu (<a href="#rob92">1992</a>, pp.
458-459).</p>

<ul> <li>The cognitive revolution;</li> <li>The relevance revolution; and</li>
<li>The interactive revolution.</li> </ul>

<p>The <i> cognitive</i> and <i> relevance revolutions</i> require realism with
reference to the formation of information need, and relevance assessment
processes.  This means that in the context of (I)IR evaluation an information
need ought to be treated as a user-individual and potentially dynamic concept,
and the multidimensional <i> and</i> dynamic nature of relevance should be
taken into account, just as relevance should be judged against the information
need situation, not the query or even request, and by the person who owns the
information need.  The <i> interactive revolution</i> points to the fact that
IR systems have become more interactive.  Due to the type of IR system, which
IIR systems constitute, that is, systems where the user dynamically conducts
searching tasks and correspondingly reacts to system responses over session
time, the evaluation of IIR systems consequently has to include the user's
interactive information searching and retrieval processes.</p>


<p>The three revolutions point to requirements that are not fulfilled by the
system-driven IR evaluation approach based on the Cranfield model.  The
Cranfield model does not deal with dynamic information needs but treats
information needs as a static concept entirely reflected by the search
statement (query).  This implies the assumption that learning and modifications
by users are confined to the search statement alone.  Furthermore, this model
uses only binary and topical-oriented relevance.  The conclusion is that the
batch-driven mode of the Cranfield model is not suitable for the evaluation of
IIR systems, which, if carried out as realistically as possible, requires human
interaction, potentially dynamic information need interpretations, and the
assignment of multidimensional and dynamic relevance.</p>

<p>It could be argued that the second main approach to IR systems evaluation,
the user-oriented approach fulfils the requirements outlined.  As opposed to
the system-driven approach the user-centred approach defines the IR system much
broader, viewing the seeking and retrieval processes as a whole.  The main
purpose of this type of evaluation is concerned with how well the user, the
retrieval mechanism, and the database interact extracting information, under
real-life operational conditions.  In this approach the relevance judgements
have to be given by the original user in relation to his or her personal
information need which may change over session time.  The assumption is that
the relevance judgements represent the value of the information objects for a
particular user at a particular point in time, hence the assessments can only
be made by the user at that time. Further, relevance is judged in accordance to
subjective situational relevance in a non-binary way.  As such the requirement
as to realism is fulfilled.  But like the system-driven approach the
user-oriented approach quantifies performance effectiveness as (relative)
recall and precision ratios in spite of collecting non-binary-based relevance
assessments (e.g., <a href="#lan69"> Lancaster, 1969</a>).</p>

<p>Briefly summarised the qualities of the two main evaluation approaches are
similar to the conflict issues between them (<a href="#rob92">Robertson &amp;
Hancock-Beaulieu, 1992</a>: 460).  The two approaches represent different
viewpoints which each aim at the same goal: reliability of the IR test
performance results.  To the system-driven approach reliability of the
experimental results is earned through control over the experimental variables
and the repeatability of the experiments.  In contrast, the user-oriented
approach puts the user in focus with reference to system development, design,
and evaluation which is basically carried out according to the (potential)
end-user's information use, retrieval, and searching behaviour with the
objective of obtaining realistic results.  To the user-oriented approach the
results become reliable by being loyal to the IR and searching processes.  So
far the approaches have carried out satisfying jobs, however, this changes with
the development of IIR systems as illustrated by the <i> interactive
revolution</i>.  IIR systems are by definition broader in scope than
traditional IR systems.  By incorporating the interface functionality as well
as communicative behaviour by users IIR systems are defined just as broadly as
in the user-oriented approach, and the focus of the evaluation is similarly
wider than in non-interactive IR.  The foci of IIR evaluation include all the
user’s activities of interaction with the retrieval and feedback mechanisms as
well as the retrieval outcome itself.  The overall purpose of the evaluation of
IIR systems is to evaluate the systems in a way which takes into account the
dynamic natures of information needs and relevance as well as reflects the
interactive information searching and retrieval processes.  Thus, a hybrid
evaluation approach is proposed – <i> a combination of elements</i> from the
two main approaches, the issue of experimental control plus the user-individual
and dynamic nature of information needs and relevance assessments – as a
reasonable setting for an alternative evaluation approach to evaluation of IIR
systems.   In addition, the dominating use of the ratios of recall and
precision for the measurement of effectiveness of IR performance forces us to
reconsidered whether these measures are sufficient in relation to the
effectiveness evaluation of IIR systems.</p>

<p>As such, the present paper contributes to the continuing development and
refinement of evaluation approaches to the research area of IR by proposing a
framework for evaluation of IIR systems and information searching behaviour –
the so-called IIR evaluation model.  The paper is organised according to the
following main sections: first, the IIR evaluation model is presented, the
three parts concerning data collection and data analysis – that is, 1) the
basic components; 2) recommendations for the applications of simulated work
task situation; and 3) alternative performance measures.  The penultimate
section looks into the provisional use of the IIR evaluation model – or parts
of the model, and hereby verifying the need for a framework for evaluation of
IIR systems.  The final section closes the paper with summary statements.</p>

<h2>The IIR evaluation model</h2>

<p>The present paper describes the framework as an aggregated model for the
evaluation of IIR systems and/or information seeking behaviour – including
rationale and recommendations for application of the model – or parts of the
model.&nbsp; During the ongoing process of developing the framework parts of
the model have been described in previous publications (e.g., <a href="#bor97">
Borlund &amp; Ingwersen, 1997</a>; <a href="#bor98"> Borlund &amp; Ingwersen,
1998</a>; <a href="#bor99"> Borlund &amp; Ingwersen, 1999</a>; <a
href="#bor00b">Borlund, 2000b</a>).  Basically, the IIR evaluation model
consists of three parts:</p>

<ul style="list-style-image: none; list-style-type: none; margin-right: 20%;"><li>
Part 1. A set of components which aims at ensuring a functional, valid, and realistic
setting for the evaluation of IIR systems;<br>
</li> 
<li>
Part 2. Empirically based recommendations for the application of the concept of a simulated work task situation [<a href="#1">1</a>]; and<br> 
</li>
<li>
Part 3. Alternative performance measures  capable of managing non-binary based relevance assessments.
</li></ul> 

<p>Parts 1 and 2 concern the collection of data, whereas part 3 concerns data
analysis. The three model parts are described in the following
sub-sections.</p>

<h3>Part 1: The components of the experimental setting</h3>

<p>The aim of the proposed experimental setting is to facilitate evaluation of
IIR systems in a way which is as close as possible to actual information
searching and IR processes, though still in a relatively controlled evaluation
environment.  This can be achieved by the use of the <i> proposed
components</i> of model part 1:</p>

<ul> <li>The involvement of potential users as test persons;&nbsp;</li> <li>The
application of individual <i> and </i> potentially dynamic information need
interpretations; and</li> <li>The assignment of multidimensional <i> and</i>
dynamic relevance assessments.</li> </ul>

<p>The basic idea is to test and evaluate by use of the users for whom a given
system is developed, and, through the users' natural use and interaction with
the system (or systems), to gain knowledge about the system(s).  Thus, the
three components are strongly interconnected.  Because without the involvement
of potential users as test persons, there would be no human interaction with
the systems during system evaluation.  Without human interaction, there would
be no application of individual and potentially dynamic information need
interpretations.  And without human involvement and the application of
individual and potentially dynamic information need interpretations, there
would be no assignment of multidimensional <i> and</i> dynamic relevance.  The
three components are thus necessary in order to carry out evaluation of IIR
systems in a way that is close to actual information searching and retrieval
processes.  The application of the components allows for the collection of both
traditional <i>system-oriented data</i> (i.e., data about system performance)
and <i> cognitive data</i> (i.e., data which inform about the behaviour of, and
experiences obtained by, the test persons when working with the retrieved
information objects and the system facilities).  One may look at the results of
every one of the iterations or only the final one.  One may hence observe the
number of iterations but also follow the development of the different types of
relevance or relevance criteria used by the test persons during the experiment.
  This aspect of system evaluation makes it more forceful, as opposed to the
traditional system-driven approach signified by the Cranfield model, since it
provides vital data on hitherto unknown properties, like shifts of focus of the
continuous interaction.  The reason is that one may allow test persons
simultaneously to provide different types of relevance for each information
object assessed per iteration.</p>

<p>The second component, the application of individual and potentially dynamic
information need interpretations, is founded on the information need
development and formation theory of the cognitive viewpoint in which an
information need is seen as a user-individual and dynamic concept that develops
as a consequence of a <i> problematic situation</i> (e.g., <a
href="#wer71">Wersig, 1971</a>; <a href="#bro80">Brookes, 1980</a>; <a
href="#bel80">Belkin, 1980</a>; <a href="#bel82">Belkin, <i>et al.</i>,
1982</a>; <a href="#ing92"> Ingwersen, 1992</a>; <a href="#ing96">1996</a>).
Thus, the introduction of the concept of a simulated work task situation, which
is essential to the experimental setting, because of its function to ensure the
experiment both realism and control.  Generally, the issues of experimental <i>
realism</i> and <i> control</i> are ensured by the application of all three
basic components.  Specifically, the simulated work task situation is the
realism and control ensuring device.  As in real life, the simulated work task
situation is to be seen as the cause of the 'breakdown situation' in the sense
of Winograd and Flores (<a href="#win86">1986</a>), a cognitive state which
creates an information need which has to be satisfied in order for the user to
be able to deal with the situation and move on.  The issue of realism is also
ensured by the involvement of test persons (potential users) who, based on the
simulated work task situation develop individual and subjective information
need interpretations.  Individually, the test persons interactively search,
modify and dynamically assess relevance of the retrieved information objects in
relation to their perceptions of the information needs and the underlying
simulated work task situation.  Furthermore, the involvement of potential
end-users as test persons provides for the possibility of applying real
information needs [<a href="#2">2</a>] in the experiment.  The application of
real information needs serves a twofold purpose: 1) they act as the baseline
(or control group in the sense of a classic experiment) against the simulated
information needs, both at a specific test person level and at a more general
level; and 2) they provide information about the systems' effect on real
information needs.  As for the issue of experimental control the application of
simulated work task situations ensure control by being the same for all the
test persons.  Or said differently, because the involved test persons all
search the same set of simulated work task situations, control is gained and
the search results can be compared across the systems and/or system components
as well as across the group of test persons.</p>


<h4>The simulated work task situation</h4>

<p>	A simulated work task situation is a short 'cover story' that describes
a situation that leads to an individual requiring to use an IR system.  The
'cover-story' is, semantically, a rather open description of the
context/scenario of a given work task situation.  The concept of simulated work
task situation derives from Ingwersen's cognitive communication models (e.g.,
<a href="#ing92"> Ingwersen, 1992</a>: 135; <a href="#ing96"> Ingwersen,
1996</a>: 9) and the application of the work task concept by Byström and
Järvelin (<a href="#bys95">1995</a>) to information problem solving and
information seeking processes.  The simulated work task situation serves two
main functions: 1) it triggers and develops a simulated information need by
allowing for user interpretations of the situation, leading to cognitively
individual information need interpretations as in real life; and 2) it is the
platform against which situational relevance is judged (<a
href="#bor97">Borlund &amp; Ingwersen, 1997</a>: 227-228).  With reference to
the process of information need development the simulated work task situation
more specifically helps to describe to the test persons:</p>

<ul> <li>The source of the information need;</li> <li>The environment of the
situation;</li> <li>The problem which has to be solved; and also</li>
<li>Serves to make the test person understand the objective of the search<br>
(<a href="#bor97">Borlund &amp; Ingwersen, 1997</a>: 229). </li></ul>
<p>In our setting the simulated work task situation is a stable concept, i.e.,
the given purpose and goal of the retrieval. This makes experimental control
possible by providing comparable cognitive and performance data in relation to
simulated information needs for the same data collection, ideally across
different IR techniques, but at least for one single technique.  Figure 1 shows
an example of a simulated situation/simulated work task situation.</p>

<table align="center" border="1" cellpadding=4 width="650" bgcolor="#FDFFBB" >
<tr><td valign=top width="100%"> <b>Simulated situation:</b><br><br>
<b>Simulated work task situation:</b>  After your graduation you will be
looking for a job in industry.  You want information to help you focus your
future job seeking.  You know it pays to know the market.  You would like to
find some information about employment patterns in industry and what kind of
qualifications employers will be looking for from future employees.<br><br>
<b>Indicative request:</b> Find, for instance, something about future
employment trends in industry, i.e., areas of growth and decline.</td></tr>
<caption align="bottom"><br><strong>Figure 1.  Example of a simulated
situation/simulated work task situation (<a href="#bor00a">Borlund, 2000a</a>;
<a href="#bor00b">2000b</a>).</strong></caption> </table>

<p><i>Simulated situation</i> is the label of the overall frame of the <i>
simulated work task situation</i> and the <i> indicative request</i>.  The
indicative request is a suggestion to the test person about what to search for.
  It is, however, not to be seen as an example of the underlying need of the
particular simulated work task situation.  Comparative analyses of test
persons' employment of simulated situations with or without indicative requests
have been made in order to verify possible consequences of the composition of
the simulated situation in order to determine and recommend whether or not an
<i> indicative request </i> biases the test persons' search formulations, and
whether it should be included or not in the <i> simulated situation</i>.  The
test results showed that the indicative request does not bias the search
formulations and search behaviour, and consequently it is optional whether one
wants to make use of indicative requests or not.  The tests and results have
previously been published in Borlund (<a href="#bor00a">2000a</a>; <a
href="#bor00b">2000b</a>) and are summarised below in the section on <a
href="#part2">Part 2</a> of the model.</p>

<p>A distinction is made between work task and search task.  The work task
initiates the search task, and the search task is carried out in order to meet
the requirements of the work task. Allen (<a href="#all98">1996</a>) also makes
a dual distinction, however, both referring to search task.  Allen (<a
href="#all98">1996</a>; 29) suggests that:<p/>&nbsp;&nbsp;&nbsp;

<blockquote> ...one approach to task analysis is to determine what tasks are
accomplished by users as they attempt to meet their information needs and how
those tasks are  performed. A second approach to task analysis identifies and
analyzes the tasks associated with using information devices.  (<a
href="#all98">Allen, 1996</a>: 29).</blockquote>

<p>The distinction made by Allen is similar to how Wilson (<a
href="#wil99">1999</a>; 840) differentiates between information seeking
behaviour and information searching behaviour in his 'nested model of
conceptual areas of information behaviour'.  Further, the first approach
outlined by Allen (<a href="#all98">1996</a>) is illustrated with the research
by Byström and Järvelin   (<a href="#bys95">1995</a>) and Vakkari (<a
href="#vak99">1999</a>).  Byström and Järvelin   (<a href="#bys95">1995</a>)
investigate the work task complexity of public administration workers.  So does
Vakkari (1999), who reports on the effect task complexity has on the public
administration workers' subsequent information seeking behaviour.  The second
approach pointed out by Allen concerns the analysis of search task, that is,
the actions and activities necessary for the user to perform during IR in order
to find relevant objects required by the work task as perceived by the user –
and is often used as a user-oriented approach to the evaluation of IR systems
as, for instance, in the Okapi projects  (e.g., <a href="#wal89">Walker,
1989</a>; <a href="#beu98"> Beaulieu &amp; Jones,   1998</a>). Another example
of search task analysis is the IR system evaluation carried out within the
health care domain by Hersh <i>et al.,</i> (<a href="#her96">1996</a>).  The
search task approach concerns what is commonly referred to as the user's
seeking and retrieval behaviour, which again may be seen as a consequence of
the type of work task identified in the first approach.  Byström and Järvelin
(<a href="#bys95">1995</a>: 194-195) divide work task complexity into five
categories, ranging from a genuine decision task to an automatic information
processing task, according to the pre-determinability of the information
requirement of the given work task.  Vakkari (<a href="#vak99">1999</a>: 834)
points to the relationship between task complexity and structure of the
information problem as crucial factors in determining task performance and
concludes that:</p>

<blockquote>'...they are connected to the types of information people are
looking for and using, to patterning of search strategies, and to choice of
relevance criteria in tasks.'  </blockquote>

<p>The concept of task is also used within the research area of human-computer
interaction (HCI) and because this area recently has overlapped the research
area of IR as the result of the introduction of end-user-oriented and IIR
systems the HCI task concept has started to appear in the IR literature.  The
HCI task concept is, as in the case of Reid (<a href="#rei99">1999</a>; <a
href="#rei00">2000</a>) denoted as 'task' only, and is defined similarly to
that of search task (e.g., <a href="#all98"> Allen, 1996</a>; <a
href="#dia89a">Diaper, 1989a</a>; <a href="#dia89b">1989b</a>; <a
href="#dia89c">1989c</a>; <a href="#pre94"> Preece <i>et al.</i>, 1994</a>).
HCI task analysis is the analysis of<br>

<blockquote> '…what the human has to do (or think he or she has to do) in order
to accomplish a goal… Thus, we can define a  <b> task</b>…as the activities
required, used or believed to be necessary to achieve a goal using a particular
device'  (<a href="#pre94">Preece et al., 1994</a>: 411).</blockquote>

<p>The overlapping research interest shared between HCI and IR is particularly
in relation to the design of IR interfaces and the determination of the
functionality and the level of cognitive load of already existing IR interfaces
(e.g., <a href="#hen94">Henninger, 1994</a>; <a href="#bra96">Brajnik, Mizzaro
&amp; Tasso, 1996</a>; <a href="#beu98"> Beaulieu &amp; Jones, 1998</a>).
However, since task-based interfaces, in terms of being user and domain
specific, have proven to be very effective (e.g., <a href="#ras94"> Rasmussen,
Goodstein &amp; Pejtersen, 1994</a>; <a href="#fis9501"> Fischer, 1995</a>) the
work task concept and approach is used within HCI, too.  An example is the work
by Vassileva (<a href="#vas95">1995</a>; <a href="#vas96">1996</a>) on work and
search task analysis of the users' job domain and their activities in this
domain in order to implement a task-based interface to a hyper-media
information system for hospitals.  Another example is provided by Henninger (<a
href="#hen94">1994</a>) who evaluates the interface of Codefinder by use of
so-called 'task statements'.  The task statements are categorised according to
three problem solving levels: specific, directed and ill-defined.  The levels
correspond to the three different types of information needs empirically
verified by Ingwersen (e.g., <a href="#ing92">1992</a>: 116-118): the
verificative information need; the conscious topical information need; and the
muddled topical type of an information need.  The specific and directed task
statements employed by Henninger (<a href="#hen94">1994</a>) are similar to the
'topics' used in TREC [<a href="#3">3</a>] where as the ill-defined task
statement shares characteristics with the introduced concept of a simulated
work task situation.  Henninger (<a href="#hen94">1994</a>) uses the task
statements as a problem-solving approach to comparatively monitor the test
persons use and interaction with the interfaces under investigation. For a
brief review of the task concepts applied within the research areas of
information seeking, IR and HCI the reader is directed to Hansen (<a
href="#han99">1999</a>) and Byström and Hansen (<a href="#bys02">2002</a>).</p>

<p>As mentioned, the introduced concept of a simulated work task situation
derives particularly from Ingwersen (e.g., <a href="#ing92">1992</a>; <a
href="#ing96">1996</a>) and Byström and Järvelin (<a href="#bys95">1995</a>).
Ingwersen stresses, in relation to IIR, the importance of taking into account
situational factors such as the work task the user is trying to perform, what
the user knows about the domain, the system, their own cognitive environment
and the conceptual aspects of searching in order to achieve successful and
optimal IR.  All nicely illustrated in his communication model (e.g., <a
href="#ing92"> Ingwersen, 1992</a>: 135; <a href="#ing96"> 1996</a>: 9).
Further, Ingwersen (<a href="#ing92">1992</a>: 207) describes how a work task
mirrors tasks and problems in the work domain that may affect the individual's
cognitive workspace and activities.  It is the complexity of this work task
that Byström and Järvelin (<a href="#bys95">1995</a>) analyse with reference to
information problem solving and information seeking processes.&nbsp;</p>

<p>As such, the work task is acknowledged as essential and central to IR,
therefore a potential useful tool for the evaluation of IIR systems.  The
concept of a simulated work task situation is an attempt to make the work task
operable by providing the test persons with a context description of the work
domain and the problem to solve which can be used in the evaluation of IIR
systems – as speculated on by Wilson (<a href="#wil73">1973</a>: 461) with
reference to the handling of situational relevance.  The simulated work task
situation then functions as the trigger of the test person's information need
and the platform for relevance judgement, and possible information need
refinement.  This is in line with the cognitive theories of the information
need formation and development (e.g., <a href="#tay68"> Taylor, 1968</a>; <a
href="#wer71">Wersig, 1971</a>; <a href="#bel80">Belkin, 1980</a>; <a
href="#bel82">Belkin, Oddy &amp; Brooks, 1982</a>; <a href="#ing92"> Ingwersen,
1992</a>; <a href="#ing96">1996</a>) and the multidimensional and dynamic
nature of relevance (e.g., <a href="#swa77">Swanson, 1977</a>; <a
href="#swa86">1986</a>; <a href="#sch90">Schamber, Eisenberg &amp; Nilan,
1990</a>; <a href="#har92">Harter, 1992</a>; <a href="#kuh93">Kuhlthau,
1993</a>; <a href="#par93"> Park, 1993</a>; <a href="#bru94"> Bruce, 1994</a>;
<a href="#rob97"> Robins, 1997</a>; <a href="#bat98">Bateman, 1998</a>; <a
href="#spi98"> Spink <i>et al.</i>, 1998</a>; <a href="#tan98"> Tang &amp;
Solomon, 1998</a>) who agree that the need formation is a <i> situation-driven
phenomenon</i> and that the assessment of multidimensional and dynamic
relevance of the information need is based on the underlying situation.</p>

<h3><a name="part2"></a>Part 2: Recommendations for the application of
simulated work task situations</h3>

<p>An evaluation of the applicability of simulated work task situations,
reported on in detail in Borlund and Ingwersen (<a href="#bor99">1999</a>) and
Borlund (<a href="#bor00a">2000a</a>; <a href="#bor00b">2000b</a>), positively
verified that the concept of simulated work task situations is recommendable
for purposes of (I)IR systems evaluation.  The main result of the evaluation
is: that real information needs are substitutable with simulated information
needs through the application of simulated work task situations – as such to be
considered the main recommendation.  The main, empirically-based,
recommendations for the employment of simulated work task situations are as
follows:</p>

<ul> <li>To employ both simulated work task situations and real information
needs within the same test;&nbsp;</li> <li>To tailor the simulated work task
situations towards the information environment and the group of test
persons;&nbsp;</li> <li>To employ either a combination of simulated work task
situations and indicative requests (simulated situations), or simulated work
task situations  only; and</li> <li>To permute the order of search jobs [<a
href="#4">4</a>].</li></ul>

<p> In the remainder of this section, the recommendations are explained in
detail.  First, the recommendation:

<ul><li>To employ both simulated work task situations and real information
needs within the same test.&nbsp;</li></ul>

<p>Rationale: The recommendation to employ both types of information needs
(that is, simulated and real information needs) is empirically supported by the
inference statistical analyses (t-tests and chi-square tests) of differences in
the test persons' treatment of the two types of information needs – revealing
no difference, meaning the two types of information needs are alike.  The
result of no difference between the types of information needs gives evidence
to the employment of simulated work task situations only, but at the same time
it also allows for the inclusion of real information needs.  Real information
needs may function as the baseline against the simulated information needs,
both at a specific test person level and at a more general level; and in
addition they may provide information about the systems' effect on this type of
information needs.  Further, we recommend:</p>

<ul> <li>To tailor the simulated work task situations towards the information
environment and towards the group of test persons.  The tailoring is to
include:&nbsp;

<ul> <li>A situation which the test persons can relate to and in which they can
identify themselves;&nbsp;</li> <li>A situation that the test persons find
topically interesting; and&nbsp;</li> <li>A situation that provides enough
imaginative context in order for the test persons to be able to relate and
apply the situation.</li> </ul> </li>

</ul>

<p>Rationale: Tailoring of simulated work task situations is important.
Empirical results showed that the simulated work task situation that worked the
best, that is, revealing a behavioural pattern of the test persons similar to
the pattern of their treatment of real information needs, fulfilled the
above mentioned characteristics.  The results also showed that a
less relatable
situation, from the test persons' point of view, may to some extent be
outweighed by a topically very interesting situation. Basically, tailoring of
simulated work task situations is important in order to gain a trustworthy
behaviour and IR interaction of the test persons.  Thus, knowledge is required
about the potential group of test persons in order to generate realistic and
functional simulated work task situations.  Further, evaluation of IIR systems
by use of simulated work task situations does, in the case of a highly domain
specific document collection, require that the test persons are topical domain
experts.  In the situation of evaluation by use of more general collections, as
in the case of our main experiment (news data) (Borlund, <a
href="#bor00a">2000a</a>; <a href="#bor00b">2000b</a>) no expert knowledge is
required.  However, the test persons ought always to share some common
characteristics that make them homogeneous as a group, so that simulated work
task situation can be tailored.&nbsp;</p>

<p>In addition, the empirical results revealed that the test persons' search
behaviour is not affected by whether the test persons were given a simulated
work task situation and an indicative request (simulated situation), or just a
simulated work task situation (e.g., see Figure 1).  This leads us to
recommend:

<ul> <li>To evaluate either by use of a combination of simulated work task
situations and indicative requests, or only simulated work task
situations.</li></ul>

<p>The test persons were asked in a post-search interview if it made any
difference to them, whether they had had both an indicative request and a
simulated work task situation or just a simulated work task situation 29%
replied 'yes' – it made a difference, and 71% said 'no' –&nbsp; it made no
difference (Borlund, <a href="#bor00a">2000a</a>; <a href="#bor00b">2000b</a>).

All the 'yes' answers were in favour of the indicative requests.
Interestingly, the test persons explained their 'yes' differently.  A
few of the test persons said that the indicative requests made it easier to generate the
search formulations as they picked the search terms from the indicative
requests.  One test person said it was helpful because the indicative request
helped him understand what was expected from him.  Others simply stated they
preferred having both.  Finally, one of the test persons said he did not use
the indicative request in relation to the query formulation, but had found it
useful when scanning for relevant information.  This indicates that the use of
the indicative requests can be constructively applied in combination with the
simulated work task situations.</p>

<br>In addition, a definition of the topic on search can be included in the
simulated situation, as it was done in the feasibility study (<a
href="#bor97">Borlund & Ingwersen, 1997</a>).  Empirical results reported on by
Spink, <i>et al.</i>, (<a href="#spi98">1998</a>: 118) support the application
of a definition of the topic on search, e.g., in a test situation where a
domain specific collection is applied by test persons with little knowledge of
the topic.&nbsp; Spink, <i>et al.</i>, (<a href="#spi98">1998</a>: 118) explain
that the more the test persons know about the information requiring problem,
the better they can identify the need and formulate the requests/queries which
results in focused retrieval.</p>

<p> Another recommendation concerns the issue of rotation of search jobs
between the test persons so that no test persons treat the jobs in identical
order.  We recommend:</p>

<ul><li>To permute the order of search jobs between the test persons.</li></ul>

<p>Rationale: Permutation of search jobs ought to be done for various reasons.
Firstly, in order to neutralise any effect on the results caused by increasing
familiarity with the experiment (system knowledge and topicality of data
collection), as traditionally done within the user-oriented approach (<a
href="#tag92">Tague-Sutcliffe, 1992</a>).  Secondly, in order to neutralise any
effect on the results due to the relevance assessment behaviour of the test
persons as to the order of search jobs.  The feasibility test (<a
href="#bor97">Borlund &amp; Ingwersen, 1997</a>) revealed a <i> significant
pattern of behaviour</i> among the test persons in the way they carried out the
relevance assessments of the retrieved documents.  Indicative results of the
main experiment (<a href="#bor99">Borlund &amp; Ingwersen, 1999</a>; <a
href="#bor00a">Borlund, 2000a</a>) confirms the existence of a pattern of
relevance assessment behaviour as to the order of search jobs.  Thus, the
simulated situations/simulated work task situations are to be presented to the
test persons, one at the time, in such an order that none of the test persons
get the same sequence of search jobs.</p>

<p>The final recommendation concerns the matter of pilot testing.  The
recommendation is not based on empirical evidence, but on practical experiences
obtained while planning for and executing the reported tests.  This experience
leads us to recommend:&nbsp;</p>

<ul> <li>To pilot test prior to actual testing.</li></ul>

<p>Rationale: From our perspective is pilot testing mandatory when testing by
use of test persons and simulated work task situations?  Pilot testing provides
for an opportunity to verify the essential and critical functionality of the
simulated work task situations, and if necessary to modify the simulated work
task situations towards the group of test persons with help from the pilot test
persons.  Further, we recommend to pilot test by use of both real and simulated
information needs, as real information needs may inspire to 'realistic'
simulated work task situations.  Consequently, pilot testing is not only a test
of the experimental setting and test procedure, but concerns also the design
and modification of simulated work task situations.</p>

<p>With this sub-section regarding the second part of the IIR evaluation model,
we close the aspects concerning data collection, and move on to the third part
of the model, namely: data analysis and alternative performance measures.</p>

<h3>Part 3: Alternative performance measures</h3>

<p>Basically, the third part of the model is a call for alternative performance
measures.  Two performance measures (<a href="#bor98">Borlund &amp; Ingwersen,
1998</a>; <a href="#bor00a"> Borlund, 2000a</a>) are introduced 1) the measure
of Relative Relevance (RR), and 2) the indicator of Ranked Half-Life (RHL).
The third part of the model is not limited to the RR and RHL measures, e.g.,
the novel performance measures by Järvelin and Kekäläinen (<a
href="#jär00">2000</a>) are included as fine examples of alternative
performance measures that meet the present need.  The call for alternative
performance measures is necessary because recall and precision, as the
traditional IR performance measures, are not ideal measures for the evaluation
of IIR systems.  The primary reason is that these measures are based on the
binary relationships between the number of relevant/not relevant, and
retrieved/not retrieved information objects.  Or as said by Spink and
colleagues:&nbsp;</p>

<blockquote> ...the current IR evaluation measures are…not designed to assist
end-users in evaluation of their information seeking behavior (and an
information problem) in relation to their use of an IR system.  Thus, these
measures have limitations for IR system users and researchers. (<a
href="#spi98">Spink, <i>et al.</i>, 1998</a>: 604).</blockquote>


<p>From a system-driven perspective there exists no problem with the
application of recall and precision. The problem arises when the measures are
applied to non-system-driven settings, e.g., within the user-oriented approach.
  In the latter type of settings users are involved and with them the various
subjective perceptions of what is relevant as well as how relevant.  The
measures of recall and precision do not distinguish between the different types
of relevance used for relevance assessment.  Just as they do not allow for a
non-binary indication of how relevant the relevant information objects are, but
allow only for a binary relevance representation.   The employed types of
relevance within the system-driven approach to IR evaluation are those of
algorithmic relevance and intellectual topicality (<a href="#bor98">Borlund
&amp; Ingwersen, 1998</a>). However, as even more types of relevancy may be
employed in settings involving users a need exists for performance measures,
which are  capable of handling and distinguishing between the different types
of relevance in order to provide information as to what the different  types of
relevance signify in terms of IR.  It is  often the case in tests where
non-binary relevance judgements are applied that two or more relevance
categories are merged into the  binary scale of relevant and non-relevant in
order to facilitate the calculation of the precision and recall measures (e.g.,
<a href="#su92">Su, 1992</a>).</p>

<p> According to Schamber (<a href="#sch94">1994</a>: 18) the relevance
categories get merged because it is assumed that no information is being lost
in the  merging process.  To us, the merger is a result of lack of qualified
performance measures that are capable of treating the users'  non-binary and
subjective relevance assessments.  A consequence of this is also seen in the
recent tendency to calculate precision as the mean of the relevance values,
that is, in the case where the users' relevance assessments are indicated as
numerical relevance values  (e.g., <a href="#bor98">Borlund &amp; Ingwersen,
1998</a>; <a href="#rei00">Reid, 2000</a>). Consequently, the measures of RR
and RHL are introduced, followed by a discussion of related positional oriented
performance measures for the comparison of best match retrieval.</p>

<h4>The performance measures of RR and RHL</h4>

<p>The RR measure (<a href="#bor98">Borlund &amp; Ingwersen, 1998</a>)
describes the degree of agreement between the types of relevance applied in
evaluating IR systems in a non-binary assessment context.  The RHL indicator
(<a href="#bor98">Borlund &amp; Ingwersen, 1998</a>), on the other hand,
denotes the degree to which relevant documents are located on the top of a
ranked retrieval result.  The RHL performance indicator adds to the
understanding of comparisons of IR best match performance by showing how well a
system is capable of satisfying a user's need for information for a given set
of queries at given precision levels.</p>

<h4>The RR measure</h4>

<p>Basically, the RR measure acknowledges the fact that different types of
relevance [<a href="#5">5</a>] are involved in evaluation of IR systems, and
especially in evaluation of IIR systems where more types of subjective
relevance may be applied, and the RR measure aims at understanding this fact.
The RR measure computes the degree of agreement between two results of
relevance assessments (e.g., see Table 1).  The two results of relevance
assessments (R<sub>1</sub>, R<sub>2</sub>) may represents the system's output
(algorithmic relevance) and the user's subjective assessments (by use of, e.g.,
intellectual topicality, pertinence or situational relevance) of the retrieved
output.  The RR measure proposes a pragmatic solution of how to bridge the gap
between subjective and objective relevance – the two main classes of relevance
applied to performance evaluation of IR systems, in particular IIR systems.
One consequence of the multi-dimensional relevance scenario is the extent to
which different types of objective and subjective relevance assessments are
associated across several users and retrieval engines.  Another consequence is
the fact that algorithmically ranked retrieval results become interpreted and
assessed by users during session time.  The judgements are then in accordance
with the users' dynamic and situational perceptions of a real or simulated work
task situation.  In addition, the assessments may incorporate non-binary
relevance values.  For the associative relations the suggestion is to compute a
measure of relative relevance (RR) between the relevance assessments of
different types of relevance by use of the cosine measure.</p>

<p>Hillman (<a href="#hil64">1964</a>) has suggested similar ideas of linking
and describing relevance-relations by use of association measures.  Also
Saracevic (<a href="#sar84">1984</a>) presents similar ideas in relation to
inter-search consistency.  The assumption behind the proposal is that an
associative inter-relationship exists between the various types of relevance
which may indeed be expressed by associative relations.  The RR measure is thus
supposed to yield quantitative information about the performance during IIR in
addition to the traditional recall and precision measures.  The RR measure
serves the purpose of quantifying a given relation between two types of
entities, in this case between the output of two types of relevance assessments
(R<sub>1</sub>, R<sub>2</sub>). R<sub>1</sub> and R<sub>2</sub> are constituted
by the assessment values as attributed by an assessor, a user, or an engine to
the retrieved information objects.  Informing us about how well a system is
capable of retrieving <i> predictable</i> topically relevant information
objects (algorithmic relevance) and partly how well the same objects actually
are subjectively relevant to the user in the sense of either intellectual
topicality, pertinence or situational relevance). Further, we might learn about
the nearness, as to the degrees of agreement, between the involved subjective
types of relevance.&nbsp;</p>

<p>Initially, the Jaccard association coefficient was preferred to the cosine
measure as the formula to use for the calculation of the RR measure (<a
href="#bor98">Borlund & Ingwersen, 1998</a>). However, we find that the Jaccard
measure is not capable of handling fractional counts and is
abandoned for that very reason.  For instance, in situations of identical match or total
correspondence between the two types of relevance judgements (R<sub>1</sub>,
R<sub>2</sub>), indicated by decimal values, the Jaccard measure does not
produce a value of 1.  Table 1 demonstrates a simple fictive situation of
identical match between relevance assessments of R<sub>1</sub> and
R<sub>2</sub>, and presents the results of the Jaccard and cosine measures for
the situation.  Adapted and applied to the measurement of relative relevance
(RR) the <i> Jaccard</i> and <i> cosine</i> formulas (e.g., <a
href="#ror99">Rorvig, 1999</a>: 640) can be expressed as follows:</p>

<table align="center" border="1" cellpadding=4 width="511" bgcolor="#FDFFBB" >
<tr> <th valign=middle width="234" height="32" align="center"> Jaccard:
association (R<sub>1</sub>,R<sub>2</sub>) =</th> <td valign=top width="249"
height="32"> <img border="0" src="p152fig2.gif"></td> </tr> <tr><td
valign=middle width="234" align="center"> <b>cosine: association
(R<sub>1</sub>,R<sub>2</sub>) =</b> </td> <td align="center" valign=top
width="249"> <img border="0" src="p152fig3.gif"></td> </tr> </table>

<p>The value of the RR measure when calculated according to the Jaccard formula
can be denoted as the intersection of the assessment values from the two types
of relevance (R<sub>1</sub>, R<sub>2</sub>) relative to the union of the total
number of assessments for a given retrieval situation.  The cosine formula
computes the cosine of the angle between the vector representations of the two
types of relevance assessment values.  The most significant difference between
the two formulas is the method by which the denominator of the formulas treats
the differences of relevance assessment values.</p>

<p>The situation illustrated in Table 1 shows a total correspondence between
the relevance assessment values of R<sub>1</sub> and R<sub>2</sub>.
Nevertheless, the Jaccard measure fails to illustrate the perfect level of
agreement as it produces an RR value of 0.619 opposite the cosine's RR value of
1.  Therefore the Jaccard based RR results cannot be interpreted for the
intended purpose.  As shown in Table 1 it is not a problem to calculate and
interpret the RR measure based on decimal values (non-binary relevance
assessments) by use of the cosine formula.  In the cases of total
correspondence between non-binary based relevance values the cosine attains the
maximum value of 1.  Thus, the cosine measure is preferred to the Jaccard
measure due to its robustness in a non-binary environment.</p>

<table align="center" border="0" cellpadding=4>

<tr><td valign=middle width="33%" align="center"><img border="0" src="p152fig5.gif"
width="343" height="306"></td> </tr> <caption align="bottom"><br><strong>Table
1. Fictive data illustrating the situation of complete agreement between the
non-binary relevance assessments  (R<sub>1</sub>,R<sub>2</sub>) and the
corresponding RR values of the Jaccard and cosine measures.</strong> </caption>
</table>

<p>Consequently, the cosine measure is proposed to be used to quantify and
express the degree of agreement between the two types of relevance involved
(R<sub>1</sub> and R<sub>2</sub>) constituted by the assessment values as
attributed by an assessor (i.e., intellectual topicality), a user (e.g.,
situational relevance), or an engine (i.e., algorithmic relevance) to the
retrieved objects.  The relation between <i> situational relevance</i> and <i>
algorithmic relevance</i> uncovers values which lead to: 1) an understanding of
how well the perceived work task situation, assessed through situational
relevance, is satisfied by the ranked output retrieved by the system; and 2)
the degree to which the relevant assessments (of highly or partial relevancy)
relate to the baseline measures.  The lower the value, the less correspondence
exists between the prediction of relevance by the system and the user's
interpretation of the information objects as useful to a given task. A similar
situation can be shown for the RR measure between <i> intellectual topicality
</i> and <i> algorithmic relevance</i>.  We are then informed about to what
extent the two types of topical related relevance assessments match each other.
  This tells us partly something about how well a system is capable of
retrieving <i> predictable </i> topically relevant information objects, and
partly how well the same objects actually are topically relevant in the
intellectual sense.  In cases of a high degree of equivalence in the match
between the algorithmic and intellectually assessed topicality and that of
algorithmic relevance and situational relevance, this fact is <i> no
guarantee</i> that the aboutness of the information objects also matches the
underlying purpose and work task against which the situational relevance is
assessed.  The relation between <i> intellectual topicality</i> and <i>
situational relevance</i>, tells us about the <i> nearness</i> between the two
subjective-oriented relevance types in regard to the retrieved information
objects.&nbsp;</p>

<p>The RR measure generates a more comprehensive understanding of the <i>
characteristics</i> of the performance of a single or several retrieval engines
and algorithms in between, in particular when confronted with users.  However,
when comparing several systems a scaling problem exists.  In relation to the
application of the RR measure an issue of comparability between different
engines involved in a given (I)IR experiment exists.  The problem of comparison
of the RR results exists due to the possible different score scales used for
the indication of the assigned degrees of <i> algorithmic relevance</i> in the
various engines involved in an (I)IR systems evaluation.  The problem exists
both in the cases of comparison of the results within the same experiment,
across systems, as well as across different tests.  Within the same experiment
the comparability of the RR results depends on the score scale used for the
relevance ranking of the algorithmic relevance.  However, one may indeed
compare across engines with respect to the nearness of all subjective kinds of
relevance – since they are independent of the algorithmic scales used.  For the
comparison across tests the scale or partition of the relevance categories used
for the assignment of the subjective type(s) of relevance assessments also
becomes an issue.  Put simply, the comparison of RR results requires that the
same scale be used.  Normalisation of scaling may solve this problem. </p>

<h4>The RHL indicator</h4>

<p>As a consequence of two or more types of relevance involved and the
non-binary context in IIR the issue of<i> comparisons</i> of computed retrieval
rankings become critical.  By taking into account the algorithmic <i> rank
position</i> and the various assigned relevance values of the retrieved
information objects one takes advantage of two parameters: 1) the
algorithmically ranked order which represents a list of decreasing degrees of
predicted objective relevance to the user's information need; and 2) the
applied subjective types and values of the relevance assessments representing
the assessor's or user's interpretations of the ranked information objects.
The RHL indicator makes direct use of both parameters (<a href="#bor98">Borlund
&amp; Ingwersen, 1998</a>).</p>

<p>The statistical method applied to calculate the Ranked Half-Life (RHL) value
corresponds to the computation of the median value of grouped continuous data.
The RHL value is the median “case”, i.e., the point which divides the
continuous
data area exactly into two parts.  In nuclear physics the 'half-life' of a
specific radioactive material is the time taken for half the atoms to
disintegrate (<a href="#egg90">Egghe &amp; Rousseau, 1990</a>: 267).  In
Bibliometrics 'cited half-life' is the time taken for half the citations to be
given to a particular document (<a href="#egg90">Egghe &amp; Rousseau,
1990</a>: 267).  For the RHL indicator the time dimension is substituted by the
continuous ranking of information objects produced algorithmically by a
retrieval engine.  Each listed information object represents a class of grouped
data in which the frequency corresponds to the relevance value(s) assigned the
information object.</p>

<p>The idea behind the application of the median point of grouped data is the
fact that if top-listed information objects obtain high relevance scores
assigned by the user or assessor, the median ranking for a given document
cut-off and a given precision value will rise.  With scattered or low placed
highly relevant information objects the median 'case' will drop downwards on
the original output list of objects.  In the present case (Table 2), precision
as performance indicator simply signifies the mean of the cumulated frequency,
also used for the median calculation; but traditionally it does not inform
about ranked positions.  Compared to the traditional recall and precision
measures the RHL indicator supplies additional information about the degree to
which the engine is capable of ranking its output according to user-perceived
relevance.  The interpretation of RHL indicator is: the lower the RHL value,
the higher on top of the rank, the better the retrieval engine for a given type
of relevance.</p>

<p>The formula used for calculating the RHL indicator is the common formula for
the median of grouped continuous data (e.g., <a href="#ste97"> Stephen &amp;
Hornby, 1997</a>: 53-54):</p>

<table align="center" border="0" cellpadding=4 width="510"> <tr> <th valign=top
width="494" height="32" colspan="2"> <img border="0"
src="p152fig4.gif" width="240" height="66"></th> </tr> <tr> <th
valign=top width="494" height="32" colspan="2"> <p align="left">where:</th>
</tr> <tr><td width="68" align="center" valign="top"> <p
align="left"><i>L<sub>m</sub>&nbsp;&nbsp;=</i></p> </td> <td align="center"
valign=top width="414"> <p align="left">lower real limit of the median class,
i.e., the lowest positioned information objects above the median class;</td>
</tr> <tr><td valign="top" width="68" align="center"> <p
align="left"><i>n&nbsp;&nbsp;=</i></p> </td> <td align="center" valign=top
width="414"> <p align="left">number of observations, i.e., the total frequency
of the assigned relevance values;</td> </tr> <tr><td valign="top" width="68"
align="center"> <p align="left"><i>f2&nbsp;&nbsp;=</i></p> </td> <td
align="center"  valign=top width="414"> <p align="left">cumulative frequency
(relevance values) up to and including the class preceding the median
class;</td> </tr> <tr><td valign="top" width="68" align="center"> <p
align="left"><i>F(med)&nbsp;&nbsp;=</i></p> </td> <td align="center" valign=top
width="414"> <p align="left">the frequency (relevance value) of the median
class; and</td> </tr> <tr><td valign="top" width="68" align="center"> <p
align="left"><i>CI&nbsp;&nbsp;=</i></p> </td> <td align="center" valign=top
width="414"> <p align="left">class interval (upper real limit minus lower real
limit), commonly in IR = 1.</td> </tr> </table>


<p>Table 2, presents RHL results computed for the purpose of illustration by
use of data from the feasibility test reported on in detail in Borlund (<a
href="#bor00a">2000a</a>).  In brief, the data used is based on the case of one
test person (no. 1) and a simulated situation (a).  The test person searched
the Dialog Target (<a href="#targ">1993</a>) facility (initiated by the
simulated situation) and assessed the retrieval output (algorithmic relevance)
according to usefulness (situational relevance).  A panel [<a href="#6">6</a>]
of two persons relevance assessed the same output in accordance to the
relevance type of intellectual topicality.  Further, the panel members
performed Boolean Quorum searches (<a href="#lan93">Lancaster &amp; Warner,
1993</a>) based on a direct transformation of the test person's query
formulation, and assessed the outcome by intellectual topicality and
situational relevance.  In the present analysis the cut-off was reasonably set
to fifteen documents.</p>

<table align="center" border="1" cellpadding=4 width="300" bgcolor="#FDFFBB" >
<tr><td valign=middle width="33%" align="center"> <img border="0"
src="p152fig6.gif" width="659" height="588"> </td> </tr> <caption
align="bottom"><br><strong>Table 2.	The distribution of the percentage
values of the three types of relevance assessments of version a1, including the
RHL values – for the Target and Quorum engines.</strong> </caption> </table>

<p>The Target engine achieves a precision value of 0.2 associated with
situational relevance (test person no. 1) and a RHL indicator value of 3.  This
means that, for this test person, the Target engine is capable of providing
half the cumulated relevance frequency of the 15 assessed documents within the
first three listed documents.  For the Quorum engine, however, the situational
RHL indicator value is 5.5 for the same simulated situation as assessed by both
panel members; the precision is of higher value (0.4) than for Target.
Identical precision values of 0.32 with reference to intellectual topicality
are attained for Target and Quorum by the panel.  However, the corresponding
RHL values are in favour of Target with a value of 2.63 as opposed to 4.52 for
Quorum.  Despite the identical precision values (0.32), which might leave us
with the impression of the two engines being equally good (or bad) at
retrieving topically relevant documents, it is shown that from a RHL and a
user's point of view that Target is better.  That is, the higher the engine can
place relevant information objects the better the system.  Thus, compared to
ordinary precision measures the RHL indicator supplies additional valid
information about the <i> degree</i> to which the engine is capable of ranking
its output according to user-related relevance.  As such this is a good example
of how RHL supplements, in the present case precision, but potentially both
precision and recall by providing additional performance information.  Further
it serves as a good example for the demonstration of how to calculate the RHL
indicator which again shows how the indicator functions.</p>


<table align="center" border="1" cellpadding=4 width="300" bgcolor="#FDFFBB"
style="border-collapse: collapse" bordercolor="#111111" cellspacing="0" >
<tr><td valign=middle width="33%" align="center"> <img border="0"
src="p152fig7.gif" width="291" height="432"> </td> </tr> </table>

<p>&nbsp;</p>

<table align="center" border="1" cellpadding=4 width="300" bgcolor="#FDFFBB"
height="74" >

<tr><td valign=middle width="33%" align="center" height="62"> <p
align="center"> <img border="0" src="p152fig8.gif" width="538"
height="106"> </td> </tr> <caption align="bottom"><br><strong>Table
3.	Extraction of Table 2 for the demonstration of calculation of the RHL
indicator.</strong> </caption> </table>

<p>For the purpose of the demonstration an extract of Table 2 of the relevant
data constitutes Table 3.  The figures used for the calculation of the actual
RHL values are stressed in bold, and are further shown in the formula applied.
For a given document cut-off one might prefer to obtain a RHL<i> index</i>
value which equals the computed RHL value normalised for the corresponding
precision value (precision = 1.0).  Table 2 presents the matching RHL index
values.  The index serves to emphasise the characteristics of the engines'
ranking capabilities for the same precision values across relevance types.
Thus, for a given document cut-off and a given value of precision the RHL
indicator can be examined across all test persons and all test tasks for each
of the involved types of relevance.  Realistically speaking, in IIR experiments
the document cut-offs might vary according to the engagement of each test
person – a situation which then has to be normalised.&nbsp;</p>


<p>Just as the RR and RHL measures supplement the measures of recall and
precision they supplement also each other.  One may say that the RR measure
bridges horizontally across applied types of relevance; whereas RHL indicates
the vertical position of the median value of the user's assigned relevance
values based on the ranked retrieval output.  The RR measure as well as the RHL
indicator can obviously be applied to non-interactive IR experiments like TREC,
which include algorithmic rankings and assessors' relevance values assigned to
these rankings.  In TREC-like experiments the RR measure can be used directly
to the two different types of assessments: algorithmic relevance and
intellectual topicality, for the <i> same</i> retrieval engine.  Across
retrieval engines, the comparability of the RR results depends on the score
scale used for relevance ranking of the algorithmic relevance.  This can
basically be explained with the different cognitive origin of the possible
different retrieval algorithms.  Normalisation of the scales involved may solve
this problem.  The RHL indicator is applicable across systems – but either
directly on the algorithmic level or limited to the subjective level alone.</p>

<h4>Related positional oriented single performance measures</h4>

<p>Both the RR measure and the RHL indicator represent novel approaches to the
measurement of IR performance.  But in contrast to the case of the RR measure
which has no preceding tradition or attempts to measure IR performance across
the applied types of relevance – binary or non-binary, other positional
oriented IR performance measures like RHL exist.  Of positional oriented single
performance measures the <i> expected search length</i> (ESL) measure by Cooper
(<a href="#coo68">1968</a>) is without doubt the most known.  This is further
indicated by the modification of the ESL by Dunlop (<a href="#dun97">1997</a>)
resulting in the so-called <i> Expected Search Duration</i> (ESD).  Of recent
approaches, the measure of <i> average search length </i>(ASL) by Losee (e.g.,
<a href="#los96">1996</a>; <a href="#los98">1998</a>) will also be
addressed.</p>


<p>The ASL measure developed by Losee (e.g., <a href="#los96">1996</a>; <a
href="#los98">1998</a>) is based on the binary approach to relevant ranked
information objects.  At a document level&nbsp;</p>

<blockquote> "...the ASL represents the expected number of documents obtained
in retrieving a relevant document, the mean position of a relevant document".
(<a href="#los96">Losee, 1996</a>: 96) </blockquote>

<p>  The ASL is somewhat similar to the ESL (<a href="#coo68">Copper, 1968</a>)
and ESD (<a href="#dun97">Dunlop, 1997</a>) by calculating the mean value of
the number of information objects.  But they differ in the sense that the ESL
and ESD are founded on the number of non-relevant information objects that the
user has to view in the process of retrieving a chosen relevant information
object, whereas the ASL counts the number of relevant information
objects.  Or
said differently, the ESL measure indicates the amount of wasted search effort
to be saved by using the retrieval system as opposed to searching the
collection purely at random until needed relevant information objects are found
(<a href="#coo68">Cooper, 1968</a>: 30); in contrast the ASL measure indicates
the expected position of a relevant information object in the ranked list of
information objects (<a href="#los96">Losee, 1998</a>: 89-90).  All the
measures, including the RR measure and the RHL indicator, are relatively easily
computed and comprehended analytically and also easily interpreted by the end
user.  In particular, the ESL, ASL and the RHL are related in terms of being
single performance measures which work at the statistically descriptive level
by treating the central tendency of retrieval success.  The approaches put
forward by Cooper (<a href="#coo68">1968</a>), Losee (e.g., <a
href="#los96">1996</a>; <a href="#los98">1998</a>), and Dunlop (<a
href="#dun97">1997</a>) all share the system-driven binary approach to the
calculation of IR performance.  The performance measures by Järvelin and
Kekäläinen (<a href="#jär00">2000</a>) may be seen as a response to the call
for alternative performance measures.  Järvelin and Kekäläinen (<a
href="#jär00">2000</a>: 41) introduces the three proposals as&nbsp;</p>

<blockquote> "…(1) a novel application of P-R-curves and average precision
computations based on separate recall bases for documents of different degrees
of relevance, and (2) two novel measures (CG and DCG) computing the cumulative
gain the users obtain by examining the retrieval result up to a given ranked
position”. (<a href="#jär00">Järvelin & Kekäläinen, 2000</a>: 41) </blockquote>

<p>They describe their motivation for the application of P-R-curves at
individual recall levels with reference to how the traditional IR system
performance evaluation is based on average precision over recall levels and
P-R-curves which does not take into account the multiple degree of relevance
assessments.  They continue&nbsp;</p>

<blockquote> "...even if the original assessments may have had multiple
degrees, these are generally collapsed into two for evaluation. In order to see
the difference in performance between retrieval methods, their performance
should be evaluated separately at each relevance level. For example, in case of
a four-point assessment (say, 0 to 3 points), separate recall bases are
needed for highly relevant documents (relevance level 3), fairly relevant
documents (relevance level 2), and marginally relevant documents (relevance
level 1). The rest of the database is considered irrelevant (relevance level
0)."   (<a href="#jär00">Järvelin & Kekäläinen, 2000</a>: 42) </blockquote>

<p> As such, they demonstrate that non-binary relevance is applicable within
the system-driven context of IR performance evaluation.</p>

<p>The two measures of cumulative gain, that is, 'cumulated gain' (CG) and
'cumulated gain with discount' (DCG) seek to estimate the cumulative relevance
gain the user receives by examining the retrieval result up to a given rank
(<a href="#jär00">Järvelin &amp; Kekäläinen, 2000</a>: 41).&nbsp;  As such,
both the measures are positional oriented performance measures and related to
the measures of ESL, ASL and RHL.  The CG and DCG are defined as follows (<a
href="#jär00">Järvelin &amp; Kekäläinen, 2000</a>: 43):</p>

<table align="center" border="0" cellpadding=4 width="300" height="74" >
<tr><td valign=middle width="33%" align="center" height="62"><img border="0"
src="p152fig9.gif" width="380" height="123"> </td></tr></table>

<p>The CG and DCG measures differ from each other in their way of comparing the
ranked output as a result of the processing of a query.  Järvelin and
Kekäläinen (2000: 42) explain how the CG measure builds upon the fact that
highly relevant documents are more valuable than marginally relevant documents.
  The DCG measure takes into account the relevance assessment behaviour of
users, that is, the lower the ranked position of a relevant document or
partially relevant document the less valuable it is for the user, because the
less likely it is that the user will examine the document.</p>


<p>Järvelin and Kekäläinen (<a href="#jär00">2000</a>: 43) compare the
characteristics and qualities of the CG and DCG measures to those of RHL and
ASL.  In regard to the ASL measure (<a href="#los96">Losee, 1996</a>; <a
href="#los98">1998</a>) Järvelin and Kekäläinen point out how ASL is based on
the concept of binary relevance, indirectly meaning that the measures of CG
(and DCG) as well as RHL are 'better' as they can handle non-binary relevance
assessments.  The second point concerns how the CG (and DCG) has a clear
advantage compared to the measures of ASL and RHL.  The case is that
at&nbsp;</p>

<blockquote> "…any number of retrieved documents examined (rank), it gives an
estimate of the cumulated gain as a single measure no matter what is the recall
base size."   (<a href="#jär00">Järvelin &amp; Kekäläinen, 2000</a>:
43)</blockquote>

<p>The ASL measure only gives the average position of a relevant document and
the RHL measures gives the median point of accumulated relevance for a given
query.  The third point concerns the measures' dependency or robustness with
reference to outliers, i.e., relevant documents found later in the ranked
order.  It is explained that the measure of CG&nbsp;</p>

<blockquote> "…is not heavily dependent on outliers…since it focuses on the
gain cumulated from the beginning of the result. The ASL and RHL are dependents
on outliers although RHL is less so."  (<a href="#jär00">Järvelin &amp;
Kekäläinen, 2000</a>: 43) </blockquote>
<p>The fourth point stated is about the easiness and obviousness by which the
CG measure can be interpreted, and further how it is more direct than
P-R-curves and does not mask bad performance.  No comparative comments are made
to the ASL, RHL or the RR measures in this respect.  However, all three
measures are intuitively easy to interpret and neither of them serves the
purpose of masking bad IR performance.  One comment is made about the RHL,
which is that it alone is not sufficient as an IR performance measure.  Let us
emphasise that this has never been the intention.  The measures of RHL and RR
are thought to supplement the measures of recall and precision and to emphasise
the call for alternative performance measures, and might as well supplement the
method and measures proposed by Järvelin and Kekäläinen (<a
href="#jär00">2000</a>).  In addition, the DCG measure is correctly said to
have the following advantages not provided by the ASL or RHL measures:  1) it
realistically weights down the gain received through the documents found later
in the ranked list of results; and 2) it allows modelling the user persistence
in examining long ranked result lists by adjusting the discount factor (<a
href="#jär00">Järvelin &amp; Kekäläinen, 2000</a>: 43).</p>

<p>Like the RHL indicator the proposed measures of CG and DCG require that the
same category scale of subjective relevance is used when comparing across
systems.  The measures by Järvelin and Kekäläinen (<a href="#jär00">2000</a>)
are constructive attempts of how to handle non-binary relevance assessments
experienced in IR tests that involve users or test persons, e.g., tests of IIR
systems.  Thus, we welcome and include their proposals, that is, the novel
application of P-R-curves reflecting the various degrees of relevance and the
two position measures of cumulative gain of perceived relevance.  All together
their proposals and ours form an arsenal of alternative performance measures
that fit the third part of the IIR evaluation model.</p>

<h2>Provisional use of the IIR evaluation model</h2>

<p>Though the proposed model to evaluation of IIR systems and user search
behaviour is relatively new, parts of the model have already been employed and
reported on in the research literature (e.g., <a href="#jos98"> Jose, Furner
&amp; Harper, 1998</a>; <a href="#whi01"> White, <i>et al.</i>, 2001</a>; <a
href="#rod01">Rodden, <i>et al.</i>, 2001</a>; <a href="#fer01"> Fernström
&amp; Brazil, 2001</a>; <a href="#rut02">Ruthven, 2002</a>; <a
href="#uhr02">Uhrskov, 2002</a>; <a href="#nie02"> Nielsen, 2002</a>; <a
href="#blo02">Blomgren, <i>et al.</i>, 2002</a>).  In this section we point to
these IR studies and experiments, and hereby indirectly verify the need for an
alternative framework for evaluation of IIR systems.  It is the explicit use of
simulated work task situations that is the most employed part of the model.
However, it could be argued that the use of simulated work task situations to
the evaluation of (I)IR systems is not new.  For instance, as long ago as 1967
Cuadra and Katter (<a href="#cua67">1967</a>) reported on an IR relevance test
of the variable of 'implicit use orientations' where intermediaries were given
'cover stories' of information requirements.  The intermediaries used 'cover
stories' for the purpose of imagining a given situation in order to help
finding relevant information for the possible user in that situation.
Similarly, Brajnik <i>et al.</i>, (<a href="#bra96">1996</a>) provided their
test persons with descriptions of so-called 'information problems' when
evaluating an IR user interface.  Based on the information problem descriptions
the test persons had 30 minutes to retrieve suitable documents that satisfied
the described story situation.  However, no methodological argumentation exists
in the cases of Cuadra and Katter (<a href="#cua67">1967</a>) or Brajnik, <i>et
al.</i>, (<a href="#bra96">1996</a>) of why the approaches were chosen as well
as theoretical and empirical evidence to support the use of 'cover
stories'.</p>

<p>Further, the use of simulated work task situations can be seen as related to
vignettes, which are used at various stages in the data collection and analysis
of information seeking behaviour.  The technique is scenario based and aims at
either eliciting details about information behaviour from users or displaying
data meaningfully (<a href="#urq01">Urquhart, 2001</a>).  Vignettes are short
stories presented to the test persons, who are then asked to describe their
possible reactions in response to the presented situation (<a
href="#urq99">Urquhart, 1999</a>; <a href="#urq01">2001</a>).  The vignette
technique shares characteristics with the psychology-founded method of
empathy-based stories (e.g., <a href="#esk88">Eskola, 1988</a>) also referred
to as 'non-active role-playing' (e.g., <a href="#gin79"> Ginsburg, 1979</a>).
This method involves the writing of short essays, according to instructions
given by the researcher.  The instructions are given to the test person in
terms of a 'script'.  So where we propose to use simulated work task situations
as the trigger of simulated information needs and as the platform for the
assessment of situational relevance in the context of IR, is the script used as
the guide of essay writing.  The script contains instructions and directions
for the test person to follow and use when writing the essay.  The job of the
test person is either to continue the writing of a story introduced by the
researcher, or to describe what has taken place prior to a given point of time
in the story as outlined by the researcher.  At least two different versions of
the same script are employed because variation in scripts is central to the
method.  Within information science this approach has been employed by, e.g.,
Halttunen and Sormunen (<a href="#hal00">2000</a>) in the test of the computer
supported learning environment called the 'IR Game'.</p>

<p>Jose <i>et al.,</i> (<a href="#jos98">1998</a>) are the first to evaluate by
use of simulated work task situations when carrying out performance evaluation
of a spatial querying-based image retrieval system.&nbsp; Also Reid (<a
href="#rei99">1999</a>; <a href="#rei00">2000</a>) adopts the idea of a
simulated work task situation, though simply naming it 'task'.  Reid (<a
href="#rei99">1999</a>; <a href="#rei00">2000</a>) is at an analytical level
concerned with how to evaluate IR systems according to search task processes
and subsequent information use based on simulated work task situations, which
makes her propose to use a task-oriented test collection.  White,
<i>et al.,</i>
(<a href="#whi01">2001</a>) use simulated work task situations to evaluate the
WebDocSum IR system, which is a query-biased web page summariser.  Comparative
performance evaluation of three different versions of the WebDocSum systems is
carried out also by use of simulated work task situations (<a
href="#whi02">White <i>et al.</i>, 2002</a>).  Their evaluation uses three
different types of simulated work task situations initiating three different
types of searches according to the information needs triggered, that is, fact
search, decision search, and background search.  The applied types of simulated
work task situations correspond, to some degree, to the task categories of
automatic information processing task, normal information processing task,
normal decision task, and known genuine decision task presented by Byström and
Järvelin (<a href="#bys95">1995</a>).  At the same time the different tasks
produce different types of simulated information needs within the range of the
verificative and the conscious topical information needs (<a
href="#ing92">Ingwersen, 1992</a>).  Further, by employing different types of
simulated work task situations White and colleagues (<a href="#whi02">2002</a>)
expand the concept of simulated situations, that is, the use of simulated
situations in engendering realistic searching behaviour for this range of tasks
has not yet been tested.  Ruthven (<a href="#rut01">2001</a>; <a
href="#rut02">2002</a>) employs the concept of simulated work task situations
to his doctoral work when evaluating relevance feedback taking into account
users' relevance assessment behaviour.  Rodden <i>et al.,</i> (<a
href="#rod01">2001</a>) test whether the presentation of image retrieval output
arranged according to image similarity is recommendable, and hereby employ
simulated work task situations.  Whereas, Fernström and Brazil (<a
href="#fer01">2001</a>) investigates information searching behaviour of test
persons browsing sound files, initiating the browsing by use of simulated work
task situations.  Also Uhrskov (<a href="#uhr02">2002</a>) applies simulated
work task situations in her comparative study of IR searching behaviour of
university students from two different science disciplines.  Most recently,
Nielsen (<a href="#los98">2002</a>) has employed simulated work task
situations, in the context and domain of the pharmaceutical industry, in regard
to the verification of effectiveness of a thesaurus constructed by use of the
word association method.  As for the application of alternative performance
measures, the DCG measure (<a href="#jär00">Järvelin &amp; Kekäläinen,
2000</a>) has recently been validated by Voorhees (<a href="#voo01">2001</a>)
presumably to test its fitness to TREC.  Further, the INEX initiative (<a
href="#fuh02">Fuhr et al., 2002</a>; <a
href="http://qmir.dcs.qmw.ac.uk/INEX/index.html">http://qmir.dcs.qmw.ac.uk/INEX
/index.html</a>)&nbsp; for the evaluation of XML retrieval, which relevance
assess according to four relevance categories may consequently apply
alternative non-binary based performance measures in the future computation of
retrieval performance.  So far the work (in progress) by Blomgren
<i>et al.,</i>
(<a href="#blo02">2002</a>) is the first incident of the application of the
entire IIR evaluation model, that is, all three parts of the model – the
components, the recommendations, and one of the alternative performance
measures (RHL).</p>

<h2>Summary statements</h2>

<p>The IIR evaluation model is primarily a response to the domination of the
Cranfield model and its application to the evaluation of IIR systems within the
system-driven approach to IR evaluation.  With IIR systems being defined as
systems developed for and founded on <i> interaction,</i> this type of systems
cannot be optimally evaluated in a static mode like that of the Cranfield
model.  The Cranfield model is a laboratory-based approach to systems
evaluation that requires no potential users to work with the systems under
testing.  This again has the consequence that the Cranfield model does not deal
with dynamic information needs but treats information needs as a static concept
entirely reflected by the query (search statement).  Furthermore, this model
uses only binary topical relevance ignoring the fact that relevance is a
multidimensional and potentially dynamic concept.  Hence, the batch-driven mode
of the Cranfield model is not suitable for the evaluation of IIR systems which,
if carried out as realistically as possible, requires human interaction,
potentially dynamic information need interpretations, and the assignment of
multidimensional <i> and</i> dynamic relevance.  These requirements are
signified by the proposal of a <i> set of components</i> that constitutes the
first part of the model:</p>

<ul> <li>the involvement of potential users as test persons;</li> <li>the
application of individual and potentially dynamic information need
interpretations deriving from, e.g., the sub-component of a simulated work task

situation; and</li> <li>the assignment of multidimensional <i> and</i> dynamic
relevance judgements.</li></ul>

<p>The set of components combined with the second part of the model,
recommendations for the application of simulated work task situations, provides
an experimental setting that enables to facilitate evaluation of IIR systems as
realistically as possible with reference to actual information seeking and
retrieval processes, though still in a relatively controlled evaluation
environment.  The third and final part of the model is a call for alternative
performance measures that basically are capable of managing non-binary based
relevance assessments, subsequent to the application of the model part no. one
and two.</p>

<p>The actual procedure of the application of the model includes the
recruitment of test persons to participate in the pilot and main experiment.
To inform the test persons about when and where to show up as well as to ask
them to prepare a real information need in advance of the experiment (an
information need that is capable of being met by the particular collection of
information objects in use, that is, if one decides to include also real
information needs).</p>

<p>An experimental setting, in this context, necessarily includes a database of
information objects as well as the system(s) under investigation.  However,
these components are not explicitly dealt with here.  It is assumed that the
system to be tested, other technical facilities, and the facilities of where to
carry out the experiment are already arranged for.  Knowing about the
collection of information objects' topical domain (e.g., management, medicine,
or art) or type of information (e.g., news, scientific, or fiction) and the
characteristics in common of the group of test persons the simulated work task
situations can be generated.  One way to gather information and verify
characteristics of the users within the particular domain is through interviews
prior to generation of simulated situations, but also through pilot testing by
asking the test persons to bring with them real information needs. Prior to the
pilot and main experiment decisions about methods of data collection (e.g.,
transaction log, questionnaire, interview, observation by human or by video
recording) have to be made.  Decisions about the number of relevance categories
to be employed have to be made as well as how to collect and capture the
relevance information (e.g., electronically by log; or manually on a sheet of
paper).  These decisions concern the design of the experiment.</p>

<p>When the experiment has been designed, according to the purpose of the
experiment, the experimental procedure can be verified through pilot testing,
which ought to identify the requisites necessary for the experiment.  The
requisites may, for instance, include a dictionary, pen and paper, but also a
questionnaire for the collection of demographic as well as supplementing
information about the participating test persons; and a structured post-search
interview to follow-up on the test persons' experiences and perceptions of
their participation in the experiment.</p>

<p>When the test persons show up for the experiment it is essential they get
the same treatment with reference to the introduction to the experiment, the
demonstration of IR system(s), and the conditions under which the experiment
takes place.  The experiment may start with collecting demographic information
about the test persons, this may also help the test persons to ease up and
become comfortable with the entire situation.  A (brief) demonstration may be
given of the IR system(s) and the search by the use of simulated
situations/simulated work task situations can start.</p>

<p>The simulated situations/simulated work task situations are presented one at
the time to the test persons in such an order that none of the test persons get
the same sequence of search jobs.  This is done to neutralise any effect on the
results caused by increasing familiarity with the experiment (e.g., <a
href="#tag92">Tague-Sutcliffe, 1992</a>; <a href="#bor97"> Borlund &amp;
Ingwersen, 1997</a>; <a href="#bor99">1999</a>).  Further, it is required that
the test persons see the same simulated situation/simulated work task situation
only once, and that all test persons search all simulated situations/simulated
work task situations.  Just as all test persons must search all IR systems
under testing (if multiple), and all simulated situations/simulated work task
situations must be searched against all IR systems.  If not, the results cannot
be compared across the IIR systems and the group of test persons.  No matter
the experimental conditions and set up, that is, whether the facilities are so
that several test persons can carry out searches at the same time or only one
test person at the time, the test persons are not supposed to discuss the
simulated work task situations with each other.  The reason is to avoid the
possible influence on the interpretation of the simulated work task situation.
The simulated work task situations are to be perceived as unbiased as possible
by the test persons.  This condition entails the sharing of the overall
universe in a cognitive sense by scenarios, test collections and test persons.
Based on the test persons' individual perceptions of the problems described in
the simulated work task situations and in accordance with the individual test
persons' knowledge states they formulate information statements, which are put
to the systems(s) as requests.  The requests function as indications of the
perceived information needs of the test persons.</p>

<p>According to the purpose of the experiment the test persons assess relevance
of the retrieval output in conformity with the purpose of the experiment.  That
is, if the purpose is to compare IR systems performance or to let the test
persons focus on specific actions then an 'information object cut-off' can be
incorporated into the design of the experiment.  If the purpose, in contrary,
is to gain knowledge about the test persons' searching and IR behaviour in the
process of satisfying their (simulated) information needs by use of the
particular systems under investigations then no 'information object cut-off'
ought be incorporated, but instead it should be decided whether or not time
constraints should be built into the experimental design.  The possibility of
obtaining additional information about the test persons' perception of the
system(s), their perceived satisfaction of their information needs, and their
perceived realism of the simulated work task situations is available after the
completion of each of the search jobs prior to the next search job.  This can
be done as a structured post-search interview or as a system built in
questionnaire, which in both cases the test persons have to complete before
continuing the experiment.  Just as an overall post-search interview can be
employed to unite and close the experiment for each participating test
person.</p>

<p>The experimental procedure of testing can be divided into two steps.  The
first step concerns the collection of the experimental data, the second the
analysis of the collected data – including the calculation of IR system
performance.  Due to the experimental components, the human participation, the
dynamic information need interpretations, and the multidimensional <i> and</i>
dynamic relevance assessments the relevance outcome, which provide the
foundation for the calculation of the IR system performance, is different as
opposed to the relevance outcome of traditional system-driven IR experiments.
Traditionally, recall and precision ratios are calculated as the indicators of
the IR performance.  These ratios are founded on the binary relationships
between the number of objectively relevant/not relevant, and retrieved/not
retrieved information objects.  The relevance outcome obtained by use of the
proposed experimental components can make use of the measures of relative
recall and precision.  However, relative recall and precision can be calculated
only if the relevance outcome of two or more relevance categories gets merged
with the consequence of loosing the information about how relevant the
retrieved and relevant information objects really are.  Furthermore, the
relevance outcome of the present proposed setting represents different types of
relevance, as a minimum one objective and one subjective type of relevance –
just as in traditional system-driven IR experiments.  These different types of
relevance are traditionally not distinguished between but simply treated as the
one and same type.  This is a problem because the different types of relevance
represent <i> different degrees</i> of intellectual interpretations containing
information about the IR system's capability of satisfying different types of
information problems at different stages of the information seeking process (<a
href="#kuh93">Kuhlthau, 1993</a>).  The conclusion is that IR performance based
on the relevance outcome obtained as a result of the proposed experimental
setting, which reflect the dynamic information searching and IR processes
necessary for IIR system evaluation, either cannot be calculated or are
difficult to calculate by use of the traditional static and two-dimensional
performance measures of recall and precision.  Thus, we bring attention to the
need of alternative and complementary performance measures to complete the IIR
evaluation model.  The measures of relative relevance (RR), ranked half-life
(RHL) (<a href="#bor98">Borlund &amp; Ingwersen, 1998</a>; <a href="#bor00a">
Borlund, 2000a</a>), cumulated gain (CG) and cumulated gain with discount (DCG)
(<a href="#jär00">Järvelin &amp; Kekäläinen, 2000</a>) are examples of such
alternative measures which can handle and compare different types of relevance
and manage non-binary relevance assessments.</p>

<p>In the presentation of the IIR evaluation model two issues have been
emphasised: 1) the essentiality of the sub-component of the simulated work task
situation to the experimental setting because of its function to ensure the
experiment both realism and control; and 2) how the employment of the proposed
components changes the experimental relevance outcome making recall and
precision insufficient for the measurement of IIR performance.</p>

<p>We see the proposed evaluation model as a first instance of a cognitive
approach to the evaluation of IIR systems.  The model is anchored in the
holistic nature of the cognitive viewpoint by being a hybrid of the two main
approaches to IR systems evaluation – the system-driven and the cognitive
user-oriented approaches – building upon each their central characteristics of
control <i> and</i> realism.</p>

<h2>Notes</h2>

<ol> <li><a name=1>With respect to the traditionally employed performance
measures of recall and precision.</a></li> <li><a name=2>A real information
need is defined by the user and it is characterised by being of personal
interest and importance to the user.</a></li> <li><a name=3>TREC is the acronym
for Text REtrieval Conferences.  For an overview of TREC the reader is directed
to the TREC homepage at:  http://trec.nist.gov/pubs.html, where overview papers
can be found.</a></li> <li><a name=4>'Search job' is used as a common
expression of the simulated work task situations and the test persons' real
information needs.</a></li> <li><a name=5>For definitions of different types of
relevance see, e.g., Saracevic (<a href="#sar96">1996</a>);  Borlund &amp;
Ingwersen (<a href="#bor98">1998</a>); Cosijn &amp; Ingwersen  (<a
href="#cos00">2000</a>). </li> <li><a name=6>A panel is a group of domain
experts or members of the research team, but as opposed to a control group it
is the panel that has generated the simulated situations/simulated work task
situations in advance.</a></li> </ol>

<h2>Acknowledgement</h2><p>The author thanks lecturer Dr. Ian Ruthven and
doctoral student Jesper W. Schneider for their careful reading and constructive
comments to an earlier draft of the paper.  The author is also grateful to
Professor Peter Ingwersen for many fruitful discussions.&nbsp;</p>


<p>The present work is carried out as part of the TAPIR research project headed
by Professor Peter Ingwersen at the Royal School of Library and Information
Science, Department of Information Studies.  The work is financially supported
by a grant from the Research Fund of the Danish Ministry of Culture (ref. no.:
A 2001 06-021(a)).</p>

      
<h2>References</h2>
<ul>
  <li><a name=all98></a>Allen, B.L. (1996) <i> Information tasks: towards a user-centered approach to information
    systems</i>. San Diego, CA: Academic Press.
	<!--
  <li><a name=bat98></a>Bateman, J. (1998) Changes in Relevance Criteria: a Longitudinal Study. In: Preston, C.M., ed.  <i>Proceedings of the ASIS annual meeting</i> (35). 1998, White Plains, N.Y.,  pp. 23-32.-->
  <li><a name=beu98></a>Beaulieu, M. & Jones, S. (1998) Interactive searching and interface issues in the Okapi best match probabilistic retrieval system.  <i> Interacting with Computers</i>, <b>10 </b>(3), 237-248.
  <li><a name=beu96></a>Beaulieu, M., Robertson, S. & Rasmussen, E. (1996) Evaluating interactive systems in TREC.     <i> Journal of the American Society for Information Science</i>, <b> 47</b>(1), 85-94.
  <li><a name=bel80></a>Belkin, N.J. (1980) Anomalous states of knowledge as a basis for information retrieval.
    <i>Canadian Journal of Information Science</i>, <b>5</b>, 133-143.
  <li><a name=bel82></a>Belkin, N.J., Oddy, R. & Brooks, H. (1982) ASK for information retrieval: part I. Background and theory.     <i> Journal of Documentation</i>, <b> 38</b>(2), 61-71.
  <li><a name=blo02></a>Blomgren, L., Vallo, H. & Byström, K. (2002) Evaluation of an information system in an information seeking process: the preliminary results.  (Unpublished: work in progress).   
    <a href="http://user.tninet.se/~zpd318f/paper.htm"> http://user.tninet.se/~zpd318f/paper.htm</a>
  <li><a name=bor00a></a>Borlund: (2000a) <i> Evaluation of interactive information retrieval systems</i> Doctoral dissertation: &Aring;bo Akademi University. (&Aring;bo (Turku): &Aring;bo Akademi University Press) 
  <li><a name=bor00b></a>Borlund, P. (2000b) Experimental components for the evaluation of interactive information retrieval systems.  <i> Journal of Documentation</i>, <b> 56</b>(1), 71-90.
  <li><a name=bor97></a>Borlund, P. & Ingwersen, P. (1997) The development of a method for the evaluation of interactive information retrieval systems.  <i>Journal of Documentation</i>, <b> 53</b>(3), 225-250.
  <li><a name=bor98></a>Borlund, P. &amp; Ingwersen, P. (1998) Measures of relative relevance and ranked half-life: performance indicators for interactive IR. In: Croft, B.W, Moffat, A., van Rijsbergen, C.J., Wilkinson, R., and Zobel, J., eds.
  <!--
  <i> Proceedings of the 21st ACM Sigir Conference on Research and Development of Information Retrieval.</i>pp. 324-331. Melbourne, 1998.  Melbourne: ACM Press, York Press,
  
  <li><a name=bor99></a>Borlund, P. & Ingwersen, P. (1999) The application of work tasks in connection with the evaluation of interactive information retrieval systems: empirical results. In:   Draper, S.W., Dunlop, M.D., Ruthven, I. &amp;  van Rijsbergen, C.J., eds.<i> Mira '99: Evaluating interactive information retrieval,  Glasgow, UK, 14-16 April 1999</i>. British Computer Society, pp. 29-46. -->
  
  <li><a name=bra96></a>Brajnik, G., Mizzaro, S., & Tasso, C. (1996) Evaluating user interfaces to information retrieval systems:  a case study on user support. In: Frei, H.P., Harman, D., Schäuble, P. & Wilkinson, R., eds.<i> Proceedings of the 19th ACM Sigir Conference on Research and Development of Information Retrieval, Zurich, 1996.</i> pp. 128-136. Konstanz: Hartung-Gorre.
  
    <li><a name=bro80></a>Brookes, B.C. (1980) The foundation of information science: part I: philosophical aspects.
    <i>Journal of Information Science</i>, <b>2</b>, 125-133.
	
  <li><a name=bru94></a>Bruce, H.W. (1994) A cognitive view of the situational dynamism of user-centered relevance estimation. <i> Journal of the American Society for Information Science</i>, <b>45</b>(3), 142-148.
  
  <li><a name=bys02></a>Byström, K. & Hansen, P. (2002) Work task as units for analysis in 
  information seeking and retrieval studies. In: Bruce, H., Fidel, R., Ingwersen, P. & Vakkari, P., eds.
    <i> Emerging Frameworks and Methods, Seattle, 2002.</i>. pp. 239-251.  Colarado: Libraries Unlimited. 
	
  <li><a name=bys95></a>Byström, K. &amp; Järvelin, K. (1995) Task complexity affects information seeking and use.
    <i> Information Processing &amp; Management</i>, <b>31</b>(2), 191-213.
<!--	
  <li><a name=cle66a></a>Cleverdon, C.W. & Keen, E.M. (1966) <i> Aslib Cranfield research project: factors determining the performance of indexing systems. Vol. 2: results.</i> Cranfield.
  
  <li><a name=cle66b></a>Cleverdon, C.W., Mills, J. & Keen, E.M. (1966) <i> Aslib Cranfield research project: Factors determining the performance of indexing systems. Vol. 1: design.</i> Cranfield.-->
  
  <li><a name=coo68></a>Cooper, W.S. (1968) Expected search length: a single measure of retrieval effectiveness based on the weak ordering action of retrieval systems.   <i> American Documentation,</i> <b>19</b>(1), 30-41.
  
  <li><a name=cos00></a>Cosijn, E. & Ingwersen, P. (2000) Dimensions of relevance. <i> Information Processing &amp;  Management</i>, <b>36</b>(4) 533-550.
	
  <li><a name=cua67></a>Cuadra, C.A. &amp; Katter, R.V. (1967) Opening the black box of 'relevance'.    <i> Journal of Documentation, </i> <b>23</b>(4), 291-303.
  
  <li><a name=dia89a></a>Diaper, D. (1989a) (Editor) <i> Task analysis for human-computer interaction.</i> Chichester: Ellis Horwood. 
  
  <li><a name=dia89b></a>Diaper, D. (1989b) Task analysis for knowledge descriptions (TAKD): the method and an example. In:   Diaper, D., ed.<i> Task analysis for human-computer interaction</i>. pp. 108-159.  Chichester: Ellis Horwood, 
  
  <li><a name=dia89c></a>Diaper, D. (1989c) Task observation for human computer interaction. In:
  Diaper, D., ed.<i> Task analysis for human-computer interaction </i>.  pp. 210-237.  Chichester: Ellis Horwood,
  
  <li><a name=dun97></a>Dunlop. M. (1997) Time, relevance and interaction modelling for information retrieval. In:
  Belkin, N.J., Rarasimhalu, A.D., &amp;  Willett, P., eds.<i> Proceedings of the 20th ACM SIGir Conference on Research and Development of Information Retrieval</i>. Philadelphia, 1997. pp. 206-213.   New York, N.Y.: ACM Press, 
  
    <li><a name=egg90></a>Egghe, L. &amp; Rousseau, R. (1990) <i> Introduction to informetrics: quantitative methods in library and information science. /i> Amsterdam: Elsevier Science Publishers.
	
  <li><a name=ell96a></a>Ellis, D. (1996a) <i> Progress and problems in information retrieval.</i> London: Library Association Publishing.
  
  <li><a name=ell96b></a>Ellis, D. (1996b) The dilemma of measurement in information retrieval research.
    <i> Journal of Documentation</i>, <b>45</b>(3), 23-36.
	
  <li><a name=esk88></a>Eskola, A. (1988) <i>Blind alleys in social psychology</i>. Amsterdam: Elsevier Science Publishers.
  <!--
  <li><a name=fer01></a>Fernström, M. & Brazil, E. (2001) "Sonic browsing: an auditory tool for multimedia asset management. In:  Hiipakka, J., Zacharov, N. & Takala, T., eds.<i> Proceedings of ICAD'01, Espoo, Finland, 2001,</i> pp. 132-135.-->
  
  <li><a name=fis9501></a>Fischer, G. (1994) New perspectives on working, learning, and collaborating and computational artefacts in their support. In: Böcker, H.D., ed.<i> Proceedings Software-Ergonomie '95.</i> pp. 21-41.   Stuttgart: B.G. Teubner Verlag,   
    
  <li><a name=fuh02></a>Fuhr, N., Gövert, N., Kazai, G. &amp; Lalmas, M. (2002) INEX: initiative for the evaluation of XML retrieval. <a href="http://ls6-www.informatik.uni-dortmund.de/bib/fulltext/ir/Fuhr_etal:02a.pdf"> http://ls6-www.informatik.uni-dortmund.de/bib/fulltext/ir/Fuhr_etal:02a.pdf</a>
    <!--
  <li><a name=gin79></a>Ginsburg, G.P. (1979) The effective use of role-playing in social psychological research. In:
  Ginsburg, G.P., ed.<i> Emerging strategies in social psychological research</i>. New York, NY: John Wiley &; Sons, Ltd.-->
    
  <li><a name=hal00></a>Halttunen, K. & Sormunen, E. (2000) Learning information retrieval through an educational game: is gaming sufficient for learning?   <i> Education for Information</i>,<b> 18</b> (4), 289-311.
  ------------------------------------------------
  <li><a name=han99></a>Hansen, P. (1999) User interface design for IR interaction: a task-oriented approach. In:
  Aparac, T., Saracevic, T., Ingwersen, P. & Vakkari, P., eds.<i> Proceedings of CoLIS 3, Third International Conference 
  on the Conceptions of Library and Information Science: Digital Libraries: Interdisciplinary concepts, 
  challenges and opportunities. Dubrovnik, 1999</i>. Zagreb: Zavod za informacijske studije Odsjeka 
  za informacijske znanosti: Filozofski fakultet; Lovke: Naklada Benja, pp. 191-205. 
  
  <li><a name=har92></a>Harter, S.P. (1992) Psychological relevance and information science.  <i>Journal of the American Society for Information Science</i>, <b>43</b>(9), 602-615. 
	
  <li><a name=har96></a>Harter, S.P. (1996) Variations in relevance assessments and the measurement of retrieval effectiveness.
    <i> Journal of the American Society for Information Science</i>, <b> 47</b> (1), 37-49.
  <li><a name=hen94></a>Henninger, S. (1994) Using iterative refinement to find reusable software.
    <i> IEEE Software</i>, <b> 11</b> (5), 48-59.
  <li><a name=her96></a>Hersh, W., Pentecost, J. & Hickam, D. (1996) A task-oriented approach to information retrieval evaluation.
    <i>Journal of the American Society for Information Science</i>, <b> 47 </b> (1), 50-56.
  <li><a name=hil64></a>Hillman, D.J. (1964) The notion of relevance (1). <i> American Documentation</i>,
    <b> 15 </b> (1), 26-34.
  <li><a name=inex></a>INEX (2002) <a href="http://qmir.dcs.qmw.ac.uk/INEX/index.html"> http://qmir.dcs.qmw.ac.uk/INEX/index.html</a>&nbsp;
  <li><a name=ing92></a>Ingwersen, P. (1992) <i> Information retrieval interaction</i>. London: Taylor Graham.
  <li><a name=ing96></a>Ingwersen, P. (1996) „Cognitive perspectives of information retrieval interaction: elements of a cognitive IR theory.
    <i> Journal of Documentation</i>, <b> 52 </b> (1), 3-50.
  <li><a name=jär00></a>Järvelin, K. & Kekäläinen, J. (2000) IR evaluation methods for retrieving highly relevant documents. 
  In: Belkin, N.J., Ingwersen, P. & Leong, M-K., eds.<i> Proceedings of the 23rd ACM Sigir Conference on Research and 
  Development of Information Retrieval, Athens, Greece, 2000.</i>  New York, N.Y.: ACM Press, pp. 41-48.
  <li><a name=jos98></a>Jose, J.M., Furner, J. &amp; Harper, D.J. (1998) Spatial querying for image retrieval. In:   Croft, B.W., Moffat, A., van Rijsbergen, C.J., Wilkinson, R. &amp; Zobel, J., eds.<i> 
  Proceedings of the 21st ACM Sigir Conference on Research and Development of Information Retrieval. Melbourne, 1998.</i>   ACM Press/York Press, pp. 232-240.
  <li><a name=kek02></a>Kekäläinen, J. &amp; Järvelin, K. (2002) Evaluating information retrieval systems under the challenges 
  of interaction and multidimensional dynamic relevance. In: Bruce, H., Fidel, R., Ingwersen, P. & Vakkari, P., eds.
    <i> Emerging Frameworks and Methods, Seattle, 2002.</i> Colerado: Libraries Unlimited, pp. 253-270.
  <li><a name=kuh93></a>Kuhlthau, C.C. (1993) <i>Seeking meaning: a process approach to library and information
    science</i>.  Norwood, NJ: Ablex Publishing.
  <li><a name=lan69></a>Lancaster, W.F. (1969) Medlars: report on the evaluation of its operating efficiency.
    <i> American Documentation</i>, <b>20</b> (2), 119-142.
  <li><a name=lan93></a>Lancaster, W.F. &amp; Warner, A.J. (1993) <i> Information retrieval today</i>. Arlington: Information Resources Press.
  <li><a name=los96></a>Losee, R.M. (1996) Evaluating retrieval performance given database and query characteristics: analytical determination of performance surfaces.
    <i> Journal of the American Society for Information Science</i>, <b> 47</b> (1), 95-105.
  <li><a name=los98></a>Losee, R.M. (1998) <i> Text retrieval and filtering: analytical methods of performance. Norwell</i>, Massachusetts: Kluwer Academic Publishers.
  <li><a name=nie02></a>Nielsen, M.L. (2002) <i> The word association method: a gateway to work-task based retrieval.</i> Åbo Akademi University Press, Åbo. Doctoral dissertation: Åbo Akademi University.
  <li><a name=par93></a>Park, T.K. (1993) The nature of relevance in information retrieval: an empirical study.
    <i> Library Quarterly</i>, <b> 63</b> (3), 318-351.
  <li><a name=pre94></a>Preece, J. et al. (1994) <i> Human-computer interaction</i>. Wokingham, England: Addison Wesley.&nbsp;
  <li><a name=ras94></a>Rasmussen, J., Pejtersen, A.M. & Goodstein, L.P. (1994) <i> Cognitive systems engineering</i>. N.Y.: John Wiley &amp; Sons.
  <li><a name=rei99></a>Reid, J. (1999) A new, task-oriented paradigm for information retrieval: implications for 
  evaluation of information retrieval systems. In: Aparac, T., Saracevic, T., Ingwersen, P. & Vakkari, P., eds. 
    <i> Proceedings of CoLIS 3, Third International Conference on the Conceptions of Library and Information 
    Science: Digital Libraries: Interdisciplinary concepts, challenges and opportunities. Dubrovnik 1999.</i> 
    Zagreb: Zavod za informacijske studije Odsjeka za informacijske znanosti: Filozofski fakultet; Lovke: Naklada Benja, pp. 97-108. 
  <li><a name=rei00></a>Reid, J. (2000) A task-oriented non-interactive evaluation methodology for information retrieval systems.
    <i> Information Retrieval</i>, <b> 2</b> (1), 113-127.
  <li><a name=rob81></a>Robertson, S.E. (1981) The methodology of information retrieval experiment. In: Sparck Jones, K. ed.,
    <i> Information retrieval experiments.</i> London: Butterworths, pp. 9-31.
  <li><a name=rob92></a>Robertson, S.E. &&nbsp; Hancock-Beaulieu, M.M. (1992) On the evaluation of IR systems.
    <i> Information Processing &amp; Management</i>, <b> 28 </b> (4), 457-466.
  <li><a name=rob97></a>Robins, D. (1997) Shifts of focus in information retrieval interaction. In:
  Schwartz, C. & Rovrig, M., ed.<i> Proceedings of the ASIS annual meeting</i> (34). Silver Spring, Maryland, pp. 123-134.
  <li><a name=rod01></a>Rodden, K., Basalaj, W., Sinclair, D. & Wood, K. (2001) Does organisation by similarity assist 
  image browsing? In:<i> Proceedings of Human Factors in Computing Systems (CHI 2001), Seattle, WA.</i> ACM Press, pp. 190-197. 
  <li><a name=ror99></a>Rorvig, M. (1999) Images of similarity: a visual exploration of optimal similarity metrics and scaling properties of TREC topic-document sets.
    <i> Journal of the American Society for Information Science,</i> <b> 50</b> (8), 639-651.
  <li><a name=rut01></a>Ruthven, I. (2001) <i>Abduction, explanation and relevance feedback</i>. University of Glasgow. Doctoral dissertation. Technical report: TR-2002-115.
  <li><a name=rut02></a>Ruthven, I., Lalmas, M. & van Rijsbergen, K. (2002) Ranking expansion terms with partial and ostensive 
  evidence. In: Bruce, H., Fidel, R., Ingwersen, P. & Vakkari, P., eds.<i> Emerging Frameworks and Methods, Seattle, 2002</i>. 
  Colerado: Libraries Unlimited, pp.199-219.
  <li><a name=sar84></a>Saracevic, T. (1984) Measuring the degree of agreement between searchers. 
  In: Flood, B,  Witiak, J. & Hogan, T.H., eds.<i> Proceedings of the 47th ASIS annual meeting</i>. White Plains, NY, pp. 227-230.
  <li><a name=sar95></a>Saracevic, T. (1995) Evaluation of evaluation in information retrieval. 
  In: Fox, E.A., Ingwersen, P., & Fidel, R., eds. <i> Proceedings of the 18th ACM Sigir Conference on Research and 
  Development of Information Retrieval. Seattle, 1995.</i> N.Y.: ACM Press, pp. 138-146.
  <li><a name=sar96></a>Saracevic, T. (1996) Relevance reconsidered '96. In: 
  Ingwersen, P. & Pors, N.O., eds. <i> Proceedings of CoLIS 2, Second International Conference on 
  Conceptions of Library and Information Science: Integration in Perspective.</i> Copenhagen 1996. Copenhagen: Royal School of Librarianship, pp. 201-218. 
  <li><a name=sch94></a>Schamber, L. (1994) Relevance and information behavior. In: Williams, M.E., ed.
    <i> Annual Review of Information Science and Technology (ARIST) (29).</i> 
  Medford, NJ: Learned Information, INC. pp. 3-48.
<li><a name=sch90></a>Schamber, L. Eisenberg, M.B. & Nilan, M.S. (1990) 
  A re-examination of relevance: toward a dynamic, situational definition.
    <i> Information Processing &amp; Management</i>, <b>26</b> (6), 755-775.
  <li><a name=spi98></a>Spink, A., Greisdorf, H., & Bateman, J. (1998) From highly relevant to not relevant: examining different regions of relevance.
    <i> Information Processing &amp; Management</i>, <b>34</b> (5), 599-621.
  <li><a name=ste97></a>Stephen, P. & Hornby, S. (1997) <i> Simple statistics: for library and information professionals. 2nd
    edition</i>. London: Library Association Publishing.
  <li><a name=su92></a>Su, L.T. (1992) Evaluation measure for interactive information retrieval.
    <i> Information Processing & Management</i>, <b> 28</b> (4), 503-516.
  <li><a name=swa77></a>Swanson, D.R. (1977) Information retrieval as a trial-and-error process.
    <i> Library Quarterly</i>, <b> 47</b> (2), 128-48.
  <li><a name=swa86></a>Swanson, D.R. (1986) Subjective versus objective relevance in bibliographic retrieval systems.
    <i> Library Quarterly</i>, <b>56</b> (4), 389-398.
  <li><a name=tag92></a>Tague-Sutcliffe, J. (1992) The pragmatics of information retrieval experimentation, revisited.
    <i> Information Processing &amp; Management</i>, <b> 28</b> (4), 467-490.
  <li><a name=tan98></a>Tang, R. & Solomon, P. (1998) Towards an understanding of the dynamics of relevance judgments: an analysis of one person's search behavior.
    <i> Information Processing &amp; Management</i>,<i> </i><b> 34 </b> (2/3), 237-256.
  <li><a name=targ></a>TARGET on Dialog: 'how-to' guide. (1993). Palo Alto, USA: Dialog, 10 p.
  <li><a name=tay68></a>Taylor, R.S. (1968) Question negotiation and information seeking in libraries.
    <i> College and Research Libraries</i>, <b>29</b> (3), 178-194.
  <li><a name=uhr02></a>Uhrskov, U.F. (2002) Er der forskel i søgeadfærd mellem humaniora- og naturvidenskabsstuderende?
    <i>Biblioteksarbejde</i>, <b> 22</b> 63/1, 5-19.
  <li><a name=urq99></a>Urquhart, C. (1999) Using vignettes to diagnose information seeking strategies: opportunities 
  and possible problems for information use studies of health professionals. 
  In: Wilson, T.D. and Allen, D.K., eds.<i>  Proceedings of the 2nd international conference on research in 
  information needs, seeking and use in different contexts.</i> Sheffield, UK, 1998. London: Taylor Graham, pp. 277-289.
  <li><a name=urq01></a>Urquhart, C. (2001) Bridging information requirements and information needs assessment: do scenarios and vignettes provide a link?
    <i> Information Research</i>, <b> 6 </b> (2).  <a href="http://www.informationr.net/ir/6-2/paper102.html"> http://www.informationr.net/ir/6-2/paper102.html</a>
  <li><a name=vak99></a>Vakkari, P. (1999) Task complexity, problem structure and information actions: integrating studies on information seeking and retrieval.
    <i> Information Processing &amp; Management</i>, <b>35</b> (6), 819-837.
  <li><a name=vas95></a>Vassileva, J. (1995) Ensuring a task-based individualized context for 
  information retrieval from multimedia information systems for hospitals. In:
  <i> Proceedings of IJCAI-95 Workshop on Intelligent Multimedia Information Retrieval, Montreal, 1995</i>, pp. 172-185.
  <li><a name=vas96></a>Vassileva, J. (1996) A task-centered approach for user modelling in a hypermedia office documentation system.
    <i> User Modelling and User Adapted Interaction</i>, <b> 6</b> (2-3), 185-223.
  <li><a name=voo01></a>Voorhees, E. (2001) Evaluation by highly relevant documents. 
  In: Croft, W.B., Harper, D.J., Kraft, D.H. & Zobel, J., eds. <i> Proceedings of the 24th ACM SIGIR Conference on Research a
  nd Development of Information Retrieval.</i> New Orleans, LA, 2001. New York, N.Y.: ACM Press, pp. 41-48. 
  <li><a name=wal89></a>Walker, S. (1989) The Okapi online catalogue research projects. In: 
  Hildreth, C.R., ed. <i> The Online catalogue: developments and Directions.</i> London: The Library Association, pp. 84-106. 
  <li><a name=wer71></a>Wersig, G. (1971) <i> Information - kommunikation - dokumentation: ein beitrag zur orientierung der informations- dokumentationswissenschaften.</i> München-Pullach: Verlag Dokumentation Saur KG.
  <li><a name=whi01></a>White, R., Jose, J. & Ruthven, R. (2001) Query-biased web page summarisation: a 
  task-oriented evaluation. In: Croft, W.B., Harper, D.J., Kraft, D.H. & Zobel, J., eds. 
    <i> Proceedings of the 24th ACM SIGIR Conference on Research and Development of Information Retrieval.</i> New Orleans, LA, 
  2001. New York, N.Y.: ACM Press, (poster), pp. 412-413.
  <li><a name=whi02></a>White, R., Ruthven, I. & Jose, J. (2002) Finding relevant documents using top ranking sentences: 
  an evaluation of two alternative schemes. In: Beaulieu, M., Baeza-Yates, R., Myaeng, S.H. & Järvelin. K., eds.
    <i> Proceedings of the 25th ACM SIGIR Conference on Research and Development of Information Retrieval.</i>  
    Tampere, Finland, 2002. New York, N.Y.: ACM Press, pp. 57-64.
  <li><a name=wil73></a>Wilson, P. (1973) Situational relevance. <i> Information Storage and Retrieval</i>,
    <b>9</b> (8), 457-469.
  <li><a name=wil99></a>Wilson, T. D. (1999) Exploring models of information behaviour: the 'uncertainty' project.    <i> Information Processing &amp; Management</i>, <b>35</b> (6), 839-849.
  <li><a name=win86></a>Winograd, T & Flores, C.F. (1986) <i> Understanding computers and cognition</i>.
    Norwwood, NJ: Addison-Wesley.<br>
    <br>
</ul>

<hr style="COLOR: #000080" size="1" />
<table cellspacing="10" align="center">
<tr><td colspan="2" align="center" style="font-family: verdana; font-size: small; font-weight: bold;">Find other papers on this subject.<br /></td></tr>
<tr><td align="center" valign="top">
<center>
<form method="get" action="http://scholar.google.com/scholar" target="_blank">
<table bgcolor="#ffffff">
<tr><td nowrap="nowrap" valign="top" align="center" height="32">
<input type="text" name="q" size="31" maxlength="255" value="evaluation &quot;interactive information retrieval&quot;" style="background-color: Yellow;"></input> <br />
<input type="submit" name="sa" value="Scholar Search"  style="font-family: Verdana; font-weight: bold;"></input>
<input type="hidden" name="num" value="100"></input>
</td></tr></table></form>
</center>
<td align="center" valign="top">
<!-- Search Google -->
<center>
<form method="get" action="http://www.google.com/custom" target="_top">
<table bgcolor="#ffffff">
<tr><td nowrap="nowrap" valign="top" align="center" height="32">
<input type="text" name="q" size="31" maxlength="255" value="evaluation &quot;interactive information retrieval&quot;" style="background-color: Yellow;"></input><br />
<input type="submit" name="sa" value="Google Search" style="font-family: Verdana; font-weight: bold;" /></input>
<input type="hidden" name="client" value="pub-5081678983212084"></input>
<input type="hidden" name="forid" value="1"></input>
<input type="hidden" name="ie" value="ISO-8859-1"></input>
<input type="hidden" name="oe" value="ISO-8859-1"></input>
<input type="hidden" name="cof" value="GALT:#0066CC;GL:1;DIV:#999999;VLC:336633;AH:center;BGC:FFFFFF;LBGC:FF9900;ALC:0066CC;LC:0066CC;T:000000;GFNT:666666;GIMP:666666;FORID:1;"></input>
<input type="hidden" name="hl" value="en"></input>
</td></tr></table>
</form>
</center>
<!-- Search Google -->
</td></tr>
</table>

<hr style="COLOR: #000080" size="1" />
<div align="center">
<h4>How to cite this paper:</h4>
<p style="text-align : center; color : black;">Borlund, Pia  (2003)&nbsp; &quot;The IIR evaluation model: a framework for evaluation of interactive information retrieval systems&quot;<em>Information Research</em>, <strong>8</strong>(3), paper no. 152 [Available at: http://informationr.net/ir/8-3/paper152.html]</p>
<p style="text-align : center">&copy; the author, 2003.</p></div>
<hr color="#ff00ff" size="1">
								<div align="center">Articles citing this paper, <a href="http://scholar.google.com/scholar?hl=en&amp;lr=&amp;q=link:HT5AqRIYqTMJ:scholar.google.com/" target="_blank">according to Google Scholar</a></div>
								 <hr color="#ff00ff" size="1">

<table border="0" cellpadding="15" cellspacing="0" align="center">
<tr> 
    <td><a href="infres83.html"><h4>Contents</h4></a></td>
   <td align="center" valign="top"><h5 align="center"><img src="http://counter.digits.com/wc/-d/-z/6/-b/FF0033/paper152" ALIGN=middle  WIDTH=60 HEIGHT=20 BORDER=0 HSPACE=4 VSPACE=2><br><a href="http://www.digits.com/">Web Counter</a></h5></td>
    <td><a href="../index.html"><h4>Home</h4></a></td>
  </tr>
</table>
<hr color=#ff00ff size=3>

<script src="http://www.google-analytics.com/urchin.js" type="text/javascript">
</script>
<script type="text/javascript">
_uacct = "UA-672528-1";
urchinTracker();
</script>
</body>
<!-- Mirrored from informationr.net/ir/8-3/paper152.html by HTTrack Website Copier/3.x [XR&CO'2010], Sat, 25 Dec 2010 19:11:20 GMT -->
</html>