<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
	"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html>

<!-- Mirrored from informationr.net/ir/9-3/paper177.html by HTTrack Website Copier/3.x [XR&CO'2010], Sat, 25 Dec 2010 19:20:59 GMT -->
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1" />
<link rel="stylesheet" href="../IRstyle.css" />
<link rel="alternate stylesheet" type="text/css" media="screen" title="sans" href="../IRstylesans.css" />
<title>Three looks at users: a comparison of methods for studying digital library use. User studies, Digital libraries, Digital music libraries, Music, Information use, Information science, Contextual inquiry, Contextual design, User research, Questionnaires, Log file analysis</title>
<meta name="ROBOTS" content="ALL" />
<meta name="DC.Title" content="Three looks at users: a comparison of methods for studying digital library use" />
<meta name="DC.Creator" content="Mark Notess" />
<meta name="DC.Subject" content="user studies, digital libraries, digital music libraries, music, information use, information science, contextual inquiry, contextual design, user research, questionnaires, log file analysis" />
<meta name="DC.Description" content="Compares three user research methods of studying real-world digital library usage within the context of the Variations and Variations2 digital music libraries at Indiana University. After a brief description of both digital libraries, each method is described and illustrated with findings from the studies. User satisfaction questionnaires were used in two studies, one of Variations (n=30) and the other of Variations2 (n=12).  Second, session activity log files were examined for 175 Variations2 sessions using both quantitative and qualitative methods. The third method, contextual inquiry, is illustrated with results from field observations of four voice students' information usage patterns. The three methods are compared in terms of expertise required; time required to set up, conduct, and analyse resulting data; and the benefits derived. Further benefits are achieved with a mixed-methods approach, combining the strengths of the methods to answer questions lingering as a result of other methods." /> 
<meta name="dc.publisher" content="Professor T.D. Wilson" />
<meta name="dc.coverage.placename" content="global" />
<meta name="dc.subject.keywords" content="Endangered languages, Ontology, Digital library, Multimedia, EMELD, Intelligent querying and retrieval, ImageSpace" />
<meta name="dc.type" content="text" />
<meta name="dc.identifier" scheme="ISSN" content="1368-1613" />
<meta name="dc.format" content="text/html" />
<meta name="dc.language" content="en" />
<meta name="dc.rights" content="http://creativecommons.org/licenses/by-nd-nc/1.0/" />
<link rev="made" href="mailto:t.d.wilson@shef.ac.uk" />
<script language="javascript" type="text/javascript">

		var flag;
		flag = true;
		function doChangeFont()
		{
			if (flag)
			{
			var htmlDoc = document.getElementsByTagName('head').item(0);
			var css = document.createElement('link');
			css.setAttribute('rel', 'stylesheet');
			css.setAttribute('type', 'text/css');
			css.setAttribute('href', '../sans.css');
			htmlDoc.appendChild(css);
			flag = false;
			} 
			else
			{
			var htmlDoc = document.getElementsByTagName('head').item(0);
			var css = document.createElement('link');
			css.setAttribute('rel', 'stylesheet');
			css.setAttribute('type', 'text/css');
			css.setAttribute('href', '../IRstyle.css');
			htmlDoc.appendChild(css);
			flag = true;
			}	
		}
		
	</script>

<style type="text/css">
#button {
	width: 45em;
	padding: 0 0 0 0;
	font-family: Verdana, Lucida, Geneva, Helvetica, Arial, sans-serif;
	font-size: small;
	font-weight: bold;  
	background-color: #ffffff;
	color: #000000;
	display: inline;
	text-align: center;
	}
		

#button ul {
		list-style: none;
		margin: 0;
		padding: 0;
		border: none;
		display: inline;
		}
		
#button li {
		margin: 0;
		font-family: Verdana, Lucida, Geneva, Helvetica, Arial, sans-serif;
	    font-size: small;
	    font-weight: bold;  
		background-color: #fff000<!-- #2175bc; -->
		color: #000000;
		text-decoration: none;
		display: inline;
		}

#button li a:hover {
		background-color: azure;
		color: #ff0000;
		width: auto;
		}

</style>
<style type="text/css">
<!--
.style1 {color: #FF0000}
-->
</style>
</head>

<body  bgcolor="#ffffff">
<table align="center" border="0" cellpadding="0" cellspacing="0">
<tr><td height="30" align="center" colspan="5"> <img src="../mini_logo2.gif" width="336" height="45" alt="header" /><br />
Vol. 9  No. 3, April 2004<br /><br /><div id="button">
<ul>
	<li><a href="infres93.html">Contents</a> | </li>
	<li><a href="../iraindex.html">Author index</a> | </li>
	<li><a href="../irsindex.html">Subject index</a> | </li>
	<li><a href="../search.html">Search</a> | </li>
	<li><a href="../index-2.html">Home</a></li>

</ul>
</div></td></tr>
  <tr> 
    <td>&nbsp;</td>
  </tr>
</table>
<hr size="3" style="color:#000080 ;" />

<h1>Three looks at users: a comparison of methods for studying digital library use</h1>

<h4 align="center"><a href="mailto:mnotess@indiana.edu">Mark Notess</a><br />
University Information Technology Services<br />
Cook Music Library, Indiana University<br />
Bloomington, Indiana, USA</h4>
<br />
<div align="center">
<input type="button" value="change font" class="btn" style="font-variant: small-caps; font-weight: bold; font-family: Verdana; color: Blue;" onclick="doChangeFont()" /></div>
<hr style="COLOR: #000080" size="1" />
<div align="center"><b>Abstract</b></div>
<br />
<blockquote>Compares three user research methods of studying real-world digital library usage within the context of the <strong>Variations</strong> and <strong>Variations2</strong> digital music libraries at Indiana University. After a brief description of both digital libraries, each method is described and illustrated with findings from the studies. User satisfaction questionnaires were used in two studies, one of <strong>Variations</strong> (n=30) and the other of <strong>Variations2</strong> (n=12).  Second, session activity log files were examined for 175 <strong>Variations2</strong> sessions using both quantitative and qualitative methods. The third method, contextual inquiry, is illustrated with results from field observations of four voice students' information usage patterns. The three methods are compared in terms of expertise required; time required to set up, conduct, and analyse resulting data; and the benefits derived. Further benefits are achieved with a mixed-methods approach, combining the strengths of the methods to answer questions lingering as a result of other methods.</blockquote>
<hr style="COLOR: #000080" size="1" />
<br />

<h2>Introduction</h2>

<p>Usability has been a continuing topic of interest in the digital library (DL) community. In 1995 the US National Science Foundation sponsored &quot;How We Do User-Centered Design and Evaluation of Digital Libraries: A Methodological Forum&quot; (<a href="#bis95">Bishop, 1995</a>). Two more recent examples of such workshops are the JCDL 2002 workshop, &quot;Usability of Digital Libraries&quot; (<a href="#bla02">Blandford &amp; Buchanan, 2002</a>) and the Fourth DELOS workshop, &quot;Evaluation of Digital Libraries: Testbeds, Measurements, and Metrics&quot; (<a href="#bor02">Borgman, 2002</a>), which, although not exclusively concerned with usability issues, included papers on user-centred topics.</p>

<p>Two general, comprehensive books about digital libraries each offer a chapter pertaining to usability: &quot;Usability and Retrieval Evaluation&quot; (<a href="#les97">Lesk, 1997</a>) and &quot;User Interfaces and Usability&quot; (<a href="#arm00">Arms, 2000</a>). Although both chapters emphasize fitting the DL interface to the user, neither suggests users be consulted or studied. Both take what may be called a &quot;weak&quot; approach to user-centred activity. Weak user-centredness believes it is adequate to base design upon ergonomic 
principles gleaned from prior studies or upon best practices. Strong user-centredness believes one's own users, tasks, and contexts must be examined because contexts differ too much and design is too complex an activity for general ergonomic principles and best practices, though important, to be sufficient (see <a href="#fla97">Flanagan, <em>et al.</em>, 1997</a>, for a similar, more comprehensive dichotomy).</p>

<p> One of the more common methods for studying users is laboratory-based user testing, examining representative or surrogate users as they work through set tasks in a controlled environment. For example, a survey of Digital Library Federation members reports:</p>

<blockquote>Half of the DLF respondents reported conducting or planning to conduct user protocols. With rare exception, libraries appear to view think-aloud protocols as the premier research method for assessing the usability of OPACs, Web pages, local digital collections, and vendor products. (<a href="#cov02">Covey, 2002</a>:  24).
</blockquote>

<p> Although we have used and continue to use laboratory-based testing in our work (<a href="#fuh01">Fuhrman,
<em><em>et al.</em></em>, 2001</a>; <a href="#swa02">Swan, <em>et al.</em>, 2002</a>; <a href="#swa02b">Swan, 2002b</a>; <a href="#swa03">Swan, 2003</a>), this paper describes studies of digital library usage in natural settings. We used three methods:</p>

<ul>
<li>User satisfaction questionnaire (2 studies).</li>
<li>Session activity logging.</li>
<li>Contextual inquiry.</li>
</ul>

<p>These studies were motivated by a recognition that laboratory-based testing differs from actual use, a difference we are exploring in greater detail (<a href="#not03">Notess &amp; Swan, 2003</a>). Unlike many studies of DL use, our studies primarily examined information use rather than information seeking. A final characteristic of our user studies is that they are motivated by product design: we are  interested in learning about usage for the purpose of improving the design of our digital library. This intention places us in Fallman's (<a href="#fal03">2003</a>) 'research-oriented design' category: we are focused on product, conducting user research in order to generate and improve that product. Although this paper presents findings from our various user research methods, those findings are included because they illustrate the methods rather than because the user-study findings themselves are of general importance. The significant result in this paper is the description of and comparison amongst the user study methods. After a brief description of our digital music library systems, the method and findings of each study are summarized. Then, comparisons are drawn among the methods, looking at cost, skill requirements, and the nature and usefulness of the findings.</p>

<h2>Variations and Variations2</h2>

<p><strong>Variations</strong> is a heavily used digital music library, providing online access to music recordings and scores in the Indiana University (IU) Cook Music Library since 1996 (<a href="#dun99">Dunn &amp; Mayer, 1999</a>). <strong>Variations</strong> contains over 200 scores (printed music) and 8,000 near-CD quality digitized recordings, most of which are classical, although jazz and popular music are also represented. The online collection has grown in direct response to requests for online reserve material or for access to special collection items. Items are available from the online catalogue and from course reserve Web pages. Students can use <strong>Variations</strong>  recordings from any of ninety PCs in the music library. Faculty can use <strong>Variations</strong>  from several music classrooms to support instruction.</p>

<p>Items in <strong>Variations</strong> are accessed by URL. Recording URLs in the MARC-based online library catalogue (IUCAT) lead to a Web page summarizing the item and its contents. Links on course reserve pages also lead to item Web pages. The <strong>Variations</strong> player window (Figure 1) is invoked by clicking on a link on the item Web page. Because <strong>Variations</strong> audio files are stored on tape and copied to disc-based cache on demand, users may have to wait several minutes for the player to appear if no one has requested that item during the previous day or two. <strong>Variations</strong> score items do not have summary Web pages: their
URLs display the scanned musical score in a standard browser window.</p>

<div align="center"><img border="0" src="p177fig1.gif" width="390" height="268" alt="fig1" /></div>
<br />
<div align="center">Figure 1. <strong>Variations</strong> audio player window</div>

<p><strong>Variations2</strong> is a completely new digital music library system under development as part of a research project (<a href="#var03">Variations2, 2003</a>). Even though the <strong>Variations2</strong> collection is quite limited at this point (approximately sixty recordings and six scores), it is already deployed in the music library and has experienced pilot usage by several faculty and their students. <strong>Variations2</strong> also offers an audio player, but a score viewer and search window, along with additional functionality such as bookmarking, make <strong>Variations2</strong> a far more capable system than the original <strong>Variations</strong> (see Figure 2).</p>

<div align="center"><img border="0" src="p177fig2.jpg" alt="fig2" width="591" height="575" /></div>
<br />
<div align="center">Figure 2. <strong>Variations2</strong> score viewer, audio player, and bookmark editor windows</div>
<br />

<h2>The questionnaire studies</h2>

<p>We have completed two questionnaire-based studies of actual use. The first study looked at usage of Variations; the second study looked at usage of <strong>Variations2</strong> during a pilot deployment. Both studies used a user satisfaction questionnaire adapted from QUIS (<a href="#chi88">Chin, <em>et al.</em>, 1988</a>). The questionnaire contained a section for collecting demographic information, presented 11 subjective satisfaction ratings on 7-point Likert scales, and invited respondents to offer additional comments.</p>

<p>The purpose of the first study was to establish a baseline for user satisfaction with the existing DL implementation. Students in the Cook Music Library were recruited as they entered the computer laboratory. Thirty students who used <strong>Variations</strong> during their visit completed a printed questionnaire after completing their planned work and received a gift certificate as compensation for their participation. While we could have surveyed music students at large (nearly all of whom use <strong>Variations</strong> regularly), we wanted to get questionnaire responses immediately after a specific experience with <strong>Variations</strong> rather than simply asking for their general feelings about the software.</p>

<p>The purpose of the second study was to get an early comparative reading of user satisfaction with the new DL, again taking that reading after a specific experience. <strong>Variations2</strong> is not yet in broad use because of its limited catalogue size, so we selected a particular class assignment for the
pilot. Approximately thirty students in a graduate, song literature class were asked to complete a listening assignment (students listen to several songs and write an analysis of each), but for this pilot, they were required to use <strong>Variations2</strong> instead of Variations. Before the pilot, students were given a brief introduction to <strong>Variations2</strong> along with a demonstration. The assignment was presented as a Web page with hyperlinks to each of the songs (<a href="#m5302">M531 Schubert listening assignment, 2002</a>). For each song, students could access a recording, the score, and the song text in German and English. The song texts were provided in the score viewer as scanned images even though they were not scores. The Web page also invited them to click on a link and complete a Web-based satisfaction questionnaire after completing the assignment, but no compensation was offered for doing so. Twelve students submitted questionnaires.</p>

<h3>Questionnaire results</h3>

<p>Detailed findings from the first study (Variations) are available in a technical report (<a href="#swa02a">Swan, 2002a</a>). A summary of findings is given in Table 1. A number in parentheses following a comment indicates how many respondents made a similar comment.</p>


<table width="80%" border="1" cellspacing="0" cellpadding="3" align="center" style="border-right: #99f5fb solid; border-top: #99f5fb solid; font-size: smaller; border-left: #99f5fb solid; border-bottom: #99f5fb solid; font-style: normal; font-family: verdana, geneva, arial, helvetica, sans-serif; background-color: #fdffdd">
 <caption align="bottom"><br /><b>Table 1. Variations satisfaction survey findings summary</b> </caption>
  <tr bgcolor="#6DFCCD">
    <th width="210" valign="top" bgcolor="#CCFFCC">Topic</th>
    <th width="390" valign="top" bgcolor="#CCFFCC">Findings (n=30)</th>
  </tr>
  <tr>
    <td width="210" valign="top">Frequency of <strong>Variations</strong> use</td>
    <td width="390" valign="top">26 use <strong>Variations</strong> at least once a week. 7
      of these use it more than 5 times per week. 14 had first used
      <strong>Variations</strong> more than 2 years ago.</td>
  </tr>
  <tr>
    <td width="210" valign="top">Purpose for using Variations</td>
    <td width="390" valign="top">17 were studying for an exam or completing an
      assignment for class. 5 were doing personal listening.</td>
  </tr>
  <tr>
    <td width="210" valign="top">Satisfaction ratings</td>
    <td width="390" valign="top">Overall mean: 5.56 (1-7 scale). All
      items averaged above 5 except for &quot;slow...fast&quot;, for which the
      average was 4.77.</td>
  </tr>
  <tr>
    <td width="210" valign="top">Positive comments</td>
    <td width="390" valign="top">&quot;Very useful&quot; (2); &quot;simply
      tremendous to use...a veritable heaven for all musicians here&quot;.</td>
  </tr>
  <tr>
    <td width="210" valign="top">Negative comments</td>
    <td width="390" valign="top">Waiting to retrieve recordings and the
      inability to launch multiple retrievals simultaneously (7); difficulty in
      moving to a specific point in a recording and the delay in hearing the
      music (2); sound skipping or cutting off (2).</td>
  </tr>
  <tr>
    <td width="210" valign="top">Recommendations</td>
    <td width="390" valign="top">Provide more detailed information (liner notes,
      track times, etc.) (3); more music or types of music (2); improved search
      (2).</td>
  </tr>
</table>
<br />
<p>The satisfaction ratings from the second study (<strong>Variations2</strong> pilot assignment)
are compared with the ratings from the first study in Notess &amp; Swan (<a href="#not03">2003</a>).
A summary of findings is given in Table 2. Note that even though we were
studying <strong>Variations2</strong> use, we also asked the questions about <strong>Variations</strong> use.</p>
<table width="80%" border="1" cellspacing="0" cellpadding="3" align="center" style="border-right: #99f5fb solid; border-top: #99f5fb solid; font-size: smaller; border-left: #99f5fb solid; border-bottom: #99f5fb solid; font-style: normal; font-family: verdana, geneva, arial, helvetica, sans-serif; background-color: #fdffdd">
<caption align="bottom"><br /><b>Table 2: Variations2 satisfaction survey findings summary</b>
  </caption>
  <tr bgcolor="#6DFCCD">
    <th width="210" valign="top" bgcolor="#CCFFCC">Topic</th>
    <th width="390" valign="top" bgcolor="#CCFFCC">Findings (n=12)</th>
  </tr>
  <tr>
    <td width="210" valign="top">Frequency of <strong>Variations</strong> use</td>
    <td width="390" valign="top">All use <strong>Variations</strong> twice a week or more; 3 use it five times a week or more.</td>
  </tr>
  <tr>
    <td width="210" valign="top">Typical purposes for using Variations</td>
    <td width="390" valign="top">Class assignments, listening to course reserves, or exam preparation (11); performance or voice lesson preparation (11); personal listening or pleasure (4); research (1).</td>
  </tr>
  <tr>
    <td width="210" valign="top">Satisfaction ratings</td>
    <td width="390" valign="top">Overall mean: 5.38 (1-7 scale). All items averaged 5 or above except for &quot;number of screens/windows: confusing - very clear&quot;, for which the  average was 4.86.</td>
  </tr>
  <tr>
    <td width="210" valign="top">Positive comments</td>
    <td width="390" valign="top">Five students specifically
      praised the easy availability of the scores and song texts, saying that it
      saves time and improves the overall experience: &quot;my listening
      was accelerated greatly because I could reference the translations at any
      time&quot;, &quot;great to have the translations and text handy, saves
      time as well---great program&quot;, &quot;I love that I can look at a
      score and the text while listening to a song! That saves me so much
      time &amp; my experience is 10x better because of it.
      Thanks.&quot;. Other positive comments mentioned the speed
      improvement over <strong>Variations</strong> (2).</td>
  </tr>
  <tr>
    <td width="210" valign="top">Negative comments</td>
    <td width="390" valign="top">Students offered disparate
      complaints. The only issue mentioned by more than one student was the
      difficulty of handling the many windows (2). Most respondents offered just
      one critique although one respondent offered five thoughtful,
      well-described user interface issues.</td>
  </tr>
  <tr>
    <td width="210" valign="top">Recommendations</td>
    <td width="390" valign="top">Two students mentioned that
      they would like to have a &quot;repeat&quot; option--<strong>Variations</strong> does this
      but <strong>Variations2</strong> does not.</td>
  </tr>
</table>
<br />

<h2>The session activity logging study</h2>

<p>The <strong>Variations2</strong> software logs user activity. For this study we examined log files generated during the song literature pilot assignment mentioned above. Unlike many action log studies of digital libraries (e.g., <a href="#jon00">Jones, <em>et al.</em>, 2000</a>), users in our study were not searching for items in a digital library. Because items in the pilot assignment were pre-identified, students did not need to search. They merely opened the item and used it.</p>

<h3>Log file analysis method</h3>

<p>Each log file entry contains a timestamp, an identifier for the window, and information about the user action. Hence, logging captures information about window opening, closing and resizing; button pressing, etc.</p>

<p>Text-based log files are easily processed by scripts. We developed some basic tools for extracting each session from the log file into a separate text file and creating a summary file showing the length of each session and the materials accessed. Beyond that we did not use any specialized tools. Analysis
has been performed using Unix text processing command line tools.</p>

<p>A second form of analysis is the manual examination of a log file to try to reconstruct what the user did. The researcher walks through an individual session file performing a step-by-step analysis to interpret the user's experience.</p>

<h3>Log file analysis results</h3>

<p> Table 3 shows a selection of metrics that were generated from the log files. Although there were 175 total sessions, only sessions that included accessing media were analysed. The other sessions were either failed or neglected login attempts (13) or sessions where <strong>Variations2</strong> was launched from
the Start menu (34). In these latter cases, zero or more searches were performed but no items were opened.</p>

<table width="80%" border="1" cellspacing="0" cellpadding="3" align="center" style="border-right: #99f5fb solid; border-top: #99f5fb solid; font-size: smaller; border-left: #99f5fb solid; border-bottom: #99f5fb solid; font-style: normal; font-family: verdana, geneva, arial, helvetica, sans-serif; background-color: #fdffdd">
  <caption align="bottom"><br /><b>Table 3. Selection of log analysis results</b>
  </caption>
  <tr bgcolor="#6DFCCD">
    <th width="365" valign="top" bgcolor="#CCFFCC">Measure</th>
    <th width="156" valign="top" bgcolor="#CCFFCC">Results</th>
  </tr>
  <tr>
    <td width="365" valign="top">Number of sessions</td>
    <td width="156" valign="top">128</td>
  </tr>
  <tr>
    <td width="365" valign="top">Average session length</td>
    <td width="156" valign="top">30 minutes</td>
  </tr>
  <tr>
    <td width="365" valign="top">Average number of items
      accessed</td>
    <td width="156" valign="top">3.5</td>
  </tr>
  <tr>
    <td width="365" valign="top">Maximum number of windows
      open at one time</td>
    <td width="156" valign="top">7</td>
  </tr>
  <tr>
    <td width="365" valign="top">Maximum number of sessions in
      one day</td>
    <td width="156" valign="top">11</td>
  </tr>
  <tr>
    <td width="365" valign="top">Sessions using
      bookmarking functionality</td>
    <td width="156" valign="top">11% (14/128)</td>
  </tr>
  <tr>
    <td width="365" valign="top">Sessions where
      any menu item was selected</td>
    <td width="156" valign="top">17% (22/128)</td>
  </tr>
  <tr>
    <td width="365" valign="top">Sessions where
      record details were viewed</td>
    <td width="156" valign="top">23% (30/128)</td>
  </tr>
  <tr>
    <td width="365" valign="top">Total
      &quot;stop&quot; button presses</td>
    <td width="156" valign="top">200</td>
  </tr>
  <tr>
    <td width="365" valign="top">Total &quot;pause&quot; button presses</td>
    <td width="156" valign="top">385</td>
  </tr>
  <tr>
    <td width="365" valign="top">Total &quot;play&quot; button presses</td>
    <td width="156" valign="top">588</td>
  </tr>
  <tr>
    <td width="365" valign="top">Total number of manual slider
      adjustments</td>
    <td width="156" valign="top">295</td>
  </tr>
</table>

<p>Manual analysis yields a picture of an individual session. We have not completed manual analysis for the sessions, but we offer a narrative representation for one complete fifteen-minute session below as an example, using a pseudonym for the anonymous user. </p>

<blockquote>Karita began her session by clicking on the first song (3:02 in length) on the pilot assignment Web page. It took 28 seconds for her to log in, see the audio player, and hear the song. 16 seconds later, she paused the audio. 81 seconds later Karita clicked on the hyperlink in the audio player to view the detailed bibliographic information of the recording. After 6 seconds, she clicked on the score link on the assignment Web page. The score viewer took 11 seconds to appear. 45 seconds later, she closed the 'view details' window and maximized the score viewer. After 9 seconds, she clicked the 'Zoom in' button (zoomed to 70%). 7 seconds later, in the audio player, she clicked the 'rewind' button and then pressed 'play'. 20 seconds later, Karita clicked the 'next page' arrow in the score viewer (the score for the song is two pages long). 28 seconds later, she clicked the 'previous page' arrow (the song has a repeat of all but the two-measure introduction). After 27 seconds, she again went to the second page, and after 30 more seconds, again
back to the first page. 18 seconds later, Karita brought up the song text and translation by clicking on the Web page link. The text viewer took only 3 seconds to appear. Karita immediately   maximized the text viewer window. After 14 seconds, she went to the next page in the text (the text is split across 2 pages). 27 seconds later she pressed the 'stop' button on the audio player (this is 10 seconds before the song's end, but the singing had stopped and it was evident from the decrescendo that the song was over). After 67 seconds, Karita restarted the audio by clicking on the track in the track list and pressing 'play'. She immediately paged back to the first page in the text viewer. 41 seconds later she clicked to go to the next page, and then quickly went back and forward again. 94 seconds later she stopped the player 30 seconds shy of the song's end. Then there was no activity for 3 minutes, after which she restarted the audio as before and moved the slider twice to listen to specific parts of the song (for 11 and 21 seconds, respectively). Following a 69-second break, she proceeded to the second song in the listening list. The second audio player took only 3 seconds to appear. Karita then closed the first player, stopped the second one with the 'stop' button and then closed each of the remaining windows, ending the session. </blockquote>

<p>This session does not reveal any major issues. The slow initial startup is a known problem. The incident at the end of the session (starting a new audio player and then ending the session) is enigmatic but uninterpretable from the session's data. Looking at an immediately subsequent session from the same
network address, we can guess that Karita started a new session 42 minutes long and completed the entire assignment in these two sessions. But the reason for closing all the windows and then starting a new session 16 seconds later remains unknown. </p>

<p>A minor issue raised by this session is that Karita uses the 'stop' button and a click on the track list to go back to the beginning of the song. The <strong>Variations2</strong> 'stop' button rewinds the recording all the way to the beginning, unlike the <strong>Variations</strong> 'stop' button, which maintains playback position (analogous to 'pause'). </p>

<p>Knowing the nature of this specific assignment, we can make guesses about the reason for the gaps in listening: the student was writing up the song analysis, which is the deliverable for this assignment. But this is merely a guess. </p>

<h2>The contextual inquiry study</h2>


<p>Contextual inquiry is a field study technique combining observation and interview methods to gain an interpretation of work practice (<a href="#bey98">Beyer &amp; Holtzblatt, 1998</a>). The researcher watches someone doing normal work activity, takes notes, and asks questions to ensure a shared interpretation of the activity. Beyer &amp; Holtzblatt's Contextual Design process adds on to contextual inquiry a method for building work models (diagrams) with the inquiry data. The present study used both contextual inquiry and the work modelling process.</p>

<p>Four graduate voice students (3 male, 1 female) were observed during 14 contextual inquiry sessions that focused on information usage patterns in academic study. Participants were selected from among volunteers recruited in a graduate song literature class. Participants received a gift certificate for participating in up to 5 hours of sessions. We observed a range of academic activities. Although we mainly observed library work, we wanted to get a holistic picture of the students' information-related activities, so we also included observations of voice lessons, a class session and a rehearsal. The library work often included use of Variations, but not <strong>Variations2</strong>, which at that time had an extremely limited amount of content.</p>

<p>For each contextual inquiry session, the participants were asked to do whatever work they needed to do next and were observed in their usual contexts, typically a library computer carrel or work table. While a participant worked, the researcher took handwritten notes. Discussion of the work sometimes happened during the observations, and the sessions nearly always ended with a discussion of the observed activity. The focus of both the observations and discussions was to understand what the user was doing and why. Especially of interest were any problems participants encountered in their work (breakdowns).</p>

<h3>Contextual inquiry findings</h3>

<p>Most of the observations (10/14) occurred in the music library. Kinds of activities observed include the following:</p>

<ul>
<li>Listening assignment: students listen to an assigned set of songs, or select from among assigned songs, and write brief analyses of what they hear and think.   (The pilot assignment described in the log file analysis section, above, is this kind of assignment.)</li> 

<li>Recital assignment: students plan an imaginary voice recital following one or more specific themes, e.g., British art songs related by some thread such as having the same poet or same subject matter.</li> 

<li>Audition 'package' preparation: students auditioning for, e.g., a summer singing job, select vocal material to polish that will meet usual audition criteria for quantity and variety while also showing off the strengths of their individual voices.</li> 

<li>Lesson piece, recital or performance part preparation: students study a particular piece or part for performance in their lesson, recital or a production. Study includes not only listening to and/or watching various performances but also uncovering background information about the composer, the poet, the performers, etc. Study may also include making a literal translation of the text.</li> 

<li>Song analysis project: students perform an in-depth analysis of a particular song, tracing its history through various performers and performances.</li> 

<li>Examination preparation: students study a body of work so that they can identify and discuss a given song upon hearing it in an examination.</li></ul>

<p>Contextual inquiry generates a large amount of data, and the work models themselves are also detailed and extensive. This paper presents a fraction of the findings to give a flavour for the kind of results achievable with this research method. Of the various work models constructed from the data, the sequence model was the most interesting. A sequence model captures the step-by-step actions users take to accomplish an intent. Multiple sequence models can be consolidated to derive typical task sequences for a category of work. Table 4 shows a high-level view of a consolidated sequence model. The contextual inquiry data from the library observations consolidated into two typical sequences, 'study in detail' and 'collect and select'. The two consolidated sequences share their overall activity structure (shown in the first column) and also share some steps (shown by the table cells that span both of the right-hand columns). Although there were indications (and it is rather obvious) that the 'collect and select' consolidated sequence is sometimes followed by 'study in detail', none of the sessions included both.</p>

<table width="80%" border="1" cellspacing="0" cellpadding="3" align="center" style="border-right: #99f5fb solid; border-top: #99f5fb solid; font-size: smaller; border-left: #99f5fb solid; border-bottom: #99f5fb solid; font-style: normal; font-family: verdana, geneva, arial, helvetica, sans-serif; background-color: #fdffdd">
  <caption align="bottom"><br /><b>Table 4. Two common, consolidated sequences of library work</b>
  </caption>
  <tr>
    <th width="33%" bgcolor="#CCFFCC">Activity</th>
    <th width="33%" bgcolor="#CCFFCC">&quot;Study in Detail&quot;</th>
    <th width="34%" bgcolor="#CCFFCC">&quot;Collect and Select&quot;</th>
  </tr>
  <tr>
    <td width="33%" rowspan="2">Prepare to do library work</td>
    <td width="67%" colspan="2">- get headphones<br />
      - find available carrel<br />
      - locate assignment<br />
      - log in and locate on-line tools</td>
  </tr>
  <tr>
    <td width="33%">- select piece to study
      <br />- retrieve known recording<br />- retrieve known auxiliary materials
      (scores, texts, reference works)</td><td width="33%">&nbsp;</td>
      </tr>
  <tr>
    <td width="33%" rowspan="2">Work with library materials</td>
    <td width="33%">- study material (listen, and follow along in score and/or
      text; repeat whole piece or key parts)<br />
      - make personal notes to capture key points gleaned from studying</td>
    <td width="34%">- find candidate materials<br />
      - examine many details quickly to decide which to select (listen, check
      length, performer, key, etc.)<br />
      - make personal notes to guide selection</td>
  </tr>
  <tr>
    <td width="67%" colspan="2">- write assignment deliverable</td>
  </tr>
  <tr>
    <td width="33%">Wrap-up the work</td>
    <td width="67%" colspan="2">- preserve notes and/or assignment deliverable
      (email to self, save on Zip disk or network drive, print)<br />
      - log out<br />
      - pack up<br />
      - return reserve materials<br />
      - return headphones</td>
  </tr>
</table>


<p>For each of the sequence steps, there is a further level of detail showing some of the variation in how the work is performed and the breakdowns (problems) that can occur. For example, Table 5 shows the three ways to &quot;retrieve known recording&quot; from the &quot;study in detail&quot; task sequence. &quot;BD&quot; indicates a breakdown.</p>

<table width="80%" border="1" cellspacing="0" cellpadding="3" align="center" style="border-right: #99f5fb solid; border-top: #99f5fb solid; font-size: smaller; border-left: #99f5fb solid; border-bottom: #99f5fb solid; font-style: normal; font-family: verdana, geneva, arial, helvetica, sans-serif; background-color: #fdffdd">
  <caption align="bottom"><br /><b>Table 5. Detailed steps for &quot;retrieve
    known recording&quot;</b>
  </caption>
  <tr><th  bgcolor="#CCFFCC">Option 1:</th><th  bgcolor="#CCFFCC">Option 2:</th><th  bgcolor="#CCFFCC">Option 3:</th></tr>
  <tr>
    <td valign="top">
    - Find course reserve list<br />
    - Scroll to desired recording (BD: reserve list may be very long)<br />
    - Select item (BD:  easy to pick wrong item due to title similarities)</td>
    <td valign="top">
       - Looking at assignment sheet, type <strong>Variations</strong> URL for item in browser field</td>
    <td valign="top">
       - Enter search terms in online catalogue<br />
    - Scroll through search results to
      find desired item (BD: easy to pick wrong item due to title similarities
      and lack of distinct visited-link colour)<br />
    - Select item</td>
  </tr>
  <tr>
    <td valign="top" colspan="3">Common final step:<br />
      - Select CD/Side within <strong>Variations</strong> Web page to retrieve item</td>
  </tr>
</table>
<br />
<p>Each of the breakdowns in Table 5 provides an opportunity for improving usability in the existing product or points out a difficulty to consider in the design of a new product. The different approaches to accomplishing the work indicate differences between users (e.g., willingness to type in a long URL), as
well as between faculty (e.g., willingness to put the item URL on an assignment sheet and hand it out to students).</p>

<h2>Comparison of methods</h2>

<p>User study methods can be compared in several dimensions. From the  perspective of research methodology, the satisfaction questionnaires and log analysis are primarily quantitative whereas contextual inquiry is a qualitative approach. Using Wilson's (<a href="#wil02">2002</a>) methods classification, satisfaction questionnaires are an indirect, imposed method of observation; contextual inquiry and log analysis are both direct, emergent forms of observation, albeit very different ones. For broad descriptions of and comparisons amongst user study methods, see Kuniavsky (<a href="#kun03">2003</a>). The paper by Covey (<a href="#cov02">2002</a>) provides a survey of how such methods are currently used in research libraries, and Bishop, <em>et al.</em>, (<a href="#bis03">2003</a>) contains a variety of digital library user study reports.</p>

<p>In this section, we will describe our experience with each of the methods we used in terms of the
expertise and time required, other costs, and the kind of benefit derived. In examining benefits, we consider whether the results seem most helpful formatively (for design) and/or summatively (for overall system assessment).</p>

<h3>Questionnaire studies</h3>

<p>The chief expertise required for our questionnaire studies was survey design. The need for this expertise was moderately reduced because we borrowed ideas and items from an established user satisfaction instrument (<a href="#chi88">Chin, <em>et al.</em>, 1988</a>). However, survey design was still important because we wanted to modify the survey to fit our study, which included adding items. Some Web form processing technical expertise was required to set up the online survey.</p>

<p>For the first questionnaire, conducted at point of use in the library, we spent three separate three-hour sessions recruiting participants. After we collected the paper surveys, it took a half-day to transcribe the results. The second questionnaire, conducted online, took a half-day to create, aided by a survey generator tool (<a href="#pernd">Perlman, n.d.</a>). This survey took no time to administer and several minutes to copy and paste the e-mailed responses into a text file, which was then imported into a spreadsheet. For both questionnaires, analysis time was limited to a half-day spent categorizing the free response data and summarizing the demographic and Likert-scale items. An additional cost of the first questionnaire study was providing gift certificates worth $3 for each of the 30 participants.</p>


<p>The benefit derived from the survey data was mainly summative: the ability to compare between
<strong>Variations</strong> and <strong>Variations2</strong>, identifying major differences. The formative benefit was not large. Although we learn key satisfiers, dissatisfiers, and opportunities for improvement, the findings tended to confirm things we already knew or suspected. For example, we 
expected students would appreciate not having to wait while recordings were  copied from tape to disk, and in fact the <strong>Variations2</strong> survey results confirmed this. But the survey's open-ended items also yielded insights we did not expect, e.g., that students were delighted with the online scores because they felt they were wasting time hunting down scores in the library. This insight suggests an 
area for further investigation so that we can better characterize the benefit of <strong>Variations2</strong> to library patrons. Also helpful were the five recommendations for user interface improvement from one respondent.  But many of the open-ended item responses were unhelpfully cryptic (e.g., &quot;I love Variations!&quot;, while heartening, doesn't give us much to work on).</p>

<h3>Session logging study</h3>

<p>Our session logging study required significant technical expertise. To log user activity, we had to  insert statements into our program at the appropriate places so that, when users click a button or take some other action in <strong>Variations2</strong>, the program writes out a record of that action to the log file. This required expertise in Java programming as well as access to the source code. Processing of the data was accomplished by writing some Perl scripts and running Unix commands. The detailed session analysis was accomplished by hand on paper but also required running <strong>Variations2</strong> and examining logging output experimentally to try to reproduce the sequence of entries in the log file.</p>

<p>Once the logging mechanism was in place, the study took no time to execute. Analysing the results has taken several days and is still incomplete. For example, analysing the fifteen-minute session included in this paper took approximately two hours and is the only session we have yet analysed in detail. To
analyse and summarize all sixty-four hours of data would be a full-time job for several months. There were no additional costs for this study beyond disk space, which only amounted to a trivial 1.4MB for all the log files.</p>

<p>The main benefit of the session logging data was obtaining accurate, quantitative measures of overall utilization and feature usage. These data may identify usability problems or issues addressable either by training or software modification, but the benefit is more summative. Part of the problem is that the log files do not tell us anything about user motivation or rationale. For instance, we noted that only 11% of user sessions used bookmarking. But we do not know why the other 89% did not make use of this feature. Was it because they did not see the feature? Or because they did not need it for the tasks they were doing? We can only guess.</p>

<p>Detailed log file analysis offers more promise in providing formative insights, but such analysis can likewise raise as many questions as it answers. Why did the user close all the windows after opening the second audio player and then immediately restart? Log file analysis raises such questions; other methods
must answer them.</p>

<h3>Contextual inquiry study</h3>

<p>The contextual inquiries and subsequent analysis required expertise in the Contextual Design methodology, a skill best developed by guided practice under expert supervision.</p>

<p>The inquiry sessions took eight to ten hours to plan and arrange, including visiting a class to explain the study and recruit participants, and email exchanges with participants to schedule the sessions. Because contextual inquiry is a real-time method, the fourteen inquiries of one to two hours in length took approximately twenty-four hours to conduct. Creating the work models and then putting them into a presentable format took approximately ten days. An additional cost of this study was $25 gift certificates for each of the four participants.</p>

<p>Contextual inquiry illuminated how our tools fit into users' tasks and contexts. A strength of the method is the comprehensive user stories it generates. The stories are also detailed (because of the observations and note taking) and memorable (because they were seen first hand). We have used contextual inquiry data to make design decisions and prioritize requirements, a significant formative benefit. The researcher who conducted the inquiries internalized much of the data simply by observing, but the modelling process helped identify patterns and made the results sharable with the other members of the development team. The small number of subjects in this study limited the summative benefit.</p>

<p>Without exception, the contextual inquiry participants (people being observed) were enthusiastic about the process. They enjoyed sharing their work and life with someone who was interested in understanding it and possibly improving it. Scheduling observations was sometimes difficult because of the students' busy schedules. Because of this, contextual inquiry is not a method to be used on a very short time schedule.</p>

<h3>Comparison summary</h3>

<p>Method choice depends on available resources and objectives. Table 6 summarizes our experience with these three methods. The values in this table are approximate and represent what happened in our particular studies.</p>


<table width="80%" border="1" cellspacing="0" cellpadding="3" align="center" style="border-right: #99f5fb solid; border-top: #99f5fb solid; font-size: smaller; border-left: #99f5fb solid; border-bottom: #99f5fb solid; font-style: normal; font-family: verdana, geneva, arial, helvetica, sans-serif; background-color: #fdffdd">
  <caption align="bottom"><br /><b>Table 6: Method comparison summary</b>
  </caption>
  <tr>
    <th width="25%">&nbsp;</th>
    <th width="25%" bgcolor="#CCFFCC" valign="bottom">Satisfaction Questionnaire</th>
    <th width="25%" bgcolor="#CCFFCC" valign="bottom">Session Activity Logging</th>
    <th width="25%" bgcolor="#CCFFCC" valign="bottom">Contextual Inquiry</th>
  </tr>
  <tr>
    <td width="25%" bgcolor="#CCFFCC" valign="top">Expertise</td>
    <td width="25%" valign="top">survey design</td>
    <td width="25%" valign="top">technical (programming,
      scripting)</td>
    <td width="25%" valign="top">observation, interviewing,
      work modelling</td>
  </tr>
  <tr>
    <td width="25%" bgcolor="#CCFFCC" valign="top">Time to set up</td>
    <td width="25%" valign="top">moderate</td>
    <td width="25%" valign="top">low</td>
    <td width="25%" valign="top">moderate</td>
  </tr>
  <tr>
    <td width="25%" bgcolor="#CCFFCC" valign="top">Time to conduct</td>
    <td width="25%" valign="top">none (online) to moderate
      (recruit &amp; use paper survey)</td>
    <td width="25%" valign="top">none</td>
    <td width="25%" valign="top">high, longitudinal</td>
  </tr>
  <tr>
    <td width="25%" bgcolor="#CCFFCC" valign="top">Time to analyse</td>
    <td width="25%" valign="top">low</td>
    <td width="25%" valign="top">moderate (metrics generation)
      to very high (manual analysis of all files)</td>
    <td width="25%" valign="top">high</td>
  </tr>
  <tr>
    <td width="25%" bgcolor="#CCFFCC" valign="top">Benefit</td>
    <td width="25%" valign="top">primarily summative; can 
    uncover some topics for further investigation</td>
    <td width="25%" valign="top">summative (metrics
      generation) and formative (manual analysis)</td>
    <td width="25%" valign="top">primarily formative</td>
  </tr>
</table>


<p>It is not necessary, of course, to use these methods in isolation from one 
another. We are using all these methods in our research and development effort, 
and it can be very helpful to compare findings amongst methods. For example, the 
lowest-rated item in the <strong>Variations2</strong> satisfaction survey was 'number of 
screens/windows: confusing - very clear'. Our log analysis helps us 
determine how many windows users had open at one time (maximum was seven). This 
combined information on usage patterns and satisfaction can help us seek better 
design solutions. For example, we are now working on a playlist concept so 
that students' listening pieces for a given unit may all be accessed from within 
a single window.</p>


<p>Contextual inquiry, because of its high cost, cannot be done often enough to 
provide quantitative data. So even though we discovered two common work 
activities, 'collect and select' and 'study in detail', contextual inquiry will 
not tell us what proportion of users engage in which type of activity. However, 
it is a simple matter to use a questionnaire to determine this, and we can base 
the questionnaire items on the task steps uncovered by contextual inquiry.</p>


<h2>Conclusion</h2>


<p>This paper has described using three methods to study digital library use in
its natural setting and has also compared the costs and benefits of the methods.
Each of these methods has its value, and we plan to continue using all
three. The time required for contextual inquiry can be reduced simply by
doing fewer sessions. Watching even one user is likely better than
watching none. We can build better tools for analysing log files, perhaps
automating much of the metrics generation. Questionnaires
offer an
inexpensive albeit gross measure of user satisfaction and occasionally yield
helpful suggestions. And all three methods can be fruitfully combined to provide 
a more holistic picture of use.</p>


<p>A limitation of this paper is that we did not apply all three methods to the
same digital library: the studies were split between <strong>Variations</strong> and
<strong>Variations2</strong>. A future opportunity is to use all three methods during a
pilot project and compare the value of the results. In addition, findings from
these studies of usage should be compared to findings from laboratory-based testing to
help us better understand the difference between the two contexts.
Finally, we plan to compare findings from different user populations to see how
DL use by, e.g., a small sample of graduate voice students compares to other user
communities.</p>


<h2>Acknowledgements</h2>


<p>The author acknowledges all the members of the <strong>Variations2</strong> team
for their help with this research. Jon Dunn, Maggie Swan, and
Michelle Dalmau provided helpful review. Special thanks are due to
Gary Arvin for granting access to his excellent students, to whom also thanks.</p>


<p>This material is based upon work supported by the National Science Foundation
under Grant No. 9909068. Any opinions, findings, and conclusions or
recommendations expressed in this material are those of the author and do not
necessarily reflect the views of the National Science Foundation.</p>


<h2>References</h2>

<ul>

<li><a id="arm00" name="arm00"></a>Arms, W.Y. (2000). <i>Digital libraries.</i>   Cambridge, MA: MIT Press.</li>

<li><a id="bis95" name="bis95"></a>Bishop, A. (Ed). (1995). <a  href="http://www.lis.uiuc.edu/gslis/allerton/95/index.html"><i>How we do user-centered design and analysis for digital libraries: a methodological forum</i></a> Paper delivered at the 37th Allerton   Institute, Oct. 29-31, 1995, Graduate School of Library and Information Science University of Illinois at Urbana-Champaign.  Retrieved 13 June, 2003 from http://edfu.lis.uiuc.edu/allerton/95.</li>

<li><a id="bis03" name="bis03"></a>Bishop, A.P., VanHouse, N.A., &amp; Buttenfield, B.P., (Eds.) (2003). <i>Digital library use: social practice in design and evaluation.</i> Cambridge, MA: MIT Press.</li>

<li><a id="bey98" name="bey98"></a>Beyer, H. &amp; Holtzblatt, K. (1998). <em>Contextual design: defining customer-centered systems.</em> San Francisco, CA: Morgan Kaufmann.</li>

<li><a id="bla02" name="bla02"></a>Blandford, A. &amp; Buchanan, G. (Eds.) (2002). <i><a href="http://www.uclic.ucl.ac.uk/annb/DLUsability/JCDL02.html">JCDL'02 Workshop on Usability of Digital Libraries.</a></i> Retrieved 13 June, 2003 from University College London Interaction Centre Website:  http://www.uclic.ucl.ac.uk/annb/DLUsability/JCDL02.html.</li>

<li><a id="bor02" name="bor02"></a>Borgman, C.  (2002). <i><a href="http://www.sztaki.hu/conferences/deval/presentations/final_report.html"> Final Report to National Science Foundation, Computer and Information Science Directorate, Information and Intelligent Systems Division, Digital Libraries Program on the Fourth DELOS Workshop. Evaluation of Digital Libraries: Testbeds, Measurements, and Metrics Hungarian Academy of Sciences Computer and Automation Research Institute (MTA SZTAKI) Budapest, Hungary, 6-7 June 2002</a></i>. Retrieved 13 June, 2003 from the Computer and Automation Research Institute of the Hungarian Academy of Science Website: http://www.sztaki.hu/conferences/deval/presentations/final_report.html</li>

<li><a id="chi88" name="chi88"></a>Chin, J.P., Diehl, V.A., &amp; Norman, K. (1988).  Development of an instrument measuring user satisfaction of the human-computer interface. In: E. Soloway, D. Frye, &amp; S.B. Sheppard (Eds.), (pp. 213-218).  <i> Proceedings of the SIGCHI conference on Human factors in computing systems, Washington DC, May 1988.</i>  New York, NY: ACM Press.</li> 

<li><a id="cov02" name="cov02"></a>Covey, D.T. (2002). <i><a href="http://www.clir.org/pubs/reports/pub105/pub105.pdf">Usage and usability assessment: library practices and concerns.</a></i> Washington, DC: Digital Library Federation and Council on Library and Information Resources. Retrieved 13 June, 2003 from http://www.clir.org/pubs/reports/pub105/pub105.pdf</li>

<li><a id="dun99" name="dun99"></a>Dunn, J.W., &amp; Mayer, C.A. (1999) "VARIATIONS: A digital music library system at Indiana University." In: Rowe, N. &amp; Fox, E.A. (Eds.) <i>DL '99: Proceedings of the Fourth ACM Conference on Digital Libraries, Berkeley, CA, August 1999</i>. (pp. 12-19).  New York, NY:  ACM Press.</li>

<li><a id="fal03" name="fal03"></a>Fallman, D. (2003). Design-oriented human-computer interaction. In: <i>Proceedings of the conference on Human factors in computing systems, Ft. Lauderdale, Florida, USA </i>, (pp. 225-232). New York, NY: ACM Press.</li>

<li><a id="fla97" name="fla97"></a>Flanagan, J., Huang, T., Jones: , &amp; Kasif, S., (Eds.) (1997). <em><a href="http://www.ifp.uiuc.edu/nsfhcs/final_report/toc.html">Human-centered systems:  information, interactivity, and intelligence. Final report.</a></em>  Retrieved 31 March, 2004 from the Image Formation and Processing Group at University of Illinois at Urbana-Champaign Website: http://www.ifp.uiuc.edu/nsfhcs/final_report/toc.html</li>
 
<li><a id="fuh01" name="fuh01"></a>Fuhrman, M., Gauthier, D., &amp; Dillon, A. (2001). <i><a href="http://variations2.indiana.edu/pdf/VariationsTest.pdf">Usability test of <strong>Variations</strong> and DML prototypes.</a></i> Retrieved 13 June, 2003 from the Variations 2 Digital Music Library Project, Indiana University Web site:  http://variations2.indiana.edu/pdf/VariationsTest.pdf</li>
  
<li><a id="jon00" name="jon00"></a>Jones, S., Cunningham, S.J., McNab, R. &amp; Boddie, S.  (2000). A transaction log analysis of a digital library. <i>International Journal of Digital Libraries,</i> <b>3</b>(2), 152-169.</li>
  
<li><a id="kun03" name="kun03"></a>Kuniavsky, M. (2003). <i>Observing the user experience: a practitioner's guide to user research.</i> San Francisco, CA: Morgan Kaufmann.</li>
  
<li><a id="les97" name="les97"></a>Lesk, M. (1997). <i>Practical digital libraries.</i> San Francisco, CA: Morgan Kaufmann.</li>
  
<li><a id="m5302" name="m5302"></a>M531 Schubert listening assignment. (2002). <a href="http://variations2.indiana.edu/pilot/example-schubert.html"><i>M531 Schubert listening assignment.</i></a>  Retrieved 13 June, 2003 from the Variations 2 Digital Music Library Project, Indiana University Web site: http://variations2.indiana.edu/pilot/example-schubert.html.</li>
  
<li><a id="not03" name="not03"></a>Notess, M. &amp; Swan, M.B. (2003). Predicting user satisfaction from subject
satisfaction. In: <i>Proceedings of the conference on Human factors in computing systems, Ft. Lauderdale, Florida, USA </i>, (pp. 738-739). New York, NY: ACM Press.</li>
  
<li><a id="pernd" name="pernd"></a>Perlman, G. (n.d.). <a href="http://www.acm.org/~perlman/question.html"><i>Web-based user interface evaluation with questionnaires.</i></a> Retrieved 13 June, 2003 from G. Perlman's personal Website: http://www.acm.org/~perlman/question.html</li>
  
<li><a id="swa02a" name="swa02a"></a>Swan, M. (2002a). <a href="http://variations2.indiana.edu/pdf/var-sat-survey.pdf"><i>Variations satisfaction survey results.</i></a> Retrieved 13 June, 2003 from the Variations 2 Digital Music Library Project, Indiana University Website: http://variations2.indiana.edu/pdf/var-sat-survey.pdf.</li>
  
<li><a id="swa02b" name="swa02b"></a>Swan, M. (2002b). <i><a href="http://variations2.indiana.edu/pdf/v2v1-early-test.pdf">Variations2:
  IU digital music library first-round usability test report.</a></i> Retrieved 13 June, 2003 from the Variations 2 Digital Music Library Project, Indiana University Website:
  http://variations2.indiana.edu/pdf/v2v1-early-test.pdf.</li>
  
<li><a id="swa03" name="swa03"></a>Swan, M. (2003). <i><a href="http://variations2.indiana.edu/pdf/v2v1-test.pdf">Variations2: IU digital music library version 1.0 usability test report.</a></i> Retrieved 13 June, 2003 from the Variations 2 Digital Music Library Project, Indiana University Website: http://variations2.indiana.edu/pdf/v2v1-test.pdf</li>
  
<li><a id="swa02" name="swa02"></a>Swan, M., Notess, M. &amp; Isaacson, E. (2002). <a href="http://variations2.indiana.edu/pdf/mmtt-usab-test-1.pdf"><i>MMTT
  prototypes usability testing first round report.</i></a>  Retrieved 13 June, 2003 from the Variations 2 Digital Music Library Project, Indiana University Website:  http://variations2.indiana.edu/pdf/mmtt-usab-test-1.pdf</li>
  
<li><a id="var03" name="var03"></a>Variations2 (2003). <a href="http://variations2.indiana.edu/"><i> Variations2: the Indiana University digital music library project.</i></a> Retrieved 13 June, 2003 from the Variations 2 Digital Music Library Project, Indiana University Website: http://variations2.indiana.edu</li>
  
<li><a id="wil02" name="wil02"></a>Wilson, T.D. (2002). <a href="http://informationr.net/tdw/publ/papers/slovak-02.html">  'Information science' and research methods.</a> <em>Kni&#381;nicn&aacute; a Informacn&aacute; Veda</em>, <strong>19</strong>, 63-71. Retrieved 3 March, 2004 from http://informationr.net/tdw/publ/papers/slovak-02.html</li>
 
</ul>
<hr style="COLOR: #000080" size="1" />
<table cellspacing="10" align="center">
<tr><td align="center" valign="top">
<form method="get" action="http://scholar.google.com/scholar" target="_blank">
<table bgcolor="#ffffff">
<tr><td nowrap="nowrap" valign="top" align="center" height="32">
<input type="hidden" name="q" size="31" maxlength="255" value="&quot;digital libraries&quot; &quot;user studies&quot;" style="background-color: Yellow;" /></input> <br />
<input type="submit" name="sa" value="Scholar Search"  style="font-family: Verdana; font-weight: bold; font-size: small;" /></input>
<input type="hidden" name="num" value="100" /></input>
</td></tr></table></form>
<td style="font-size: small; font-family: Verdana, sans-serif; font-weight: bold;">Find other papers on this subject</td>
<td align="center" valign="top">
<!-- Search Google -->
<form method="get" action="http://www.google.com/custom" target="_top">
<table bgcolor="#ffffff">
<tr><td nowrap="nowrap" valign="top" align="center" height="32">
<input type="hidden" name="q" size="31" maxlength="255" value="&quot;digital libraries&quot; &quot;user studies&quot;" style="background-color: Yellow;"></input><br />
<input type="submit" name="sa" value="Google Search" style="font-family: Verdana; font-weight: bold;font-size: small" /></input>
<input type="hidden" name="client" value="pub-5081678983212084"></input>
<input type="hidden" name="forid" value="1"></input>
<input type="hidden" name="ie" value="ISO-8859-1"></input>
<input type="hidden" name="oe" value="ISO-8859-1"></input>
<input type="hidden" name="cof" value="GALT:#0066CC;GL:1;DIV:#999999;VLC:336633;AH:center;BGC:FFFFFF;LBGC:FF9900;ALC:0066CC;LC:0066CC;T:000000;GFNT:666666;GIMP:666666;FORID:1;"></input>
<input type="hidden" name="hl" value="en"></input>
</td></tr></table>
</form>
<!-- Search Google -->
</td></tr>
</table>

<hr style="COLOR: #000080" size="1" />
<div align="center">
<h4>How to cite this paper:</h4>
  Notess, M. (2004)  &quot;Three looks at users: a comparison of methods for studying digital library use&quot;  <em> Information Research</em>, <strong>9</strong>(3)  [Available at http://InformationR.net/ir/9-3/paper177] </div><br />


<hr size="1" style="color:#000080 ;" />
<div align="center">Articles citing this paper, <a href="http://scholar.google.com/scholar?hl=en&amp;lr=&amp;q=link:OCdwiutJ37YJ:scholar.google.com/" target="_blank">according to Google Scholar</a></div>
<hr size="1" style="color:#000080 ;" />


<table align="center" cellpadding="10">
<tr><td align="center" valign="top"><div>
<img src="http://counter.digits.com/wc/-d/-z/6/-b/FF0033/paper177" align="middle"  width="60" height="20" border="0" hspace="4" vspace="2" alt="counter" /><br /><a href="http://www.digits.com/">Web Counter</a>
</div></td>


<td align="center" valign="top"><div>
&copy; the author, 2004. <br />Last updated: 31 March, 2004</div></td>

<td align="center" valign="middle"><img src="../valid-xhtml10.gif" alt="Valid XHTML 1.0!" height="16" width="44" />
</td></tr>
</table>
<hr size="1" style="color:#000080 ;" />

<table align="center"><tr><td><div id="button">
<ul>
	<li><a href="infres93.html">Contents</a> | </li>
	<li><a href="../iraindex.html">Author index</a> | </li>
	<li><a href="../irsindex.html">Subject index</a> | </li>
	<li><a href="../search.html">Search</a> | </li>
	<li><a href="../index-2.html">Home</a></li>
</ul>
</div></td></tr></table>         
		 
		 
		 
		 
		 
<hr size="3" style="color:#000080 ;" />
<script src="http://www.google-analytics.com/urchin.js" type="text/javascript">
</script>
<script type="text/javascript">
_uacct = "UA-672528-1";
urchinTracker();
</script>
</body>

<!-- Mirrored from informationr.net/ir/9-3/paper177.html by HTTrack Website Copier/3.x [XR&CO'2010], Sat, 25 Dec 2010 19:21:00 GMT -->
</html>