<?xml version="1.0" encoding="windows-1252"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
	"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

<!-- Mirrored from informationr.net/ir/8-4/paper161.html by HTTrack Website Copier/3.x [XR&CO'2010], Sat, 25 Dec 2010 19:11:22 GMT -->
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1" />
<title>Synchronised Object Retrieval: the enhancement of  information retrieval performance in multimedia environments using synchronisation protocols</title>
<link href="../IRstyle.css" rel="stylesheet" />
<meta http-equiv="Content-Type" content="text/html; charset=windows-1252" />
<meta content="Microsoft FrontPage 5.0" name="generator" />
<link rev="made" href="mailto:t.d.wilson@shef.ac.uk" />
<meta name="Keywords" content="information, information retrieval, synchronisation, multimedia retrieval, ynchronized Multimedia Integration Language, SMIL" />
<meta content="The retrieval of objects from within collections of multimedia presentations poses a number of problems but also offers opportunities for enhancing retrieval performance, by utilising  information about the relationships between objects. This paper is concerned with the theoretical  possibility of using the synchronisation information contained in SMIL-compliant multimedia presentations to retrieve objects which may either lack appropriate metadata or where the metadata  is insufficient to enable reliable retrieval. It suggests that the synchronicity of display of objects could be used to infer their content and that this would provide possibilities for enhancement of retrieval performance. It further suggests how this process might be achieved and recommends that an experimental collection of SMIL-compliant presentations needs to be established to enable experimental work to be undertaken." name="Description" />
<meta name="rating" content="mature" />
<meta name="vw96.objecttype" content="document" />
<meta name="robots" content="all" />
<meta name="dc.title" content="Synchronised object retrieval: the enhancement of information retrieval performance in multimedia environments using synchronisation protocols" />
<meta name="dc.creator" content="Brophy, P." />
<meta name="dc.subject" content="Synchronized Multimedia Integration Language, information, information retrieval, synchronisation, multimedia retrieval SMIL" />
<meta name="dc.description" content="The retrieval of objects from within collections of multimedia presentations poses a number of problems but also offers opportunities for enhancing retrieval performance, by utilising information about the relationships between objects. This paper is concerned with the theoretical  possibility of using the synchronisation information contained in SMIL-compliant multimedia  presentations to retrieve objects which may either lack appropriate metadata or where the metadata  is insufficient to enable reliable retrieval. It suggests that the synchronicity of display of  objects could be used to infer their content and that this would provide possibilities for  enhancement of retrieval performance. It further suggests how this process might be achieved and  recommends that an experimental collection of SMIL-compliant presentations needs to be established to enable experimental work to be undertaken." />
<meta name="dc.publisher" content="Professor T.D. Wilson" />
<meta name="dc.coverage.placename" content="global" />
<meta name="DC.Subject.keywords" content="information, information retrieval, synchronisation, multimedia retrieval SMIL" />
<meta name="DC.Subject" content="multimedia retrieval" />
<meta name="DC.Type" content="text" />
<meta name="DC.Identifier" scheme="ISSN" content="1368-1613" />
<meta name="DC.Relation.IsPartOf" content="infres84.html" />
<meta name="DC.Format" content="text/html" />
<meta name="DC.Language" content="en" />
<meta name="DC.Rights" content="http://creativecommons.org/licenses/by-nd-nc/1.0/" />
<script language="JavaScript" type="text/javascript">
<!--
function MM_preloadImages() { //v3.0
  var d=document; if(d.images){ if(!d.MM_p) d.MM_p=new Array();
    var i,j=d.MM_p.length,a=MM_preloadImages.arguments; for(i=0; i<a.length; i++)
    if (a[i].indexOf("#")!=0){ d.MM_p[j]=new Image; d.MM_p[j++].src=a[i];}}
}

function MM_findObj(n, d) { //v4.01
  var p,i,x;  if(!d) d=document; if((p=n.indexOf("?"))>0&&parent.frames.length) {
    d=parent.frames[n.substring(p+1)].document; n=n.substring(0,p);}
  if(!(x=d[n])&&d.all) x=d.all[n]; for (i=0;!x&&i<d.forms.length;i++) x=d.forms[i][n];
  for(i=0;!x&&d.layers&&i<d.layers.length;i++) x=MM_findObj(n,d.layers[i].document);
  if(!x && d.getElementById) x=d.getElementById(n); return x;
}

function MM_nbGroup(event, grpName) { //v6.0
  var i,img,nbArr,args=MM_nbGroup.arguments;
  if (event == "init" && args.length > 2) {
    if ((img = MM_findObj(args[2])) != null && !img.MM_init) {
      img.MM_init = true; img.MM_up = args[3]; img.MM_dn = img.src;
      if ((nbArr = document[grpName]) == null) nbArr = document[grpName] = new Array();
      nbArr[nbArr.length] = img;
      for (i=4; i < args.length-1; i+=2) if ((img = MM_findObj(args[i])) != null) {
        if (!img.MM_up) img.MM_up = img.src;
        img.src = img.MM_dn = args[i+1];
        nbArr[nbArr.length] = img;
    } }
  } else if (event == "over") {
    document.MM_nbOver = nbArr = new Array();
    for (i=1; i < args.length-1; i+=3) if ((img = MM_findObj(args[i])) != null) {
      if (!img.MM_up) img.MM_up = img.src;
      img.src = (img.MM_dn && args[i+2]) ? args[i+2] : ((args[i+1])? args[i+1] : img.MM_up);
      nbArr[nbArr.length] = img;
    }
  } else if (event == "out" ) {
    for (i=0; i < document.MM_nbOver.length; i++) {
      img = document.MM_nbOver[i]; img.src = (img.MM_dn) ? img.MM_dn : img.MM_up; }
  } else if (event == "down") {
    nbArr = document[grpName];
    if (nbArr)
      for (i=0; i < nbArr.length; i++) { img=nbArr[i]; img.src = img.MM_up; img.MM_dn = 0; }
    document[grpName] = nbArr = new Array();
    for (i=2; i < args.length-1; i+=2) if ((img = MM_findObj(args[i])) != null) {
      if (!img.MM_up) img.MM_up = img.src;
      img.src = img.MM_dn = (args[i+1])? args[i+1] : img.MM_up;
      nbArr[nbArr.length] = img;
  } }
}
//-->
</script>
</head>

<body  bgcolor="#ffffff" onload="MM_preloadImages('../figs/iauthori1.gif','../figs/isubji1.gif','../figs/isearch1.gif','../figs/ihome1.gif','../figs/contents1.gif')">
<table align="center" border="0" cellpadding="0" cellspacing="0">
<tr><td height="30" align="center" colspan="5"><h4>Information Research, Vol. 8  No. 4, July 2003</h4></td></tr>
  <tr> 
    <td><a href="infres84.html" target="_top" onclick="MM_nbGroup('down','group1','contents','',1)" onmouseover="MM_nbGroup('over','contents','../figs/contents1.gif','',1)" onmouseout="MM_nbGroup('out')"><img src="../figs/contents.gif" alt="contents" name="contents" border="0" id="contents"  /></a></td>
    <td><a href="../iraindex.html" target="_top" onclick="MM_nbGroup('down','group1','authorindex','',1)" onmouseover="MM_nbGroup('over','authorindex','../figs/iauthori1.gif','',1)" onmouseout="MM_nbGroup('out')"><img src="../figs/iauthori.gif" alt="auindex" name="authorindex" width="120" height="20" border="0" id="authorindex" /></a></td>
    <td><a href="../irsindex.html" target="_top" onclick="MM_nbGroup('down','group1','subjindex','',1)" onmouseover="MM_nbGroup('over','subjindex','../figs/isubji1.gif','',1)" onmouseout="MM_nbGroup('out')"><img src="../figs/isubji.gif" alt="subindex" name="subjindex" width="120" height="20" border="0" id="subjindex" /></a></td>
    <td><a href="../search.html" target="_top" onclick="MM_nbGroup('down','group1','search','',1)" onmouseover="MM_nbGroup('over','search','../figs/isearch1.gif','',1)" onmouseout="MM_nbGroup('out')"><img src="../figs/isearch.gif" alt="search" name="search" width="120" height="20" border="0" id="search" /></a></td>
    <td><a href="../index.html" target="_top" onclick="MM_nbGroup('down','group1','home','',1)" onmouseover="MM_nbGroup('over','home','../figs/ihome1.gif','',1)" onmouseout="MM_nbGroup('out')"><img src="../figs/ihome.gif" alt="home" name="home" border="0" id="home" /></a></td>
  </tr>
</table>
<hr size="3" style="color:#000080 ;" />
<h1>Synchronised Object Retrieval: the enhancement of  information retrieval performance in multimedia environments using synchronisation protocols</h1>

<h4 align="center"><a href="mailto:p.brophy@mmu.ac.uk">Peter Brophy</a><br />
  Centre for Research in Library and Information Management<br />
  Department of Information and Communications<br />
  Manchester Metropolitan University<br />
  Manchester, UK</h4>
<br />

<div align="center"><b>Abstract</b></div>

<blockquote>The retrieval of objects from within collections of multimedia presentations poses a 
number of problems but also offers opportunities for enhancing retrieval performance, by utilising
 information about the relationships between objects. This paper is concerned with the theoretical 
 possibility of using the synchronisation information contained in SMIL-compliant multimedia 
 presentations to retrieve objects which may either lack appropriate metadata or where the metadata 
 is insufficient to enable reliable retrieval. It suggests that the synchronicity of display of 
 objects could be used to infer their content and that this would provide possibilities for 
 enhancement of retrieval performance. It further suggests how this process might be achieved and 
 recommends that an experimental collection of SMIL-compliant presentations needs to be established 
 to enable experimental work to be undertaken.</blockquote>
 
<br />
 
<h2>Introduction</h2>

<p>One of the roles of a research centre is to engage in speculative enquiry which may increase 
understanding of existing or future systems or open up possible new avenues of enquiry. This paper 
is concerned with one such exploratory study undertaken by CERLIM (the Centre for Research in Library 
and Information Management at the Manchester Metropolitan University) from 1999 to 2002. The study in 
question arose from observations of a variety of separate research and development projects in the 
digital library and information retrieval area (including CERLIM's <a href="http://www.cerlim.ac.uk/projects/reviel.html">REVIEL</a> and <a href="http://www.cerlim.ac.uk/edner/welcome.html">EDNER</a> projects),  and concerned the question of how information objects expressed in a variety of media and perhaps embedded in multimedia presentations might be brought to the attention of end users. An obvious example would be an image or audio file which had been published as part of a complex multimedia presentation but which might contain information of relevance to a wide range of users who were not primarily interested in the complex object itself. An example might be a multimedia presentation on the Kalahari desert which incidentally contained an image of  a sunset. How might an end user compiling a presentation on sunsets find such an embedded image?</p>

<p>While the example above may appear trivial, it illustrates what is likely to be a growing problem, namely that information objects will increasingly be 'packaged' in ways that make them invisible to standard methods of retrieval. Are there potential ways of addressing this problem in real world scenarios which do not simply involve the well-nigh impossible – and probably unacceptable to rights-holders – task of creating vast new datasets of disaggregated objects?</p>

<p>One approach, which has intriguing possibilities, would be to exploit the existence of documented 
relationships within complex objects. The most useful part of such documentation might be the information 
which is created to control the display of the complex object on the client workstation. In particular, 
it might be feasible to use information designed to enable the streamed delivery of multimedia across 
the Internet for the enhancement of retrieval. The major problem with this type of delivery is that 
it cannot be taken for granted that each element of the presentation will arrive at the workstation 
when required. For example, because of network bandwidth or other limitations, it may be that a video 
track will be delayed while the accompanying audio arrives unhindered. Clearly, it would be 
unsatisfactory simply to play the audio and later, when it arrives, the video. To overcome this 
problem, the presentation includes a synchronisation track which not only tells the client the order 
in which to play individual components but contains check-points and other instructions on how to 
handle delayed or missing files.</p>

<p>One of the leading standards for synchronisation is called SMIL, the Synchronized Multimedia 
Integration Language, which is pronounced 'smile' (<a href="#WWWC">World Wide Web Consortium, 2000</a>). This formed the environment for a feasibility study, carried out with funding from the UK's <a href="http://www.resource.gov.uk/">Re:source, the Council for Museums, Archives and Libraries.</a> This paper presents the findings of both that study (the Final Report is available online: <a href="#Brophy">Brophy, <em>et al.</em>, 2000</a>) and subsequent work to examine the ways SMIL is being implemented. Sample SMIL presentations and a tutorial are available online from <a href="http://www.realnetworks.com/resources/samples/smilbasics.html">
RealNetworks</a>.</p>

<p>While for the purposes of the study reported here it was deemed useful to explore multimedia 
objects with explicit relationship coding, in principle there is no reason to suppose that the broad 
approach would not be more widely applicable. For example, it might be used to enhance retrieval of 
objects embedded in PowerPoint presentations by making an assumption that the individual slide itself 
signifies that a relationship exists between the different objects it contains.</p>

<h2>The concept of Synchronised Object Retrieval</h2>

<p>Because SMIL defines temporal and other relationships between objects within a multimedia presentation, it should be possible to extract retrieval clues from the simultaneity of playback of constituent 'micro-objects'. The term 'micro-object' is used here to describe any mono-media file which is a component of a multimedia presentation. For example, if it is known that a text file is to be played back at the same time as an image file, and a text-based search of the text file retrieves relevant keywords, then if the same query expressed as a content-based search of the image file also produces possible matches, the inference can be drawn that</p>

<ul>
  <li>the multimedia presentation as a whole has relevance to the query</li>
  <li>the instance identified (i.e. the simultaneous display of the text and image files) has relevance</li>
  <li>the text file has relevance</li>
  <li>the image file has relevance</li>
</ul>

<p>and each of these conclusions may be drawn with a higher degree of confidence than a simple 
search of any one component part of the multimedia presentation could produce. In other words, 
the known synchrony of the files allows reinforcement of retrieval conclusions. We have termed 
this concept Synchronised Object Retrieval (SOR).</p>

<p>It is suggested that the SOR approach could be used either</p>

<ul>
  <li>to enable the retrieval of individual micro-objects from within multimedia presentations across
   collections of heterogeneous multimedia presentations, or</li>
  <li>to enable the retrieval of multimedia presentations from one or more collection of such 
  presentations, by examining the results of searching across some or all of a presentation's 
  constituent micro-objects.</li>
</ul>

<p>There are many possible applications of such capability – an example would be that of a lecturer searching across a large collection of multimedia learning objects to find micro-objects to re-use in a new presentation.</p>

<p>In essence a SOR system would need to examine each multimedia presentation to determine its content, which might be expressed explicitly in metadata associated either with the complex object or with each micro-object or might be inferred by, for example, content-based image retrieval. The relationship between objects then needs to be examined to infer degrees of relevance.</p>

<p>As far as we have been able to determine the only related research in this area is that reported by 
Little, <em>et al.</em> where the objective is the dynamic generation of SMIL presentations by examining metadata and inferring <strong>semantic</strong> relationships. Temporal relationships in this work refer to, for example, dates of publication or of the life of a 'subject' or 'creator' of an object, not to temporal relationships in the display of the presentation:</p>

<blockquote>Using the Open Archive Initiative (OAI) as a testbed, we have developed a service which uses the Dublin Core metadata published by the OAI data providers, to infer semantic relations between mixed-media objects distributed across the archives. Using predefined mapping rules, these semantic relationships are then mapped to spatial and temporal relationships between the objects.... Our premise is that by using automated computer processing of metadata to organize and combine semantically-related objects within multimedia presentations, the system may be able to generate new knowledge, not explicitly recorded, by inferring and exposing previously unrecognized connections.  (Little, <em>et al.</em> <a href="#Little">2002</a>) </blockquote>

<p>With large collections of multimedia presentations a SOR system would require a very high level of computing power. While in the past this may have been a reasonable objection to the SOR approach, the effects of <a href="http://www.intel.com/research/silicon/mooreslaw.htm">Moore's Law</a>, which posits the 
inexorable doubling of computing power every eighteen to twenty-four months, suggests that computing power may not be an issue in the future; although recent commentary (e.g., <a href="#Tweney">Tweney, 2002</a>) has questioned whether this trend will continue. However, while this may be the case in theoretical terms, it seems unlikely that computing costs for large-scale retrieval systems will not continue to fall  or power per unit of expenditure increase for some time. Arms (<a href="#Arms">2000</a>) has made the observation that 'simple algorithms plus immense computing power often outperform human intelligence' and points out that processes which in the past would have been unthinkable are now routine, using the example of the Google Internet search engine:</p>

<blockquote>Evaluating the importance of documents would appear to be a task that requires human understanding, but Google's ranking algorithm does remarkably well entirely automatically…  Calculating the ranks requires the algorithm to iterate through a matrix that has as many rows and columns as there are pages on the web, yet with modern computing and considerable ingenuity, Google performs this calculation routinely.</blockquote>

<h2>SMIL - the Synchronized Multimedia Integration Language</h2>

<p>The Synchronized Multimedia Integration Language or SMIL standard is based on XML (<a href="http://www.w3.org/XML/">the eXtensible Markup Language</a>) and its syntax is defined in an XML DTD (document type definition). It is an official World Wide Web Consortium (W3C) standard. SMIL documents can be authored using a simple text editor, since they are in essence similar to HTML, although a variety of SMIL-enabled authoring tools are available. The project team investigated these and used the <a href="http://www.creativepro.com/software/home/1367.html">RealSlideshow Plus</a> package as a basis for exploring the potential of SMIL. A demonstration SMIL application was developed during the study by CERLIM staff using this tool.</p>

<p>As part of the study the team undertook a detailed evaluation of mainly web-based resources relevant to SMIL. Because the concept being explored would require familiarity with recent research in text, audio, video and other media retrieval, a wide-ranging literature review was also undertaken and this has been reported elsewhere (<a href="#Hartley">Hartley, <em>et al.</em> 2000)</a>.</p>

<p>The SMIL standard defines a number of different types of media micro-objects that can be included in a presentation.</p>

<ul>
  <li>animation: animated vector graphics or other animated format </li>
  <li>audio: audio clip </li>
  <li>img: still image </li>
  <li>text: text reference </li>
  <li>textstream: streaming text </li>
  <li>video: video clip </li>
  <li>ref: generic media reference (in effect, 'other')</li>
</ul>

<p>For example, an author can write synchronisation instructions such as: 'play audio file A in parallel with video file B' and 'display text file C after audio file A and in parallel with animation file D and with text stream E'. In addition SMIL enables the author to define the positioning of micro-objects on the user's screen (or 'visual rendering surface' in the terminology of the standard). Various other variables can be used, such as the ability to test the end-user's bandwidth and deliver alternative micro-objects accordingly. However, these features are unlikely to be of major significance in the context of an information retrieval system.</p>

<p>One of the advantages of the SMIL approach is that it effectively enables objects which would normally require high-bandwidth, such as multimedia presentations, to be delivered across low-bandwidth networks since each constituent micro-object  can be transmitted separately and the presentation correctly reassembled and played at the client end, using the synchronisation information. It thus has enormous potential for the distribution of all kinds of multimedia objects. Furthermore, there is no reason why the micro-objects cannot be repackaged and delivered within separate presentations, without the expense of re-recording a video or other complex element as might be the case were the presentation to be recorded as a single file of data. Among other useful aspects of the approach is the use of alternative files: for example, a commentary can be stored in a variety of languages and the user given the option as to which should be played, or the option can be picked up automatically from the user's preference settings. SMIL 2.0 (the current standard) has introduced the concepts of modularisation and profiling to enable functionality to be extended and the integration of related mark-up languages.</p>

<h2>SMIL and information retrieval</h2>

<p>While SMIL has not been designed with information retrieval (IR) applications in mind, the schema does contain some explicit IR-related features, most notably the provision of metadata fields at both object and micro-object levels. Thus, the standard defines a <a href="http://www.w3.org/RDF/">Resource Description Framework</a> (RDF) compliant 'metainformation' module which should enable the macro-object and its constituent micro-objects to be retrieved in the same way as any other documents by examining metadata elements. While the schema used is open, the SMIL 2.0 standard suggest the use of <a href="http://dublincore.org/">Dublin Core</a> (DC) as a generalised approach and an example of this usage can be found in section 8.5 of the standard. In addition to, say, subject-based retrieval, using the DC  subject, title and description elements for instance, a SOR system could use some elements to refine search results. Using the DC 'creator' element to find micro-objects which were created by the same person so as to infer some commonality might be an example of this approach. There is also a field intended for content rating metadata, such as the <a href="http://www.w3.org/PICS/">Platform for Internet Content Selection (PICS)</a>, which, in some circumstances, might be a useful aid to retrieval.</p>

<p>However a difficulty, as our later investigations showed, is that subject metadata fields are frequently unused by multimedia producers, especially at the micro-object level, or where they are used there is no meaningful vocabulary control. Thus while these fields would be examined in any real-life SOR application, it is to other features of SMIL that developers would need to look. Among those relevant to IR are:</p>

<ul>
  <li>SMIL supports the inclusion of linked objects in a presentation, through its Linking Module, enabling events 
such as a user clicking on an area of the screen, to trigger a link to another part of the presentation. As in HTML,  provision is made for supporting name fragment identifiers, by using the # symbol, to enable a link to be made to any tagged element within a micro-object.  For example, if the SMIL 2.0 documentation was itself part of a SMIL presentation, a link could be provided to http://www.w3.org/TR/smil20/extended-media-object.html#edef-ref where definitions of media object elements would be found. If this is not the start of the micro-object the effect would be as if the user had fast-forwarded within that object to a specified place. Further refinement is possible: for example, SMIL contains provision to enable the author to specify precedence levels for cases where the origin and linked objects continue to play simultaneously - an example would be setting the relative audio playback levels. Again, although we did not explore these in detail in the feasibility study, it would be possible for software to follow embedded links and to take account of associated variables to retrieve further micro-objects which may themselves contain both usable metadata and other retrieval clues.</li>

  <li>The SMIL 2.0 standard has specified various features intended to improve accessibility, especially for people with visual or other disabilities. For example a 'long description' attribute enables the author to provide a long text description, separate from the metadata fields, which could be very useful for enhancing retrieval. Because of the ability to define user-required variants it is also possible to allow for the identification of, for example, text equivalents of audio intended for closed-captioning, thus providing a further accessibility feature and additional IR possibilities. </li>

  <li>As noted above, a 'language' attribute is available, intended for presentations where the user has choice of, say, a French, German or English sound track: the multimedia presentation, of course. would contain three files, each synchronised with the video and other elements. Again, the possibilities for enriching IR capability, by combining searches in a variety of languages, are intriguing. </li>
</ul>

<p>Finally, it should be noted that some experimental work has been carried out on the building of SMIL compliant multimedia archives from the perspective of metadata <strong>creation</strong> (<a href="#Hunter">Hunter and Little, 2001</a>) although this does not address the use of relationships between micro-objects for retrieval.</p>

<h2>Experimental Design</h2>

<p>Although the CERLIM team has not yet developed the SOR concept beyond consideration of feasibility, initial work has been carried out to define the requirements for moving the SOR concept into an experimental stage. We believe that the elements described in the following sub-sections would be required.</p>

<h3>Establishment of test collection</h3>

<p>It will be necessary to establish a reasonably-sized test collection of SMIL-compliant multimedia presentations, showing a degree of heterogeneity in structure and micro-object composition  so as to challenge applications. Despite examing a number of applications we have not been able to identify a suitable collection of SMIL-compliant objects at the present time, although the work of Little, <em>et al.</em> referred to above must be noted. We also note that some candidate collections are restricted by IPR considerations.</p>

<p>It will be necessary to establish a test collection along the lines of <a href="http://trec.nist.gov/">TREC</a>, the Text Retrieval Conference. In order to test the SOR approach thoroughly in experimental conditions, a carefully designed collection would be needed. In other words, it would be essential to put effort into ensuring that the collection displayed examples of all the different elements on which retrieval could be based, but with special emphasis on syncronicity. Thus, not only would examples of objects with comprehensive metadata  at the object and micro-object levels be needed, but care would need to be taken to ensure that the different media used contained retrievable content, the attributes of which were known, in order that testing of software and algorithms could take place.</p>

<p>Because of the complexities inherent in the SOR approach, our conclusion is that a test collection should be domain-limited initially. For example, the use of a domain such as, say, the music of a specific composer or a specific locality's history and culture would avoid the worst trans-domain semantic and other problems. The examples that Little, <em>et al.</em> (<a href="#little">2002</a>) provide in their paper also show this concentration on a specific domain.</p>

<p>The development of a test collection would be a challenging piece of work in its own right, but would be an essential first step to the establishment of an experimental programme. It would be helpful to consult with the TREC team in defining this collection and it may be that the <a href="http://www-nlpir.nist.gov/projects/t01v/">TREC video track</a> would form a good starting point.</p>

<p>It is perhaps worth noting here that although the test collection would need to be carefully crafted, this does not mean that it would have to be entirely artificial. There would be merit in identifying a nascent, real-life collection which could be SMIL-enabled, not least because experimental work could then contribute to the solution of real-world problems and real-world queries could inform the evaluation of retrieval.</p>

<h3>User queries</h3>

<p>The analysis of the user query into mono-media components will itself be a complex process. Although in theory this could be automated, so that software converted a query expressed, let us say, as a text string ('Find something about the sun') into a series of media specific queries ('sun' [text]; 'sun' [audio wave file, retrieved from a 'thesaurus']; 'sun' [image description,'round yellow object on blue background', retrieved from another 'thesaurus']; and so on), in practice this is currently impractical  or at least a different area for research. In practice it may be feasible to generate a multi-media query by providing the user with an interface which enabled descriptions to be selected/entered in a series of 'channels' e.g. text box, select from image thumbnails to find 'things like', audio input terms and so on, although the problem of segmenting and recognising audio-based queries is non-trivial, and non-verbal audio, the sound of waves breaking on a beach, for example, poses even greater problems than speech.</p>

<p>It would be possible to make user input an iterative process by presenting to the user examples of retrieval terms, especially where non-textual retrieval was being undertaken. Thus, to extend the above example, an initial user query for an image of the sun might result in a series of thumbnails, extracted by reference to a thesaurus, being displayed and the user being invited to select from them or rank them in order of relevance. This additional user input could also be used to enable the system to learn from search preferences and possibly provide personalisation of results, although again this would be beyond the scope of the initial experiment.</p>

<p>As part of the feasibility study we developed a mock-up of the type of user interface we have in mind. This is shown in Fig. 1.</p> <div align="center"><img src="p161fig1.gif" alt="fig1" width="519" height="803" border="0" /></div>

<div align="center"><strong>Figure 1: Mock-up of a user interface</strong></div>

<h3>Query Analysis</h3>

<p>The user query is analysed into query statements capable of being applied within specific mono-media contexts as discussed above. So, for example, the query might be expressed as a series of text strings, some 
of which might be derived from speech, for matching against a text file, as a voice/sound wave format for matching against an audio file and so on. A later project might examine how, for example, text strings might be applied to an intermediate image thesaurus to create image queries not present in the original input data. As indicated above a possible way to use such query enhancement would be to display a set of images to the user and invite selection of the nearest matches. Again, we would expect this phase of work to draw on and be influenced by a range of research, especially in the content-based information retrieval (CBIR) field.</p>

<h3>Search of presentations</h3>

<p>The first process should consist of a search of the metadata associated with each multimedia presentation  as a whole. A ranking of objects in the collection could be achieved in this way, although it would be important not to discard presentations with apparent zero relevance from this process, since many objects may lack metadata and much presentation-level metadata may be irrelevant to specific component micro-objects.  An example of this could be a presentation designed to illustrate a geographical area which contained images of flora and fauna of the region but described merely by geographical location – the metadata would probably produce a 'zero relevance' result for a query for a particular plant or animal even if they were in fact represented in micro-objects.</p>

<p>This process leads to a score 'A' being computed for each presentation. As with other searches, this stage is intended to reveal clues, not to stand on its own as a retrieval process. Where some presentations do not have associated metadata it would be appropriate to provide a neutral score.</p>

<h3>Search of micro-objects' metadata</h3>

<p>The search of the micro-objects themselves could be carried out in a number of ways. For simplicity we suggest that the first step should be to check all micro-objects for embedded metadata, to search on this and to rank micro-objects for relevance accordingly. Again, many micro-objects will lack metadata so again all that is produced is a set of clues to relevance. This process allows a score Bx to be computed, where x is the micro-object sequential number. It should be noted that micro-objects may be regarded as being in any order for this purpose, provided only that they can be processed sequentially. Thus it may be appropriate to treat them as sequenced by filename, by file type, by 'running order' or in a variety of other ways. To avoid micro-objects without metadata being given the lowest scores it would again be appropriate to calculate a mean score for all micro-objects which have metadata and use this to ensure that metadata-less micro-objects score neutrally. It would of course be necessary to identify the most appropriate scoring model for the purpose.</p>

 <h3>Search of micro-objects' content</h3>
 
<p>Here we reach the core of the approach. We assume here that the aim is to retrieve micro-objects, although the process of retrieving presentations is similar, requiring only a method for combining the retrieval scores of constituent micro-objects. The process might take the following base model:</p>

<ol>
  <li>The synchronisation file is analysed to identify the first micro-object – note again that 'first' here is arbitrarily defined, simply implying that there is some ordering which ensures that each micro-object will be examined in sequence. The media type should be defined in the SMIL type attribute and this should be used to determine the type of retrieval process to be engaged. Using this process, micro-object 1 is examined, using a retrieval method other than examining metadata, as this has already been undertaken, and a score C1 computed. Again, the establishment of a scoring algorithm would be an important part of the experimental research.</li>
  <li>Micro-objects which are due to be played simultaneously with the first object (this should be interpreted as meaning that there is some temporal overlap, not that they start and end at the same time) are then examined against whatever media retrieval process is appropriate. The results of each analysis are used to modify C1, producing D1: again we have not defined the algorithm to be used, although it may be noted that different media types might be weighted differently if it was found that, say, text retrieval produced more reliable results that content-based image retrieval. The effect of this stage is that C1 is increased whenever a high probability of a match in a micro-object due for simultaneous play is identified.</li>
  <li>When micro-object 1 and all other micro-objects with which it shares some temporal space have been processed, the next micro-object in the sequence is examined and steps 1 and 2 repeated, providing scores D2, D3, D4 ……….. Dn.   It should be noted that because the synchrony is defined for each micro-object, it will be necessary to use the relevance score for each multiple times – clearly it will be more efficient to store these and re-use them during the process than to recompute them.</li>
  <li>At the end of the object analysis a relevance score for each micro-object is available. This is combined 
  - again, experimentation will be needed to establish the algorithm to be used - with the overall score A and the  metadata score Bn to produce a 'final' score for each micro-object, En. </li>
  <li>When all objects in the collection have been analysed, the micro-object scores En can be ranked to indicate the most likely matches overall. These micro-objects can then be displayed to the user.</li>
</ol>

<p>Where the object is to retrieve multimedia presentations rather than micro-objects, clearly a score for each macro-object can be computed by combining - again with a suitable, yet to be defined, algorithm - the scores for all constituent micro-objects and then ranking the overall score against those for other presentations.</p>

<h2>Conclusions</h2>

<p>The examination of the SMIL standard and its application described in this paper demonstrates that even in the absence of explicit metadata, or where the quality of available metadata is poor, it should be possible to develop systems which use information on the relationships between micro-objects to achieve enhanced retrieval performance. The standard also contains features, such as those intended to improve accessibility, which should be explored from a retrieval perspective.</p>

<p>It is suggested that the next stage of research should involve the development of a suitable test collection of SMIL-compliant material and the establishment of an experimental programme to enable the ideas explored in outline in the feasibility study to be subjected to in-depth research and development. It is the intention of CERLIM to pursue funding to enable these programmes of research to be established as soon as possible.</p>

<p>It is clear from our research that SOR has enormous potential for application in networked information environments containing heterogeneous collections of multimedia presentations. As such environments proliferate, as the number of multimedia presentations mushrooms and as the cost of computing power reduces, it is to be expected that SOR approaches will prove ever more valuable.</p>

<h2>Acknowledgements</h2>

<p>The author wishes to acknowledge the contributions of many colleagues in CERLIM and the Department of Information and Communications to this work. Particular thanks are due to the late Tony Oulton, who undertook much of the background literature review, and to Richard Eskins who set up SMIL demonstrators and designed the mock-up of a query entry page which is illustrated here in Figure 1.</p>

<h2>References</h2>
<ul>
  <li><a name="Arms"></a>Arms, W.Y. (2000) <a href="http://webdoc.gwdg.de/edo/d-lib/dlib/july00/arms/07arms.html">Automated digital libraries: how effectively can computers be used for the skilled tasks of professional librarianship?</a> <em>D-Lib Magazine</em>, <strong>6</strong>(7/8)  Retrieved 10 February 2003 from   http://webdoc.gwdg.de/edo/d-lib/dlib/july00/arms/07arms.html</li>
  
  <li><a name="Brophy"></a>Brophy, P., Eskins, R. and Oulton, T. (2000) <em><a href="http://www.cerlim.ac.uk/projects/synchro/finalv2.pdf">Synchronised Object Retrieval: a feasibility study into enhanced   information retrieval in multimedia environments using synchronisation protocols</a></em> Manchester: Centre for Research in Library &amp; Information Management.  (Library and Information Commission Research Report 92)  Retrieved 17 July 2003 from http://www.cerlim.ac.uk/projects/synchro/finalv2.pdf</li>

  <li><a name="Hartley"></a>Hartley, R.J.,  Johnson, F.J. and Oulton, A.J. (2000) Image, audio, text: a review of recent research in information retrieval. <em>The New Review of Information and Library Research.</em> <strong>6</strong>, 171-206. </li>

  <li><a name="Hunter"></a>Hunter, J. and Little, S. (2001) <a href="http://archive.dstc.edu.au/RDU/staff/jane-hunter/ECDL01/ECDL01.html">Building and indexing a distributed multimedia presentation archive using SMIL.</a>  In: P. Constantopoulos and I.T. Solvberg <em>eds.</em> <em>Research and advanced technology for digital libraries.  5th European Conference, ECDL 2001, Darmstadt, Germany, September 4-9, 2001. Proceedings.</em> (pp. 415-428) Heidelberg: Springer Verlag.  Retrieved 17 July 2003 from 
http://archive.dstc.edu.au/RDU/staff/jane-hunter/ECDL01/ECDL01.html</li>

  <li><a name="Little"></a>Little, S., Guerts, J. and Hunter, J. (2002) <a href="http://archive.dstc.edu.au/maenad/ecdl2002/ecdl2002.html">Dynamic generation of intelligent multimedia presentations through semantic inferencing</a>.   In: M. Agosti and C. Thanos, <em>eds.</em>  <em>Research and advanced technology for digital libraries.  6th European Conference, ECDL 2002, Rome, Italy, September 16-18, 2002. Proceedings.</em> (pp. 158-189). Heidelberg: Springer Verlag.  Retrieved 26 May 2003 from http://archive.dstc.edu.au/maenad/ecdl2002/ecdl2002.html </li>

<li><a name="Tweney"></a>Tweney, D. (2002) <a href="http://dylan.tweney.com/writing.php?display=331">Does Moore's Law still hold true?</a> [Originally published in <em>Business 2.0</em>]  Retrieved 17 July 2003 from http://dylan.tweney.com/writing.php?display=331 </li>

  <li><a name="WWWC"></a>World Wide Web Consortium (2000) <em><a href="http://www.w3.org/TR/smil20/">Synchronized Multimedia Integration Language (SMIL 2.0) specification.</a></em>  Cambridge, MA: World Wide Web Consortium. Retrieved 20 May 2003 from http://www.w3.org/TR/smil20/</li>

</ul>
<hr style="COLOR: #000080" size="1" />
<table cellspacing="10" align="center">
<tr><td colspan="2" align="center" style="font-family: verdana; font-size: small; font-weight: bold;">Find other papers on this subject.<br /></td></tr>
<tr><td align="center" valign="top">
<center>
<form method="get" action="http://scholar.google.com/scholar" target="_blank">
<table bgcolor="#ffffff">
<tr><td nowrap="nowrap" valign="top" align="center" height="32">
<input type="text" name="q" size="31" maxlength="255" value="&quot;information retrieval&quot; performance multimedia" style="background-color: Yellow;" /></input> <br />
<input type="submit" name="sa" value="Scholar Search"  style="font-family: Verdana; font-weight: bold;" /></input>
<input type="hidden" name="num" value="100" /></input>
</td></tr></table></form>
</center>
<td align="center" valign="top">
<!-- Search Google -->
<center>
<form method="get" action="http://www.google.com/custom" target="_top">
<table bgcolor="#ffffff">
<tr><td nowrap="nowrap" valign="top" align="center" height="32">
<input type="text" name="q" size="31" maxlength="255" value="&quot;information retrieval&quot; performance multimedia" style="background-color: Yellow;"></input><br />
<input type="submit" name="sa" value="Google Search" style="font-family: Verdana; font-weight: bold;" /></input>
<input type="hidden" name="client" value="pub-5081678983212084"></input>
<input type="hidden" name="forid" value="1"></input>
<input type="hidden" name="ie" value="ISO-8859-1"></input>
<input type="hidden" name="oe" value="ISO-8859-1"></input>
<input type="hidden" name="cof" value="GALT:#0066CC;GL:1;DIV:#999999;VLC:336633;AH:center;BGC:FFFFFF;LBGC:FF9900;ALC:0066CC;LC:0066CC;T:000000;GFNT:666666;GIMP:666666;FORID:1;"></input>
<input type="hidden" name="hl" value="en"></input>
</td></tr></table>
</form>
</center>
<!-- Search Google -->
</td></tr>
</table>

<hr style="COLOR: #000080" size="1" />
<div align="center">
<h4>How to cite this paper:</h4>
Brophy, P. (2003) 
&quot;Synchronised Object Retrieval: the enhancement of  information retrieval performance in multimedia environments using synchronisation protocols.&quot; &nbsp; <em>Information Research</em>, <strong>8</strong>(4) paper 161 [Available at http://InformationR.net/ir/8-4/paper161.html]</div>
<br />
<hr size="3" style="color:#000080 ;" />

<div align="center">&copy; the author, 2003. <br />Last updated: 26 May 2003</div>
<div align="center"><img src="../valid-xhtml10.gif" alt="Valid XHTML 1.0!" height="31" width="88" /></div>
<hr size="3" style="color:#000080 ;" />
<table border="0" cellpadding="5" cellspacing="10" align="center">
<tr> 
    <td><h4><a href="infres84.html">Contents</a></h4></td>
   <td align="center" valign="top"><h5 align="center"><img src="http://counter.digits.com/wc/-d/-z/6/-b/FF0033/paper161" align="middle"  width="60" height="20" border="0" hspace="4" vspace="2" alt="counter" /><br /><a href="http://www.digits.com/">Web Counter</a></h5></td>
    <td><h4><a href="../index.html">Home</a></h4></td>
  </tr>
</table>
<hr size="3" style="color:#000080 ;" />
<script src="http://www.google-analytics.com/urchin.js" type="text/javascript">
</script>
<script type="text/javascript">
_uacct = "UA-672528-1";
urchinTracker();
</script>
</body>

<!-- Mirrored from informationr.net/ir/8-4/paper161.html by HTTrack Website Copier/3.x [XR&CO'2010], Sat, 25 Dec 2010 19:11:22 GMT -->
</html>